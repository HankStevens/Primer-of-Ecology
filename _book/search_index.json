[
["index.html", "Primer of Ecology with R Preface", " Primer of Ecology with R Hank Stevens 2020-08-21 Preface In spite of the presumptuous title, my goals for this book are modest. I wrote it as the manual I wish I had in graduate school, and a primer for our graduate course in Population and Community Ecology at Miami University.1 It is my hope that readers can enjoy the and ignore the R code, if they care to. Toward this end, I tried to make the code easy to ignore, by either putting boxes around it, or simply concentrating code in some sections and keeping it out of other sections. It is also my hope that ecologists interested in learning R will have a rich yet gentle introduction to this amazing programming language. Toward that end, I have included some useful functions in an R package called . Like nearly all R packages, it is available through the R projects repositories, the CRAN mirrors. See the Appendix for an introduction to the R language. I have a hard time learning something on my own, unless I can something with the material. Learning ecology is no different, and I find that my students and I learn theory best when we write down formulae, manipulate them, and explore consequences of rearrangement. This typically starts with copying down, verbatim, an expression in a book or paper. Therefore, I encourage readers to take pencil to paper, and fingers to keyboard, and copy expressions they see in this book. After that, make sure that what I have done is correct by trying some of the same rearrangements and manipulations I have done. In addition, try things that aren’t in the book — have fun. For centuries, musicians and composers have learned their craft in part by to works of others. Physical embodiment of the musical notes and their sequences helped them learn composition. I have it on great authority that most theoreticians (and other mathematicians) do the same thing — they start by copying down mathematical expressions. This physical process helps get the content under their skin and through their skull. I encourage you to do the same. Whether otherwise indicated or not, let the first assigned problem at the end of each chapter be to copy down, with a pencil and paper, the mathematical expression presented in that chapter. In my own self-guided learning, I have often taken this simple activity for granted and have discounted its value — to my own detriment. I am not surprised how often students also take this activity for granted, and similarly suffer the consequences. the logic of something is not always enough — sometimes we have to actually the logic for ourselves. Using a text editor (such as the simple one in the R GUI) is one of the huge benefits of using R. Using a text editor to write code and to make extensive comments to yourself (and others!) makes R interactive. You interact with yourself by putting your thoughts on (virtual) paper, highlighting concepts and details you learn along the way, and documenting clearly what it is you have done. The code and the comments are two halves of the whole. Your comments also allow you to interact with others in a very complete way. You cannot realistically and reliably interact with others unless you put something in writing. If you use an application that is a pure GUI, you would have to describe the buttons you click and be absolutely certain that you leave nothing out and that you can actually communicate what you did. By writing and commenting code, you can send your scripts to anyone (your students, employees, boss, regulatory agency, collaborators). It may be useful to compare this book to others of a similar ilk. This book bears its closest similarities to two other wonderful primers: Gotelli’s , and Roughgarden’s . I am more familiar with these books than any other introductory texts, and I am greatly indebted to these authors for their contributions to my education and the discipline as a whole. My book, geared toward graduate students, includes more advanced material than Gotelli’s primer, but most of the ecological topics are similar. I attempt to start in the same place (e.g., ``What is geometric growth?’’), but I develop many of the ideas much further. Unlike Gotelli, I do not cover life tables at all, but rather, I devote an entire chapter to . I include a chapter on community structure and diversity, including , , , and , as well as . My book also includes code to implement most of the ideas, whereas Gotelli’s primer does not. This book also differs from Roughgarden’s primer, in that I use the Open Source R programming language, rather than Matlab, and I do not cover physiology or evolution. My philosphical approach is similar, however, as I tend to ``talk’’ to the reader, and we fall down the rabbit hole together. Aside from Gotelli and Roughgarden’s books, this book bears similarity in content to several other wonderful introductions to mathematical ecology or biology. I could have cited repeatedly (and in some places did so) the following: Ellner and Guckenheimer (2006), Gurney and Nisbet (1998), Kingsland (1985), MacArthur (1972), Magurran (2004), May (2001), Morin (1999), Otto and Day (2006), and Vandermeer and Goldberg (2007). Still others exist, but I have not yet had the good fortune to dig too deeply into them. I am indebted to Scott Meiners and his colleagues for their generous sharing of data, metadata, and statistical summaries from the Buell-Small Succession Study (http://www.ecostudies.org/bss/), a 50\\(+\\) year study of secondary succession (supported in part by NSF grant DEB-0424605) in the North American temperate deciduous forest biome. I would like to thank Stephen Ellner for Ross’s Bombay death data and for R code and insight over the past few years. I am also indebted to Tom Crist and his colleagues for sharing some of their moth data (work supported by The Nature Conservancy Ecosystem Research Program NSF DEB-0235369). I am grateful for the generosity of early reviewers and readers, each of whom has contributed much to the quality of this work: Jeremy Ash, Tom Crist, David Gorchov, Raphael Herrera-Herrera, Thomas Petzoldt, James Vonesh, as well as several anonymous reviewers, and the students of our Population and Community Ecology class. I am also grateful for the many conversations and emails shared with four wonderful mathematicians and theoreticians: Jayanth Banavar, Ben bolker, Stephen Ellner, Amit Shukla, and Steve Wright — I never have a conversation with these people without learning something. I have been particularly fortunate to have team-taught Population and Community Ecology at Miami University with two wonderful scientists and educators, Davd Gorchov and Thomas Crist. Only with this experience, of working closely with these colleagues, have I been able to attempt this book. It should go without saying, but I will emphasis, that the mistakes in this book are mine, and there would be many more but for the sharp eyes and insightful minds of many other people. I am also deeply indebted to the R Core Development Team for creating, maintaining and pushing forward the R programming language and environment . Like the air I breathe, I cannot imagine my (professional) life without it. I would especially like to thank Friedrich Leisch for the development of , which makes literate programming easy . Because I rely on Aquamacs, ESS, , and a host of other Open Source programs, I am deeply grateful to those who create and distribute these amazing tools. A few other R packages bear special mention. First, Ben Bolker’s text and packages for modeling ecological data ( and ) are broadly applicable. Second, Thomas Petzoldt’s and Karsten Rinke’s package provides a general computational architecture for ecological models, and implements many wonderful examples . Much of what is done in this primer (especially in chapters 1, 3–6, 8) can be done with , and sometimes done better. Third, Robin Hankin’s package is an excellent resource for exploring ecological neutral theory (chapter 10) . Last, I re lied heavily on the and packages . Last, and most importantly, I would like to thank those to whom this book is dedicated, whose love and senses of humor make it all worthwhile. Miami University is located in the Miami River valley in Oxford, Ohio, USA; the region is home to the Myaamia tribe that dwelled here prior to European occupation.↩︎ "],
["theory.html", "1 Theory in Ecology 1.1 Examples of theories 1.2 An example: Metabolic Theory of Ecology 1.3 Power law scaling implies constant relative differences", " 1 Theory in Ecology In this chapter, we introduce a perspective on ecological theory, and provide an example of an efficient theory, metabolic scaling. Scientific theory is a body of knowledge that provides an organized and mechanistic view of how the world works (Scheiner 2010). Theories concerning gravity, general relativity, and evolution by natural selection provide structured ways of connecting observations, patterns, and processes that provide insight into why the world is the way it is. This stands in stark contrast to the colloquial use of theory that implies a lack of knowledge, as when someone says “oh, that’s just a theory”, referring to a guess without much evidence. Scientific theory is a set of explanations whose validity has been tested repeatedly by experiments and new data. 1.1 Examples of theories Ecology has lots of theories, of all different types. Below I discuss some which may be prevalent, important, useful, or some combination. 1.1.1 Hierarchy theory An early and persistent organizing theory in ecology is based on hierarchy theory (O’Neill et al. 1986; Rose et al. 2017, and references therein). It posits that ecological systems are structured hierachically, such that each entity comprises subunits. For instance, an entity such as a population of big bluestem grass (Andropogon gerardii) is part of a larger ecology community of many species. The population of big bluestem comprises subpopulations separated in space, a subpopulation comprises separate individuals, that each individual comprises multiple ramets and a set of organ systems and tissues, which comprise different cell types. This theory posits that each entity gives rise to emergent properties to the hierarchical level above it, and influences processes within each smaller sub-entity in the hierachical level below it. As a disciplinary organizing principle, this approach structures nearly all of the ecology curriculum. Hierarchy theory gets more complicated when the levels of a hierarchy start to include fundamentally different types of entities. The big bluestem hierarchy above included only biotic components–a individual is part of a population which is part of a community of individuals of multiple species, and is made up of organ systems and tissues. Ecology, however, includes both the biotic and the abiotic parts of environments. An ecosystem includes a community of species, but also the nutrients, water, light, and other abiotic components, along with the spatial arrangement of all of these things. Different hierarchies are useful for different questions. An individual organism can play very different roles in different hierarchies. Consider and individual bunch grass. To understand how a population evolves, we need to count individuals within a population, because evolutionary fitness is tracked by the number of independent reproductive units. In contrast, to understand competitive interactions, it may be much more important to weigh the biomass of groups of individuals in a population, because biomass is more closely related of resource uptake. 1.1.2 A general theory of ecology Good scientific theories exist within a hierarchy of disciplinary knowledge (Scheiner and Willig 2011). They explain phenomena within a domain of knowledge which is organized around principles and assumptions. Scheiner and Willig posit a theory of biology that explains phenomena relating to the “diversity and complexity of living systems”. One of the ten principles on which this theory depends is that “the cell is the fundamental unit of life”. Subsumed within their theory of biology is the theory of cells whose domain is “cells and the causes of their structure, function, and variation.” This theory in turn is based on principles and has theories to organize our understanding of cells and what cells do. Models are specific and explicit manifestations of more general theories. In this book, we focus on popular mathematical models that are specific manifestions of theories of ecology. Scheiner and Willig (2011) propose a theory of ecology, some of which we cover in this book. Here is part of this theory: The General Theory of Ecology Domain: The spatial and temporal pattern of the distribution and abundance of organisms, including causes and consequences. Principles: Organisms are distributed in space and time in a heterogeneous manner. Organisms interact with their abiotic and biotic environments. Variation in the characteristics of organisms results in heterogeneity of ecological patterns and processes. The distributions of organisms and their interactions depend on contingencies. Environmental conditions as perceived by organisms are heterogeneous in space and time. Resources as perceived by organisms are finite and heterogeneous in space and time. Birth and death rates are a consequence of interactions with the abiotic and biotic environment. The ecological properties of species are the result of evolution. These principles constitute what we know is true about ecological systems. Some of these principles provide the focus for a single chapter while other principles apply broadly to many chapters in this book. Here is my own perspective on a general theory of ecology: Domain: The house of life2: its constituent entities, causes, and consequences. Principles: Entities3 are open systems with inputs and outputs. Entities have internal complexity. Entities include self-replicating components (living elements). Entities interact via inputs, outputs, and behavior. Rates of change, including inputs and outputs, are influenced directly by physical factors: space, temperature, and concentration. You will see elements of these principles throughout this book as well. 1.1.3 Efficient theory Marquet et al. (2014) argue that the best theories are those which are efficient. Such theories tend to be based on first principles, which are observations and laws that are fundamental assumptions in a scientific domain. In biology, such principles can include the laws of thermodynamics, and mathematical properties such as the central limit theorem. Theories built upon first principles are thus well-grounded in reality as we understand it and lead logically to refinements. Marquet and his colleagues also claim that efficient theory is expressed in mathematics. Mathematics is a universal language that is unamibiguous. It forces us to be as clear as possible about what we mean when we state a theory.4 Last, efficient theories are those that make a large number of predictions using only a small number of free parameters.5 Examples of efficient theories we cover in this book include metabolic scaling, exponential growth, density dependence, and ecological neutral theory. Marquet et al. and Scheiner and Willig emphasize slightly different features of the definition of “theory”. Scheiner and Willig emphasize relatively broad ideas that are well-supported by experiments and repeated observation. Marquet and colleagues tend to mean something fairly specific and narrow, typically something that can be expressed mathematically. Scheiner and Willig might refer to such theory as constitutive theory or even simply a model. Next, I describe the Metabolic Theory of Ecology. This theory is based on first principles, and its central tenets are expressed mathematically. It’s core equation has a very small number of free parameters (fitted constants) and makes a very large number of testable predictions. Parts of this theory are supported by a very large number of observations. It fits everyone’s definition of theory. 1.2 An example: Metabolic Theory of Ecology Metabolic rate is central to how rapidly individuals forage for, consume and use resources, reproduce and die. The metabolic theory of ecology (Brown et al. 2004) is a well-supported body of knowledge about the underlying mechanisms, and the resulting profound and wide-ranging consequences for populations and ecosystems. Body size and temperature are fundamental properties of organisms and the environment. The study of how body shape and body processes scale with body size is allometry. Because body size affects metabolic rate, body size indirectly helps determine population growth rates and how species interact with each other. Temperature affects how molecules vigorously molecules vibrate and move, and so increasing temperature tends to speed up chemical reactions. As metabolism is really just a complex network of biochemical reactions, temperature influences metabolic rate. The core of this theory is expressed in a simple mathematical equation that describes how body size and temperature govern metabolic rate. 1.2.1 Body-size dependence There is a profoundly simple and general rule describing the effect of interspecific variation in body size on metabolism.6 This biological law is referred to as the Kleiber law (Kleiber 1932), or quarter power scaling (Brown et al. 2004). When we compare the basal (i.e. resting) metabolic rates of different species, across a wide range of body sizes spanning many orders of magnitude, we find that whole-organism resting metabolic rate increases with organism mass raised to the three-quarter power, or, \\[ B = aM^{z} \\quad;\\quad z = 3/4 \\] In this equation, \\(B\\) is basal, or resting, metabolic rate, \\(M\\) is body mass, \\(a\\) is a proportionality constant, and \\(z\\) is the power law scaling coefficient. The proportionality constant \\(a\\) varies depending on the type of organism such as arthropods, fish, or mammals. Plants scale in the same manner (Niklas and Enquist 2001), although size or mass is a little trickier to measure. The scaling coefficient, \\(z\\), is the seemingly magical constant that many have argued does not vary substantially among different types of organisms. Ecologists typically describe metabolism-mass relations and other power law behavior using logarithmic scales. When we do that, power law relations become linear. Using our rules for exponents and logarithms, metabolic scaling becomes \\[ \\log B = \\log a + z\\log M\\] so that \\(\\log B\\) increases linearly with \\(\\log M\\) with a slope of \\(3/4\\). Our brains can process and compare linear relations much more easily than curvilinear ones. Here we plot the curvilinear relation in R using curve() in the graphics package of R that is included in the base installation as one of the core packages. The function curve() can plot any curve that be expressed as a function of x. Below, we draw a curve of a dotted 1:1 line for comparison, and then add the power function \\(x^{3/4}\\). Figure 1.1: Metabolic rate increases predictably with species body sizes. ## using curve, let your variable be &#39;x&#39;. curve(1*x, from = .01, to=100, ylab = &quot;Metabolic rate (B)&quot;, xlab=&quot;Body mass (M)&quot;, lty=3) curve(x^(3/4), from = .01, to = 100, add=TRUE) To help us grasp the implications of this, let’s consider mass-specific metabolic rates. “Mass-specific” means on a per-gram basis.7 Mass-specific metabolic rate is basal metabolic rate of an individual divided by its mass, or \\(B/M\\). The mass-specific metabolic rate allows us to compare directly, for example, the metabolic rate of a cell in a shrew vs. a cell in an elephant. Which cell is burning fuel faster? We can estimate this from the above metabolic scaling principle and the using rules exponents \\[ \\frac{B}{M} = a \\frac{M^z}{M^1} = a M^{z-1} = aM^{-1/4}\\] From this, we now have the rule that mass-specific metabolic rate declines with organisms mass raised to the negative one quater power eq1 = function(M, a){a*M^-0.25} # create the function, F(M) ggplot(data=data.frame(x=c(0.1, 100)), aes(x=x) ) + stat_function(fun=eq1, geom=&quot;line&quot;, args=list(a=1)) + xlab(&quot;Mass (M)&quot;) + ylab(&quot;Mass-specific metabolic rate (B/M)&quot;) Figure 1.2: Mass-specific metabolic rate declines predictably with species body sizes. Over the years, there has been heated debate about (i) the precise value of the scaling coefficient \\(z\\), and (ii) the underlying mechanism. Early arguments suggested that \\(z \\approx 2/3\\) because the rate heat dissipation scales with the amount surface area. Why \\(2/3\\)? Let’s envision the volume of an organism having three linear dimensions, so the volume scales to the cube of linear dimensions, while the surface area scales to the square of these linear dimensions,8 \\[V \\propto L^3\\] \\[A \\propto L^2\\] The early explanation was that metabolic rate, \\(B\\), scales linearly with area, \\[B \\propto A^1 \\propto L^2\\]. With substitution we get, \\[L^2 \\propto V^z \\propto (L^3)^z\\] implying that the exponents \\(2 = 3z\\) or \\(z=2/3\\), so we get, \\[B=V^{2/3}\\], and, for the most part, mass scales linearly with volume for mammals or any other such group. This early theory was because it started with first principles (heat dissipation and geometry) and resulted in the prediction of a single parameter. It could then be used to make predictions about how metabolic rate scales with body mass. Metabolic rate governs a huge amount of biology and ecology, including resource consumption rates, lifespan, and maximum population growth rates. Therefore, this theory and this model could be powerful tools for understanding the world and making testable predictions. The above model is good because it could be tested. That is what has been done, and scientists found that there was a consistent mismatch between observations and the theory. Investigators showed that the value of the exponent appeared closer to 3/4 raher than 2/3. In the 1990s, a group including Jim Brown and Geoffrey West (West, Brown, and Enquist 1997) proposed an underlying mechanism that explained why it should be 3/4. They assumed that organisms must distribute resources from a single source through a branching, fractal-like, space-filling network to all parts of the body, the size of the smallest branch ( a capillary) was the same for organisms of all sizes. the energy required to distribute the resources must be minimized, that less energy-efficient designs would be lost through natural selection. The prediction that resulted from these assumptions was that the exponent would be 3/4. This theory and model begin with different first principles and makes a different prediction. Soon Jayanth Banavar and his colleagues offered an alternative (Banavar, Maritan, and Rinaldo 1999; Banavar et al. 2002), arguing that the assumption of the fractal-like network was not correct, and in any event, was not necessary and did not apply to all organisms. They proposed different theory with less restrictive assumptions and found nonetheless that the exponent was also predicted to be 3/4. At the base of all these arguments is the geometry of the resource distribution system. All organisms take in limiting resources and have to distribute those resources to each part of each cell in the body. The key point is that the larger the organism, the greater the portion of the resources are in transit at any instant in time. This leads to an increasingly inefficient system, in which the metabolism of larger organisms has to run more slowly per unit resource: Larger organisms can process more resources per unit time (\\(B=aM^{3/4}\\)), but do so less and less efficiently (\\(\\frac{B}{M}=aM^{-1/4}\\)) due to resources in transport. Banavar, Brown and others eventually collaborated to address quarter power scaling in animals in particular which led to additional novel predictions (Banavar et al. 2010). This theory remains a fertile and active area of research (Glazier 2018). The interested reader should be careful to distinguish between patterns observed across many species of very different sizes, versus patterns observed in a single species with individuals of different sizes versus other types of patterns. Subtly different patterns may be driven be very different mechanisms. 1.2.2 Temperature dependence In addition to body size, temperature plays the other key role in regulating metabolic rate. The Arrhenius equation connects the macroscopic property of temperature to the kinetic energy of molecules and the rates they govern. Metabolic rate is proportional to these rate determining processes, \\[B = a e^{\\frac{-E_a}{kT}}\\] where \\(a\\) is just a constant, \\(e\\) is the exponential, \\(E_a\\) is the average activation energy of rate-limiting enzymes (units, eV), \\(k\\) is Boltzmann’s constant (units eV\\(\\,\\)K\\(^{-1}\\)), and \\(T\\) (units deg K). Bolztmann’s constant (\\(\\backsim 8.6 \\times 10^-5\\)\\(\\,\\)eV\\(\\,\\)K\\(^{-1}\\)) converts the macroscopic property of temperature to kinetic energy of molecules. Individual biochemical reactions combine to determine basal metabolic rate, so Gillooly (2000) have taken this as a foundation for the metabolic theory of ecology (Brown et al. 2004). In 2000, they suggested that the average activation energy is approximately \\(E_a = 0.23\\,\\)eV . Subsequent work has described this as “temperature sensitivity”, where larger numbers imply that organisms respond more strongly to temperature variation. The Arrhenius equation is a little more complicated that a simple power law, but not too much. Over the range of biologically relevant temperatures, it is dominated by a largely exponential increase in metabolic rate with increasing temperature (Fig @(fig:arrh)). # with base R # base R: curve(10^4*exp(-0.23/(8.5 * 10^-5 *x)), 276, 316), ylab=&quot;B&quot;, xlab=&#39;T&#39;) # or ggplot2 # the function, with default parameter values eq.t &lt;- function(t,a=10^4,E=0.23,k=8.6 * 10^-5){a*exp(-E/(k*t))} # the data used in our function temps &lt;- data.frame(t=276:316) ggplot(data=temps, aes(x=t)) + # set the basic properties of the plot stat_function(fun=eq.t, geom=&quot;line&quot;) + # set the function to plot xlab(&quot;Temperature (K)&quot;) + ylab(&quot;Metabolic rate (B)&quot;) Figure 1.3: The effect of body temperature on ectothermic metabolic rates can be approximated with the Arrhenius function, \\(B = a e^{-E_a/(kT)}\\). Here \\(a = 10^4\\), and \\(E_a = 0.23\\). It is similar in shape to a power law with z &gt; 1, over the range of biologically relevant temperatures. # add labels When we linearize the relation between metabolic rate and temperature, we get \\[ \\begin{aligned} B &amp;= a e^{\\frac{-E_a}{kT}}\\\\ \\log(B) &amp;= \\log{a} - E_a\\frac{1}{kT}\\\\ \\end{aligned} \\] where the dependent variable is \\(1/(kT)\\), \\(-E_a\\) is the slope, and \\(\\log a\\) is the intercept. Thus, the negative slope of this relation describes theoretical prediction for the effect of temperature on metabolic rate. So, there you have it. The metabolic theory ecology is the algebraic product of body size- and temperature-dependence: \\[B = a M^{3/4} e^{\\frac{-E_a}{kT}}\\] This theory makes quantitative predictions regarding all kinds of ecology phenomena (Brown et al. 2004), including home range size population growth population size resource uptake predation and other species interactions, and ecosystem cycling. Note that these relations are based on first principles of geometry and thermodynamics, and that they depend on only a small number of parameters (\\(a\\), \\(-E_a\\), and perhaps \\(z=3/4\\)), and makes a tremendous number of predictions. Therefore, Marquet et al. (2014) propose that this is “good” theory, and very efficient. 1.3 Power law scaling implies constant relative differences In power law scaling, relative change is constant. That is, a proportional change in one variable results in a proportional change in the other. For instance, when we compare a smaller species to a larger species with \\(100 \\times\\) the body mass, we can expect to see metabolic rate increase by \\(31.6 \\times\\), regardless of the mass of the smaller species. For now, we will verify this numerically for some limited cases. # define body mass and metabolic rate m &lt;- c(.01, 1, 100, 10000) b &lt;- m^.75 Now we will divide each mass \\(i\\) by the next smallest mass \\(i-1\\). We do that using a vector by dividing each mass except the first one, by each mass except the last one. # round(x, digits=0) rounds number to zero decimal places round( m[-1]/m[-length(m)], digits = 0) round( b[-1]/b[-length(b)], digits = 1) When we do these divisions, we see the constant relative change (1.1). Table 1.1: As we increase mass by a constant multiplier (10x), power law scaling results in a constant proportional change in basal metabolc rate. Small Med. Big Huge Mass 0.01 1.00 100.00 10000.00 Basal.metabolic.rate 0.03 1.00 31.62 1000.00 Relative.change.m NA 100.00 100.00 100.00 relative.change.b NA 31.62 31.62 31.62 We can verify this generally using algebra, not just in the particular case above. We will show that if mass increases by a constant multiplier, metabolic rate will also, regardless of the particular masses involved. Let mass \\(m_2\\) be greater than mass \\(m_1\\) by a factor of \\(c\\), so that \\(m_2 = c m_1\\), and \\[\\frac{m_2}{m_1} = c\\]. We would like to show that the ratio of the metabolic rates \\(b_2 / b_1\\) is also a constant. Since \\(m_2 = cm_1\\), we can say that \\[b_1 = a m_1^{3/4}\\] \\[b_2 = a (cm_1)^{3/4} = ac^{3/4}m_1^{3/4}\\] \\[\\frac{b_2}{b_1} = \\frac{ac^{3/4}m_1^{3/4}}{am_1^{3/4}}\\] When we reduce this fraction, we a left with \\[\\frac{b_2}{b_1} = c^{3/4}\\] This shows that with power law scaling, increasing \\(x\\) by a constant multipier (or proportion), \\(y\\) increases by the same proportion raised to that power. Let’s represent this graphically in a couple of ways, reusing data we made up previously in this chapter. First, we just change the axes themselves, so that the units of the scales are multiples of 10 (often in scientific notation). # using base R par(mar=c(5,4,0,0), mgp=c(1.5,.4,0) )# set figure margins in &quot;lines&quot; curve(x^(3/4), from = .01, to = 100, log=&quot;xy&quot;, ylab=&quot;Basal metabolic rate&quot;, xlab=&quot;Mass&quot;) text(10, 80^.7, expression(M^0.75)) Figure 1.4: changing the scales of the axes to linearize power law relations. Note scales are logarithmic, using the original linear values. References "],
["oft.html", "2 Optimal Foraging 2.1 A prey model 2.2 The patch model 2.3 A simulation of a prey model", " 2 Optimal Foraging Figure 2.1: Optimal foraging theory (OFT) generates testable quantitative predictions that allow a less ambiguous description and explanation for observed patterns and processes. Here, a simplistic model of Great Tit (Parus major) foraging that includes only gross energy intake underestimates the time spent in patches (dashed). In contrast, a model that includes energetic costs of traveling and searching matches predictions far better (solid). From Cowie (1977). It can be useful to think of natural selection as an optimizing process: phenotypes diversify, winners replicate and losers don’t, and the phenotypes of winners tend to get passed on to the replicants. Therefore, we often assume, as did Dr. Pangloss, that the species that exist now are the best of all possible species, that is, they are of optimal design. And like Dr. Pangloss, we would be woefully mistaken if we stopped there. Nonetheless, optimization, that is, the tendency toward an optimum, helps us generate testable hypotheses and we consider some of these below. Optimal Foraging Theory (OFT) helps us consider what organisms would do if they foraged optimally. All organisms–plants, fungi, archaea, and even animals–forage, and they are all subject to natural selection. Therefore, their phenotypes work pretty well, but probably not optimally and definitely not optimally for all times and places. Nonetheless, OFT is an efficient theory about the behavior of an organism, in the absence of other complications. Therefore, it allows us to study the relative importance of those “other complications.” Foraging is a key link between the individual, and communities and ecosystems (Beckerman, Petchey, and Morin 2010). All organisms interact with their environment via consumption, and the choices they make influence population dynamics, species interactions, nutrient cycling, and even the physical structures of terrestrial and aquatic habitats. The text and logic of this chapter rely heavily on Stephens and Krebs (1986) and Ellner (2009). In Scheiner and Willig’s edited volume on The Theory of Ecology, Andy Sih (Sih 2011) proposes that the following propositions form the basis of foraging theory: Foraging patterns maximize fitness or a correlate of fitness. Foraging patterns depend on the range of options available to the forager and on how each available option affects fitness or a correlate of fitness. Foraging behaviour balances conflicting demands–tradeoffs are important in shaping foraging behaviour. These properties are the outcome of natural selection operating on foraging behavior. Our understanding of foraging itself considers these three features (Stephens and Krebs 1986): currency (what is being optimized), constraints (features of behaviour that limit optimality), and the resulting decision rules. Currency is that quantity that is optimized by the forager. This currency is assumed to be a quantity that limits fitness, such as energy or a particular consumable resource. We measure it as a rate, for instance, as the energy gained per unit time (E/T) or the uptake of a critically limiting resource per unit time (R/T). Constraints are limitations that we assume about organisms. These might include distances between resource patches, the time and costs associated with extracting a resource from a substrate or subduing prey. They also include constraints imposed by other species including competitors and predators. Constraints can get complicated quickly; however, simple quantitative theory makes predictions against which we can evaluate more complicated assumptions. Decision rules are what we ascribed to a forager’s choices. A decision rule concern the probability of attacking prey if encountered, or when to leave one resource patch in order to search for another. An additional way to think about all this is where, when, and what. A great deal of effort has focused on understanding patch use: where foragers should explore for resources, and when they should give up and go in search of another patch (Charnov 1976). These are patch use models, and are based on economic models and the marginal value theorem. Another avenue of inquiry concerns what animals should eat. For instance, should they go after big prey that may be hard to catch and difficult to subdue, or just snack on what is easy? These are prey models or diet models, and attempt to explain why organisms consume what they do. A note on “prey”. All organisms forage for resources. Plants extend branches toward the light, and proliferate leaves and roots into resource rich patches, and rhizomes grow longer faster through resource-poor soils. Bumblebees search for and learn where to find nectar-rich flowers, and wolves hunt in packs to take down large ungulates. Some bacterivorous nanoflagellates intercept particles selectively depending on the perceived nutritional value of particles (Boenigk et al. 2002). So, depending on the forager, its “prey” may be \\(\\mathrm{NO}_{3}^-\\) ions, nectar, moose, or bacteria. Therefore, we will refer to these resources variously as prey, prey items, resources, and resource items. Some of these ideas are best handled with patch-based models (Charnov 1976) where a “resource patch” is a more intuitive and useful unit. A note on “handle”. All organisms pays costs to consume resources. In OFT, “handle” typically means expending energy an time to attack and subdue prey (predators), proliferate into resource rich areas (plants), exude extra cellular enzymes (fungi); ingest the item(s), and then resume searching. 2.1 A prey model …in which a forager asks, “should I eat this?”9 Let’s start where this field started, with a prey-centered model (MacArthur and Pianka 1966; Emlen 1966). The goal is to optimize the currency. 2.1.1 Our intuition Figure 2.2: The amount of energy, E (y-axis), that is lost and gained by a foraging ant–it may decline slowly over time (x-axis) while searching, and decline quickly while handling a food item. Our ant gains energy when it consumes an item. Below: Our ant. She expends energy while searching for food. Upon encountering a food item, she may choose to ‘handle’ it (encounters 1 and 3) and gain energy, or not handle it (encounter 2) and save the added cost of handling it. It seems reasonable that if a forager encounters food, it should eat it. However, if handling it costs more than the forager gets back in energy, then it isn’t worth it. We might think of this as the ratio as profitability, \\[\\frac{e_i}{h_i}\\] where \\(e_i\\) is the energy in an item of type \\(i\\), and \\(h_i\\) is the cost of handling said item. If \\(e_i/h_i&lt;1\\), then it doesn’t make sense to select the item. Further, handling an item means that the forager is not looking for a better food item. This suggests that even if \\(e_i/h_i&gt;1\\), a forager may not want to handle it if it is likely to soon encounter food items of higher energy content. On top of this, the act of searching may expose a forager to a risk of running into competing foragers, or even being eaten by a bigger forager. Clearly, a forager faces tradeoffs as it searches and when it encounters resources. 2.1.2 Mathematical support One of the reasons to represent ideas mathematically is that we make concrete assumptions, and then the math can tell us what the predictions are. That is what we will do here. Let’s assume that natural selection tends to maximize the currency as Gain per unit Time, \\(G/T\\). Our model will use these parameters and variables: \\(i =\\) index for prey type \\(S =\\) total time spent searching (units = seconds, \\(s\\)). \\(\\lambda_i =\\) rate of encounter with prey of type \\(i\\) (units = # encountered/s = \\(\\#/s\\); note this can also be #/area × area/s, if we like)10 \\(p_i =\\) probability that a forager attacks prey if encountered (units are number handled per number encountered, or #/#; this is a dimensionless parameter) \\(h_i =\\) handling time for an item of type \\(i\\), (units = s/#). \\(T =\\) total elapsed time (units = s) From these definitions we can calculate other important quantities. Total number of items encountered of type \\(i\\) is \\[S \\lambda_i\\] The units are \\(s\\, \\#\\,s^{-1} = \\#\\). Total number of type \\(i\\) items handled is the proportion, \\(p_i\\), of those encountered that the foragers chooses to go after, or \\[S\\lambda_i p_i\\] The units are \\(\\#\\). Total time spent handling all items of type \\(i\\) is \\[H=S\\lambda_i p_i h_i\\] The units are \\(s\\).11 Total elapsed time is time spent searching plus time spent handling, which is \\[T=S + \\sum_i^n S\\lambda_i h_i p_i\\] where we use the summation to add together the total handling times for each prey or resource type \\(i = \\{1,\\,2, \\ldots ,\\,n\\}\\). Let \\(e_i =\\) net energetic gain from catching and consuming a single type-i prey item (units = Joules, J). This includes the gross energy of the item minus handling costs plus energy not lost by searching during that time. \\(c =\\) energy cost per unit of time while searching (units = J). Total energy gain from eating all the items is the number of items of each type \\(i\\) handled times the net amount of energy per item of type \\(i\\), \\(e_i\\), \\[\\sum_{i=1}^n S\\lambda_i p_i e_i\\] where units are \\(\\# (\\mathrm{J}/ \\#) = \\mathrm{J}\\). Therefore, rate of energy intake (J/\\(s\\)) while handling and eating is \\[\\mathrm{intake} = \\frac{\\sum_{i=1}^n S\\lambda_i p_i e_i}{S + \\sum_{i=1}^n S\\lambda_i h_i p_i}\\] If we then subtract the cost of searching, we arrive at the quantity we want to maximize, \\[G/T=\\frac{\\sum_{i=1}^n \\lambda_i p_i e_i}{1 + \\sum_{i=1}^n \\lambda_i p_i h_i}-c\\] A major question in OFT is whether a forager should include a particular prey type. Say we rank the prey types, \\(i=\\{1,2,...,m,...,n\\}\\), in terms of energy content, where type \\(i=1\\) has the most energy per item, \\(i=m\\) is intermediate, and type \\(i=n\\) has the least. Which items should a forager include in her diet? Should it be only the most energy-dense, or should it include the second as well, or should it be all of them? Part of the answer rests on the ratio of energy gain versus handling costs, or profitability, \\(e_i/h_i\\). If we maximized \\(G/T\\) with respect to \\(p_j\\), we would be able to determine whether to include item \\(j\\). Doing so leads to several predictions. Prediction 1 A less energy-dense item should be added if its net energy content is greater than the realized energy gain from all the other items, \\[\\begin{equation} \\frac{e_{m+1}}{h_{m+1}} &gt; \\frac{\\sum_{i=1}^m \\lambda_i e_i}{1 + \\sum_{i=1}^{m} \\lambda_i h_i} \\tag{2.1} \\end{equation}\\] where the diet already includes items 1-\\(m\\), and the realized energy content of the diet takes into account average encounter rates of each item type. It means that a foraging will always select a particular type (\\(p_j =1\\)), or never select it (\\(p_j=0\\)); this is known as the “zero-one rule”. Prediction 2 Foragers will rank prey types by their profitability, \\(e/h\\). Prediction 3 When encounter rates increase (as with increasing abundances), selectivity increases. Note that encounter rates are in the right hand side, so as they increase, so will that fraction on the right. That will make it harder for the above inequality to be true, and a forager will be pickier. If you don’t believe it, try this simplified version (Fig. 2.3). G.T = function(lambda, h=1, e=1){lambda*e/(1+lambda*h)} # create the function you want myData &lt;- data.frame( lambda=c(0, 10) ) # data you need ggplot(data=myData, aes(x=lambda)) + # set the basic properties of the plot # in the stat_function, we indicate the parameters in our equation. stat_function(fun=G.T, geom=&quot;line&quot;) + ylab(bquote(over(lambda*e, 1 + lambda*h))) + xlab(bquote(lambda)) # add labels Figure 2.3: Selectivity increases with average encounter rates. Prediction 4 Inclusion of type \\(m+1\\) in the diet does not depend on its encounter rate. Thus, a particular type should be included if the instantaneous net gain of that type is greater than the realized long term average net gain of all the more profitable types. Note that encounter rate appears on the right hand side, but not the left. So how does this model fair in the real world? Well, the zero-one rule doesn’t work at all; it turns out that for a variety of reasons, foragers do not completely ignore low-profit prey. However, there is great support for the other predictions (above) (Stephens and Krebs 1986). Most importantly, in all cases, the theory has provided a clear framework to generate testable predictions from unambiguous assumptions, and that is what we want from efficient theory. The model itself helped guide research, and inclusion of greater complexity has led to deeper understanding of behavior and its consequences for species interactions. 2.2 The patch model …in which omniscient rationale agents roam free. Here the forager asks, “how long should I stay here?” In the simple prey model, a forager searches for and then encounters prey one at a time, makes a decision to consume or not, and then resumes searching. In a simple patch model, a forager searches for and encounters patches one at a time, first consumes resources and then makes a decision to leave or not. Perhaps the single most important prediction of the simple patch model is that a forager should leave a patch when its current rate of energy gain drops down to the average or expected rate of energy gain for the habitat at large. In what follows, we rely on Charnov (1976), who applies the marginal-value theorem to explain optimal behavior. Here, as in economics, “marginal value” refers to a rate - the slope of a function. In calculus, this is a derivative. Here, it is the derivative (i.e. slope) of the relation between energy gain and time. Let’s assume the simplest of all patch models: one patch type, all patches are the same, and they are distributed randomly in the habitat. Assume also that a forager uses time to travel between patches (travel time, \\(t_t\\)) and time searching within a patch (residence time, \\(t_r\\)). A forager encounters patches at random, with a rate of \\(\\lambda\\), and as such, would have a mean time to next encounter of \\(1/\\lambda\\). The patch is characterisized by its gain function \\(g(t_r)\\) (Fig. 2.4) which is the expected12 cumulative net energy gained, given time \\(t_r\\) spent in the patch. The gain function is a cumulative total net amount. We can imagine different types of gain functions. Figure 2.4: Net energy gain as a function of patch residence time may take different forms. Net energy gain increases through time but slows (decelerates) as a greater fraction of the resources in the patch are consumed. The top line (solid) assumes that there are diminishing returns as a patch is depleted, but the forager continues to find resources in excess of metabolic losses. The lower line (dashed) represents the net energy gain that could arise as a patch is depleted more fully and the costs continue unabated. Try this: Draw a gain function where the prey remain well hidden at first, but the forager becomes increasingly able to find more and more prey. Draw a gain function where there is no cost to foraging, and where the forager eventually depletes all the prey. In one graph, draw two gain functions for a resource rich patch and for a resource poor patch. So, our currency is long-term average energy intake, \\(R\\), and we want to maximize this. The decision our forager needs to make is how long to stay in a patch. The forager’s constraints share some similarity with the prey model (Stephens and Krebs 1986). between-patch travel time and within-patch hunting time are distinct, and … … independent of each other, a forager encounters patches sequentially and randomly, in a given patch, net expected energy gain is a function of time spent in the patch… …that is zero when \\(t=0\\), and …increases with time, but then decelerates the forager is omniscient - it knows everything about available patches and does not learn anything new as it forages (because it already knows everything). The forager must decide how long to stay in the patch to maximize \\(R\\). Let \\[R=\\frac{g(t_r)}{t_t + t_r}\\] where \\(t_t + t_r\\) is the total time from leaving one patch, traveling to the next patch, foraging in the second patch, and then leaving the second patch. Think of this as benefit (\\(g(t)\\)) per unit time. This fraction is the slope of the straight line in Fig. 2.5. Intuitively, we can imagine that the long term average rate of energy gain \\(R\\) is unimodal (hump-shaped) in the following scenario (Fig. 2.4). Upon encountering a patch the forager has no resources and thus \\(R\\) is actually negative due to the costs of traveling to the new patch. As \\(t_r\\) passes and and the forager gains energy (\\(g(t_r)\\) increases), \\(R\\) will increase and become positive. An assumption of the theory (and reality) is that the gain function, \\(g(t_r)\\), decelerates–the rate of energy intake declines as the patch is depleted. With increasing time in the patch and lower rate of energy intake, \\(R\\) starts to decline. When \\(t_r\\) is too short, \\(R\\) is not yet maximizes. When \\(t_r\\) is too long, \\(R\\) begins to decline. Because \\(R\\) is hump-shaped, we can use calculus to find its maximum. This will occur when its slope is zero, and the slope of a function, \\(F\\), is its derivative, \\(F^\\prime\\). If we asssume that travel time is constant, then we can take the partial derivative of \\(R\\) with respect to just the residence time, \\(t_r\\), \\(\\delta R / \\delta t_r\\). First, recall the product rule of differentiation: \\[F(x) = g(x)f(x)\\quad ; \\quad F^\\prime(x) =f^\\prime(x)g(x) + f(x)g^\\prime(x)\\] With that we can find the necessary derivative. \\[\\frac{\\delta R}{\\delta t_r} = - \\frac{1}{(t_t+t_r)^2} g(t_r) + \\frac{1}{t_t+t_r}g^\\prime(t_r)= g^\\prime(t_r) - \\frac{g(t_r)}{t_t+t_r}=0\\] Because this derivative equals zero when the slope of the gain function (\\(g^\\prime(t_r)\\)) equals \\(R\\), that tells use that \\(R\\) is maximized at that point. Therefore, it predicts that in order to maximize the long-term average rate, we should stay in a patch until the instantaneous rate, \\(g^\\prime(t_r)\\) drops to the long term average rate, \\(R\\) (Fig. 2.5). Figure 2.5: Energy gain vs. time: The origin is when the forager enters the patch; to the left is time spent traveling from one patch to the next, and to the right is time spent in the patch. The graph represents two different habitats, one in which the patches are easy to get to (habitat 1), and another where it takes more time to get from patch to patch (habitat 2). In all cases, the patches are identical, having the same gain function. The curved line is the gain function, the net energy gain as a function of time spent in the patch. The slope of that curve is the derivative of the gain function. Its slope at any single time point is the instantaneous rate of gain. The two straight lines are the expected gains averaged over time for each habitat as a whole. Lambda is the rate at which a forager randomly encounters patches - because it is a Poisson process, the mean or expected time is 1/lambda. The forager should leave the patch when the instantaneous rate of gain in the patch equals the long term average rate of gain for the habitat as a whole. The simple patch model predicts that when average travel time is greater, foragers will stay longer in a patch. Similarly, the model predicts that when patch quality is lower, foragers stay longer in each patch. Use Fig. 2.5 to construct explanations for these predictions. Just a starting point The simple prey and patch models have been extended a great deal to help understand a broad range of foraging situations (Sih 2011). Simultaneous, rather than sequential, encounters can lead to different predictions. In these cases, energy alone, \\(e_i\\), rather than profitability, \\(e_i/h_i\\), may determine prey selection that maximizes the long term mean average rate. Travel time and encounter rates interact with this to explain contrasting situations. Central place foragers play by slightly different rules (Stephens and Krebs 1986). Central place foragers are located in a single location, and remain there. For instance, a parent bird (or dinosaur) finds patches and returns repeatedly to the nest, bringing one or multiple prey items. With parent birds, their fitness depends on offspring viability, and so selection tends to optimize in a manner similar to an organism foraging for themselves. These cases have been built upon patch models, where the question is how to exploit patches that exist at different distances from the nest. Another example of a central place forager is a spider that acts as a ambush or sit-and-wait predator who remains stationary until a prey item gets close enough to attack. One approach to the spider problem is to consider the distance to the prey as a handling cost and search costs are negligible. These simple foraging models provide the starting points for a field of inquiry spanning many decades. The interplay between these models, the natural history of species, and experiments have led to greater appreciation of why organisms behave as they do, and the consequences for their evolution and the food webs and ecosystems in which they reside. 2.3 A simulation of a prey model Next, we embark on a simulation of the simple prey model. We will start with these assumptions: two prey types, \\(i = {1,2}\\) ranked effective energy contents, \\(e_1 &gt; e_2\\) equal handling times, \\(h_1=h_2=1\\) equal relative abundances, \\(r_1=r_2=0.5\\) encounter rates determined by an overall prey encounter rate, \\(\\lambda\\), and the relative abundances where \\(\\lambda_i = \\lambda r_i\\). equal probability of attack if prey is encountered, \\(p_1=p_2=1\\). search cost is constant, \\(c_s=0.01\\) In addition to these properties, our simulation needs several bookkeeping parameters and variables in order to track the forager energy content. It will need to run for a finite amount of time; we’ll control that with the total search time, Total. Remember that encounter rates are means but that actual encounters are random or stochastic. As a result, our forager may go through lean periods in which their net energy intake is negative. We need to keep track of total elapsed time, and for each cycle, the search time, search cost, handling time, and energy gain. optimal.forager &lt;- function( e = c(2, 1), # energy content of the prey types h = c(.5, .5), # handling times r = c(.5, .5), # relative abundance of prey types: sum(r) = 1 lambda = 0.4, # overal encounter rate, for all prey combined p = c(1,1), # prob. of attack if encountered cs = 0.4, # cost of searching per unit time Total = 10 # limit to foraging time ) { ############### ### begin foraging ec &lt;- NULL # an object to tally gains and costs. cycle &lt;- 0 # the cycle count (= search, choose and maybe attack and eat) ct &lt;- 0 # start time of the cyclesan object to tally cycle times. elapsed.time &lt;- 0 # total time spent foraging while( elapsed.time &lt; Total ) { # count which search cycle we&#39;re on (cycle &lt;- cycle + 1) # a random amount of search time, t.s, until it finds something. (lambda.r &lt;- lambda * r) (ts &lt;- rexp(2, rate=lambda.r)) if(ts[1] &lt; ts[2]) i &lt;- 1 else i &lt;- 2 i # cost of searching for that time (cost.s &lt;- ts[i] * cs) # choose to attack the encountered item with probability p (gain &lt;- if(p[i] &gt; runif(1)){e[i]} else {0}) # observed handling time if(gain &gt; 0 ){ h.obs &lt;- h[i] h.obs } else { h.obs &lt;- 0 } h.obs (cycle.time &lt;- ts[i] + h.obs ) ct &lt;- c(ct, cycle.time) (elapsed.time &lt;- elapsed.time + cycle.time) (ec &lt;- c(ec, gain - cost.s)) } df &lt;- data.frame(net.e = ec, cycle.start = cumsum(ct[1:cycle])) params &lt;- list(e=e, h=h, r=r, lambda=lambda, p=p, cs=cs, Total=Total) out &lt;- list(N = cycle, G = sum(ec), Tt = sum(ct), series = df, params = params) return(out) } Here we let the forager forage for 60 minutes and then examine the structure of the output object. myOut &lt;- optimal.forager(Total=60) str(myOut) ## List of 5 ## $ N : num 17 ## $ G : num 4.09 ## $ Tt : num 60.8 ## $ series:&#39;data.frame&#39;: 17 obs. of 2 variables: ## ..$ net.e : num [1:17] 1.911 1.35 0.551 0.533 1.822 ... ## ..$ cycle.start: num [1:17] 0 0.723 2.848 4.471 6.137 ... ## $ params:List of 7 ## ..$ e : num [1:2] 2 1 ## ..$ h : num [1:2] 0.5 0.5 ## ..$ r : num [1:2] 0.5 0.5 ## ..$ lambda: num 0.4 ## ..$ p : num [1:2] 1 1 ## ..$ cs : num 0.4 ## ..$ Total : num 60 N is the number of foraging cycles G is net energy gain Tt is total elapsed time series is a dataframe with two variables: net.e is energy gain minus search costs for each cycle, and cycle.start is the elapsed time at which each cycle starts params is a list that includes all the parameters we used in this run Now let’s graph something, because graphs are fun. ggplot(myOut$series, aes(x=cycle.start, y=cumsum(net.e))) + geom_line() Figure 2.6: The cumulative energy capital of a forager goes down while searching and handling resource items, but increases each time the prey is assimilated. Use this simulation to help solidify in your own mind predictions of the simple prey model. How should we do that? What is the prediction we are interested in? Prediction: Include type 2 if \\[\\begin{equation} \\frac{e_2}{h_2} &gt; \\frac{\\lambda_1 e_1 }{1 + \\lambda_1 h_1} \\tag{2.2} \\end{equation}\\] Figure 2.7: The right hand side of our prediction To get a sense of what our prediction (2.2) means, we should graph the righthand quantity as a function of one relevant variable, such as energy content of type 1, or the encounter rate (Fig. 2.7). The parameters that determined these curves are: unlist( myOut$params ) ## e1 e2 h1 h2 r1 r2 lambda p1 p2 cs Total ## 2.0 1.0 0.5 0.5 0.5 0.5 0.4 1.0 1.0 0.4 60.0 2.3.1 Lab exercise Do these parameter values suggest that our forager should or should not include prey type 1 in her diet? Create parameter combinations for which the forager (i) should and (ii) should not include prey type 2. Use the simulation optimal.forager() to confirm your predictions. References "],
["expo.html", "3 Simple density-independent growth 3.1 Discrete growth rates of fruit flies in my kitchen 3.2 Fruit flies with continuous overlapping generations 3.3 Properties of geometric and exponential growth 3.4 Modeling with Data: Simulated Dynamics", " 3 Simple density-independent growth Figure 3.1: Song Sparrow (Melospiza melodia) counts in Darrtown, OH, USA. From Sauer, J. R., J.E. Hines, and J. Fallon. 2005. The North American Breeding Bird Survey, Results and Analysis 1966–2004. Version 2005.2. USGS Patuxent Wildlife Research Center, Laurel, MD. Figure 3.2: Song Sparrow (Melospiza melodia) annual changes in population size as a function of population size. Between 1966 and 1971, Song Sparrow (Melospiza melodia) abundance in Darrtown, OH, USA, seemed to increase very quickly, perhaps unimpeded by any particular factor (Fig. @ref{fig:Melospiza1}, @ref{fig:Melospiza2}). In an effort to manage this population, we may want to predict its future population size. We may also want to describe its growth rate and population size in terms of mechanisms that could influence its growth rate. We may want to compare its growth and relevant mechanisms to those of other Song Sparrow populations or to other passerine populations. To do this, we start with the simplest of all population phenomena, geometric and exponential growth. Geometric and exponential growth are examples of density-independent growth. This captures the fundamental process of reproduction (e.g., making seeds or babies) results in a geometric series.13 For instance, one cell divides to make two, those two cells each divide to make four, and so on, where reproduction for each cell results in two cells, regardless of how many other cells are in the population—that is what we mean by density-independent. This myopically observed event of reproduction, whether one cell into two, or one plant producing many seeds, is the genesis of a geometric series. Therefore, most models of populations include this fundamental process of geometric increase. Second, populations can grow in a density-independent fashion when resources are plentiful. It behooves us to start with this simple model because most, more complex population models include this process. Hastings (2011) proposes that we can approach single species poulation growth from either a microscopic or macroscopic point of view. The microscopic approach begins with two propositions. The first is that if we know the location, timing, and traits of all individuals, we can predict perfectly population dynamics; the second is that we can never predict dynamics perfectly because births and deaths are fundamentally random and can be described only with probabilities.14 With this microscopic approach, we would seek a very detailed description of individuals and build a complex model to understand the consequences of the characteristics of all these interacting individuals, including the growth of the poppulation. In this chapter, I choose to start with Hastings’ macroscopic approach. These propositions appear simpler. A population grows exponentially in the absence of other forces. There are forces that can prevent a population from growing exponentially. These are the consequences of the following assumptions. all individuals in a population are identical. there is no migration in or out of the population. c1. the number of offspring per individual (or the per capita birth and death rates) are constant through time, and … c2. … independent of the number of individuals in the population. Deviations from these assumptions lead to all of the most interesting parts of single species population dynamics (Hastings 2011). The only deviation we play with in this chapter concerns assumption c; we model stochastic variation in population growth rate to investigate extinction risk. It is also worth mentioning that, although propositions 1 and 2 follow from assumptions a-d, they are not strictly necessary (Hastings 2011). For instance, individuals need not be identical, and we deal with a big exception in the next chapter where we introduce structured population growth. Also, migration is admissable, provided immigration = emigration and it does not alter growth rates. Nonetheless, other deviations from a. and b. can have very important consequences for single species population dynamics. Here we define Density-independence15 in a real population as a lack of a statistical relation between the density of a population, and its per capita growth rate. The power to detect a significant relation between any two continuous variables depends on those factors which govern statistical power, such as the number of observations, the range of the predictor variable, and the strength of the underlying relation. Therefore, our conclusion, that a particular population exhibits density-independent growth, may be trivial if our sample size is small, with few generations sampled, or if we sampled the population over a very narrow range of densities. Nonetheless, it behooves us to come back to this definition if, or when, we get caught up in the biology of a particular organism. In this chapter, we’ll introduce density-independent population projection, growth, and per capita growth, for populations with synchronous reproduction (discrete models), and continuous reproduction (continuous models). 3.1 Discrete growth rates of fruit flies in my kitchen Summertime, and the living is easy. Fruit flies in my kitchen, and their number’s quite high. Flies love my ripe fruit, and my red wine. They drown in the wine–I am not sure if that is good or bad. For now, we’ll treat fruit flies as if they grow in discrete generations. This is very common for populatilons that live in seasonal habitats - their reproduction is timed to the season, and they breed altogether in one bout.16 I count the number of flies every week, and I find these numbers: t &lt;- c(0, 1, 2, 3) N &lt;- c(2, 4, 8, 16) qplot(x=t, y=N, geom=c(&quot;line&quot;, &quot;point&quot;) ) There are several ways we can describe fruit fly population growth. We begin by thinking about the proximate causes of change to population size per unit time: births, immigration, death and emigration (Fig. 3.3). Those are the only options, and we state it thus: \\[\\frac{\\Delta N}{\\Delta t} = \\frac{B + I - D - E}{\\Delta t}\\] that is, the pop growthe rate17 is determined by the numbers of births, deaths, and migrants per unit time. Over the past month, I suspect the fruit flies are increasing primarily through reproduction in my kitchen. Clearly, at some point in the past, a fly or two (or three) must have immigrated into my kitchen, either as adults or as eggs or larvae in fruit I brought home (\\(I&gt;0\\)). For now, I will assume fruit fly population dynamics in my kitchen are governed by only births and deaths (\\(I=E=0\\)), so, we have \\[ \\frac{N_{t+1} - N_t}{(t+1) - t}=\\frac{\\Delta N}{\\Delta t}=\\frac{B+D}{\\Delta t}\\] In this equation, \\(t\\) has a particular time unit, one week, so \\(t+1\\) is one additional week. We refer to a population like this as closed, because it is closed to migration in or out. Figure 3.3: The number of fruit flies in my kitchen depends on immigration and emigration, and births and deaths. In the text, we assume that immigration and emigration are zero. All rates are individuals per unit time. I would like to represent births and deaths as proportions of existing adults. that is, as \\[B = bN;\\quad D=dN\\] This reflects the biological realities that adults produce offspring, and everyone has some chance of dying. The parameter \\(b\\) could be any positive real number, \\(b \\ge 0\\). This model of births reflects the geometric property of reproduction: over a specified time interval \\(\\Delta t\\), an average parent makes \\(b\\) babies. Parameter \\(d\\) is any real number between zero and one, \\(0 \\le d \\le 1\\). Both \\(b\\) and \\(d\\) have units of individuals per individual per unit time. They depend on that unit of time. What if offspring die before the next census? Fig. 3.3 helps us think about these things. Simplifying, we’ll assume births occur first, and then death comes to offspring and adults. Let’s define a few terms. \\(N_0\\), \\(N_1\\) - the number of flies at the start and after the first time interval. \\(N^\\prime\\), \\(N^{\\prime\\prime}\\) - distinct values of \\(N\\), just after births. \\(\\Delta N\\) - the change in \\(N\\) from one point in time to another. \\(t\\) is time, so \\(\\Delta t\\) is the time interval over which \\(N\\) may change. Let’s match these numbers to what is going on in my kitchen. For my first census count, \\(t=0\\), I counted the adults and label that number \\(N_0\\). These adults lay eggs which hatch, larvae and pupae develop, and become adults, giving us a population of \\(N^\\prime = N_0 + bN\\) Some of the eggs fail to hatch, and some of the larvae die before maturing. Many of the adults die as well. If we assume the eggs, larvae, and adults all die at the same rate, then by the end of one generation we have \\(N_1 = dN^\\prime = d(N_0 + bN)\\). Substituting and multiplying we get \\[ N_1 = N_0 + bN_0 - d\\left(N_0 + bN_0\\right)\\] We see that by the next time point, \\(t=1\\), the number of fruit flies should be equal to the number we started with, \\(N_0\\), plus the number of new individuals, \\(bN_0\\), minus the number of original adults that die, \\(dN_0\\), and minus the number of new individuals that die, \\(dbN_0\\). We can pull all of these parameters together, \\[ N_1 = N_0 + bN_0 - dN_0 - dbN_0 \\] \\[ N_1 - N_0 = N_0 \\left(b - d - db\\right) = N_0 + r_dN_0 \\tag{3.1}\\] where \\(r_d = b - d - db\\). The growth rate of the population is \\(\\Delta N / \\Delta t\\), or, at \\(t=0\\), is \\[\\frac{\\Delta N}{\\Delta t} = \\frac{N_1 - N_0}{t_1-t_0} = \\frac{(N_0 + r_dN_0) - N_0}{t_1-t_0} = r_d N_0 \\] If we generalize, we drop the zero, to get \\(r_dN\\). The per capita population growth rate is \\(r_dN/N =r_d\\)). If our time step were something other than 1, we would also divide by \\(\\Delta t\\). With the simple census data above, we can estimate \\(r_d\\) for the first time step. \\[N_1 = N_0 + r_dN_0= 2 + r_d (2) \\implies r_d=1\\] If we know that \\(r_d\\) is constant over time, we can infer a general rule to project the population forward in time an arbitrary number of time steps. We will let \\(\\lambda = 1+r_d\\). \\[N_1 = N_0 + r_dN_0 = N_0(1 + r_d) = N_0\\lambda\\] \\[N_2 = N_1\\lambda= (N_0 \\lambda)\\lambda\\] \\[N_3 = N_2\\lambda= (N_0 \\lambda)\\lambda\\lambda\\] or simply, \\[N_t = N_0\\lambda^t\\] To summarize our model of discrete population growth, we have the following statements: Projection: \\[N_t = N_0\\lambda^t\\] Population growth rate: \\[\\frac{\\Delta N}{\\Delta t} = r_dN; \\quad \\mathrm{where~} \\lambda=1+r_d\\] Per capita opulation growth rate: \\[\\frac{\\Delta N}{N\\Delta t} = r_d\\] At last, we see how this is a model of density-independent growth: per capita growth rate does not include \\(N\\). 3.2 Fruit flies with continuous overlapping generations In the reality that is my kitchen, individual fruit flies are having sex and reproducing on their own schedules. As a population, they breed continuously, so the cohorts re not synchronous. For populations like that, we need to describe instantaneous growth rates, where \\(\\Delta t\\) is no longer a fixed period of time, but is an instant, or infinity small. We return to our example above (Fig. 3.3), which we summarize in (3.1). Please take a look at that equation; here we make time explicit so that it appears in the equation. We begin by remembering that \\(b\\) and \\(d\\) have time units. Let \\(\\Delta t\\) be a small fraction of \\(t\\), so that the time step goes from \\(t\\) to \\(t + \\Delta t\\). As \\(\\Delta t \\rightarrow 0\\), \\(b\\) and \\(d\\) need to shrink as well, to \\(\\Delta t b\\) and \\(\\Delta t d\\). \\(dN/dt\\) is how we identify the differential equation that is the instantaneous rate of population growth, with lower case \\(d\\) symbolizing infinitesimally small change. We now have to solve for the limit of \\(\\Delta N /\\Delta t\\) as \\(\\Delta t\\) goes to zero. \\[\\frac{dN}{dt}=\\lim_{\\Delta t \\rightarrow 0} \\frac{N_{t+\\Delta t} - N_t}{\\Delta t} = \\lim_{\\Delta t \\rightarrow 0} \\frac{\\Delta t\\,bN_t - \\Delta t \\,dN_t - \\Delta t\\, d (\\Delta t\\, b)N_t}{\\Delta t} \\] If we divide through by \\(\\Delta t\\) and then let \\(\\Delta t \\rightarrow 0\\), we get \\[\\frac{dN}{dt}=\\lim_{\\Delta t \\rightarrow 0} bN_t - dN_t - \\Delta t\\, d bN_t = bN_t - dN_t=rN\\] To arrive at the projection equation for a continuously growing population, we integrate \\(rN\\) with respect to time. Integration is the cumulative summing of \\(y\\) across a range of \\(x\\). It even uses an exagerated “S” to indicate summation, \\(\\int\\). Here we integrate population growth across time. We start by rearranging \\[\\frac{dN}{dt} = rN \\Rightarrow \\frac{dN}{N} = r dt\\] Now we integrate from \\(N\\) and \\(r\\) from \\(t=0\\) to \\(t=t\\), \\[\\int_{N_0}^{N_t} \\frac{1}{N}dN = \\int_{0}^{t}rdt\\] \\[\\ln(N_t) - \\ln(N_0) = rt - r\\,0\\] \\[\\ln(N_t) = \\ln(N_0) + rt\\] We now exponentiate (\\(e^x\\)) both sides to arrive at our projection equation. \\[N_t = e^{\\ln(N_0) + rt} = N_0 e^{rt}\\] To summarize our model of continuous population growth, we have the following statements. Projection: \\[N_t = N_0 e^{rt}\\] Population growth rate: \\[\\frac{dN}{dt} = rN\\] Per capita population growth rate: \\[\\frac{dN}{Ndt} = r\\] Once again, we see why we refer to exponential growth as density-independent: the per capita growth rate does not depend on \\(N\\). 3.3 Properties of geometric and exponential growth Compare the projection equations for geometric and exponential growth. We find that \\[\\lambda = e^{r} \\quad ; \\quad \\ln \\lambda = r\\] This gives us a few useful rules of thumb. No change: \\(r = 0\\quad;\\quad\\lambda =1\\) Growing population: \\(r &gt; 0 \\quad;\\quad \\lambda &gt; 1\\) Shrinking population: \\(r &lt; 0 \\quad;\\quad \\lambda &lt; 1\\) # Let r take on three values r &lt;- c( -1, 0, 1) # Convert to lambda exp(r) ## [1] 0.3678794 1.0000000 2.7182818 Time scaling This is a useful property if we ever want to change time units in a discrete model. We must first \\(\\lambda\\) to \\(r\\), change units in \\(r\\) and convert back to \\(\\lambda\\). For instance, if we find that the annual finite rate of increase for a population of crickets is \\(\\lambda = 1.2\\), we cannot convert that to a monthly rate of \\(1.2/12 = 0.1\\). Instead we convert to \\(r\\) and back to \\(\\lambda\\). lambda &lt;- 1.2 # Convert lambda to r r &lt;- log(lambda); r ## [1] 0.1823216 # Scale r from year to month r2 &lt;- r/12; r2 ## [1] 0.01519346 # Convert back to lambda (per month) lambda2 &lt;- exp(r2); lambda2 ## [1] 1.015309 This is very, very different than \\(\\lambda/12\\). Doubling time Sometimes we gain a more intuitive grasp of an idea when we convert to a different form of the same relationship. Exponential growth is one of those ideas that can be hard to grasp. A more intuitive way to compare or express exponential grwoth rate is through doubling time, the time required for the population to double in size. For instance, a per capita growth rate of \\(r = 0.14\\,\\mathrm{inds}\\cdot \\mathrm{ind}^{-1} \\mathrm{y}^{-1}\\) means that the population will double in less than 5 years. We determine this by letting \\(N_t = 2N_0\\). \\[2N_0 = N_0 e^{rt}\\] \\[\\ln 2 = rt\\] \\[t =\\frac{\\ln 2}{r}\\] # let r be a sequence from r &lt;- c(0.01, 0.05, 0.1, 0.5) #doubling time will be log(2)/r ## [1] 69.314718 13.862944 6.931472 1.386294 # and a picture par(mgp=c(1.2, .2, 0), mar=c(2, 2, 1, 1), tcl=-.2) curve( log(2)/x, xlab=&quot;r&quot;, ylab=&quot;Doubling time&quot;) Figure 3.4: Doubling time is inversely related to the intrinsic rate of increase, r. 3.3.1 Average growth rate In any real data set, such as from a real population of fruit flies or Song Sparrows, \\(N_{t+1}/N_t\\) will vary from year to year. How do we calculate an average growth rate for a fluctuating population? Let’s consider the case where a population increases and then decreases. For each year, we will calculate the annual rate of increase \\(R = N_{t+1}/N_t\\), and take the arithmetic average of those rates to see if it makes sense. N &lt;- c(20, 30, 15, 15) R &lt;- N[2:4]/N[1:3]; R ## [1] 1.5 0.5 1.0 The arithmetic average of those rates is \\((1.5 + 0.5 + 1.0)/3=1.0\\). If \\(R=1.0\\), then, on average, the population should stay the same, but it decreased. Why is that? Let us do the annual time steps explicitly to see what is going on. \\[N_3 = (N_0 R_0) R_1 R_2\\] # Remember that we call the first time t=0 and N0, but # when coding, these values are the first in a series, so # N0 is N[1] # Now we do the annual changes which should equal N3 N[1]*R[1]*R[2]*R[3] ## [1] 15 From this calculation, we see that when we start with \\(N_0=20\\) and do the annaul steps, we wind up with a smaller population, even though the arithmetic average is \\(R_{\\mathrm{ave}} = 1\\). How do we calculate an average of numbers that we multiply together? We want a number \\(\\bar{R}\\) such that \\[\\bar{R}^t = R_1R_2\\ldots R_t\\] To find that, we simply solve for \\(\\bar{R}\\) \\[(\\bar{R}^t)^{1/t} =\\bar{R} = \\left(R_1R_2\\ldots R_t\\right)^{1/t}\\] We take the \\(t\\)-th root of the product of all the \\(R\\). This is called the geometric average. Another way of writing this would be to use the product symbol, as in \\[\\bar{R} = \\left(\\prod_{i=1}^t R_i\\right)^{1/t}\\] R ## [1] 1.5 0.5 1.0 #arithmetic average mean(R) ## [1] 1 # geometric average t &lt;- length(R); t ## [1] 3 prod(R)^(1/t) ## [1] 0.9085603 # shows the population should decline Another way to do the same thing is to take the arithmethic mean of the log-growth rates, and back-transform, exp( mean( log(R) ) ) ## [1] 0.9085603 Now we see the effect of calculating the average growth rate correctly. This leads to a useful rule of thumb: random variation in growth rate suppresses population growth. Here we illustrate that. We start with a growing population. lambda &lt;- 1.01 # positive growth rate N0 &lt;- 100 # starting N t &lt;- 20 # 20 years Nt1 &lt;- N0*lambda^t; Nt1 ## [1] 122.019 Here \\(\\lambda &gt; 1\\), so the population grows. Now we do a simulation in which we let \\(\\lambda\\) have a mean of 1.01 but allow it to vary randomly. # create a vector to hold all N N &lt;- rep(0, t); N[1] &lt;- N0 # create t-1 random lambdas with a mean of 1.01 # ranging from 0.41 to 1.61 set.seed(3) # makes the radnom sequence repeatable random.lambda &lt;- runif(n=(t-1), min=0.41, max=1.61) # the geometric mean prod(random.lambda)^(1/length(random.lambda)) ## [1] 1.00105 # actual simulated projection for(i in 1:(t-1)) { N[i+1] &lt;- N[i] * random.lambda[i] } qplot(x=0:(t-1), N, geom=c(&quot;line&quot;, &quot;point&quot;), xlab=&quot;Time (y)&quot;) Figure 3.5: Random variation in growth rate alters the long term average growth rate. Sometimes the arithmetic average is close to the correct average, but it is never the correct approach. 3.4 Modeling with Data: Simulated Dynamics Science strives to make predictions about about the behavior of systems. Ecologists and conservation biologists frequently strive to predict the fate of populations. Here we put into practice ideas about population biology to make informed predictions about the fate of the Song Sparrow population in Darrtown, OH. We also illustrate simple commputational methods for doing so. The preceding sections (the bulk of the chapter) emphasized understanding the deterministic underpinnings of simple forms of density independent growth: geometric and exponential growth. This section explores the stochastic simulation of density independent growth. Our simulation makes most of the same assumptions we made at the beginning of the chapter. In addition, we assume that the observed annual growth rates (\\(N_{t+1}/N_t\\)) are representative of future growth rates, and that the growth rate in one year is entirely independent of any other year. To make meaning full projections of future population size, we should quantify the uncertainty with our guess. Simulation is one way we can project populations and quantify the uncertainty. The way one often does that is to use the original data and sample it randomly to calculate model parameters. This way, the simulations are random, but based on our best available knowldge, i.e., the real data. The re-use of observed data occurs in many guises, and it is known often as bootstrapping or resampling. 3.4.1 Data-based approaches We could use the observed changes in population counts \\(R_t=N_{t+1}/N_t\\) as our data. We would then draw an \\(R_t\\) at random from among the many observed values, and project the population one year forward. We then repeat this into the future, say, for ten years. Each simulation of a ten year period will result in a different ten year trajectory because we draw \\(R_t\\) at random from among the observed \\(R_t\\). However, if we do many such simulations, we will have a distribution of outcomes that we can describe with simple statistics (e.g., median, mean, quantiles). A different approach would be to estimate the individual probabilities of births and deaths in the entire Darrtown population, and use those probabilities and birth rates to simulate the entire population into the future. In such an individual-based simulation, we would simulate the fates of individuals, keeping track of all individual births and deaths. There are myriad other approaches, but these give you a taste of what might be possible. In this section we focus on the first of these alternatives, in which we use observed \\(R_t\\) to simulate the dynamics of Song Sparrow counts. Do do so, in part, because we have those data, while we do not have any estimates of birth rates or death rates. Here we investigate Song Sparrow (Melospize melodia) dynamics using data from the annual U.S. Breeding Bird Survey (http://www.mbr-pwrc.usgs.gov/ bbs/). Below we will create and examine visually the data (annual \\(R\\)’s), simulate one projection, scale up to multiple simulations, simplify simulations and perform them 1000s of times, and analyze the output. 3.4.2 Creating and visualizing the data Let’s start by graphing the data18. Graphing the data is always a good idea — it is a principle of working with data. We first load the data from the primer R package, and look at the names of the data frame. We then choose to attach the data frame, because it makes the code easier to read.19 library(primer) data(sparrows) names(sparrows) ## [1] &quot;Year&quot; &quot;Count&quot; &quot;ObserverNumber&quot; attach(sparrows) Now we plot these counts through time (Fig. 3.6). ggplot(data=sparrows, aes(x=Year, y=Count)) + geom_line() + geom_point(pch=1) Figure 3.6: Observations of Song Sparrows in Darrtown, OH (http://www.mbr-pwrc.usgs.gov/bbs/). We see that Song Sparrow counts at this site (the DARRTOWN transect, OH, USA) fluctuated a fair bit between 1966 and 2003. They never were completely absent and never exceeded \\(\\sim 120\\) individuals. Next we calculate annual \\(R_t=N_{t+1}/N_t\\), that is, the observed growth rate for each year \\(t\\). # the use of [-1[ in the index tells R to exclude the first element. # length() is the length of a vector, so [-length(X)] means exclude the last obs.R &lt;- Count[-1]/Count[-length(Count)] Thus our data are the observed \\(R_t\\), not the counts per se. These \\(R\\) form the basis of everything else we do. Because they are so important, let’s plot these as well. Let’s also indicate \\(R=1\\) with a horizontal dotted line as a visual cue for zero population growth. Note that we exclude the last year because each \\(R_t\\) is associated with \\(N_t\\) rather than \\(N_{t+1}\\). qplot(x=Year[-length(Count)], y=obs.R, geom=&quot;point&quot;) + geom_hline(yintercept=1, lty=3) + labs(y=bquote(N[t+1]/N[t]), x=&quot;Year (t)&quot;) Figure 3.7: Annual growth rates (R=N[t+1]/N[t]) for Song Sparrows One thing that emerges in our graphic data display (Fig. 3.7) is we have an unusually high growth rate in the early 1990’s, with the rest of the data clustered around 0.5–1.5. We may want to remember that. 3.4.3 One simulation Our simulation will, determine the number of years we wish to simulate, create an empty vector, N, to hold our simulated \\(N\\), which is years + 1 long, draw a random sample of \\(R_t\\), one for each year (R), select a starting abundance \\(N_0\\) and put it in N[1]. multiply our first random \\(R\\), R[1], times N[1] to generate the next, N[2]. repeat step 5 for each year to simulate each N[t+1] from R[t] and N[t]. First, we decide how many years we want to simulate growth, and create an empty vector that will hold our data. years &lt;- 10 N &lt;-numeric(years+1) # rep(0,years+1) would do the same thing. Our vector of \\(N\\) has to be one longer than the number of \\(R\\) we use. This is because each \\(R\\) is sthe change from one year to the next and there will always be one more next than there is \\(R\\). Next we draw 10 \\(R\\) at random with replacement. This is just like having all 35 observed \\(R\\) written down on slips of paper and dropped into a paper bag. We then draw one slip of paper out of the bag, write the number down, and put the slip of paper back in the bag, and then repeat this 9 more times. This is resampling with replacement. In that case, we would be assuming that all of these \\(R_t\\) are important and will occur at some point, but we just don’t know when—they constitute the entire universe of possiblities. The R function sample will do this. [A random process occurs only in our imagination, or perhaps at the quantum level.20 A stochastic process is one which we treat operationally as random while acknowledging that there are complex underlying deterministic drivers. A pseudorandom process is a completely deterministic and hidden process used by computers and their programmers to generate numbers that cannot be distinguished from random; we can repeat a pseudorandom process by stipulating a key hidden starting point.] We can use set.seed() to make your pseudorandom process the same as mine, i.e., repeatable. set.seed(3) # Draw a sample of our observed R with replacement, &quot;years&quot; times. (rRs &lt;- sample(x=obs.R, size=years, replace = TRUE)) ## [1] 1.4489796 0.8125000 1.0714286 1.2857143 0.7727273 0.4805195 1.2857143 ## [8] 1.0500000 0.7204301 1.4489796 Now that we have these 10 \\(R\\), all we have to do is use them to generate the population sizes through time. For this, we need to use what programmers call a for-loop. In brief, a for-loop repeats a series of steps for a predetermined number of times. Let’s start our simulated N with the sparrow count we had in the last year. N[1] &lt;- Count[length(Count)] Now we are ready to use the for-loop to project the population. For each year \\(t\\), we multiply \\(N_t\\) by the randomly selected \\(R_t\\) to get \\(N_{t+1}\\) and put it into the \\(t +1\\) element of N. for( t in 1:years) { # starting with year = 1, and for each subsequent year, do... N[t+1] &lt;- N[t] * rRs[t] } Let’s graph the result. qplot(0:years, N, geom=c(&quot;point&quot;,&quot;line&quot;)) Figure 3.8: A single simulated population projection. It appears to work (Fig. 3.8). Let’s review what we have done. We had a bird count each year for 36 years. From this we calculated 35 \\(R\\) (for all years except the very last). decided how many years we wanted to project the population (10,y). * drew at random and with replacement the observed \\(R\\)—one \\(R\\) for each year we want to project forward. * we created an empty vector and put in an initial value (the last year’s real data). * performed each year’s calculation, and put it into the vector we made. So what does Fig. 3.8 represent? It represents one possible outcome of a trajectory, if we assume that \\(R\\) has an equal probability of being any of the observed \\(R_t\\). This particular trajectory is very unlikely, because it would require one particular sequence of randomly selected \\(R\\)s. However, it is no less likely than any other particular trajectory. As only one realization of a set of randomly selected \\(R\\), Fig. 3.8 tells us very little. What we need to do now is to replicate this process a very large number of times, and examine the distribution of outcomes, including moments of the distribution such as the mean, median, and confidence interval of eventual outcomes. 3.4.4 Multiple simulations Now we create a way to perform the above simulation several times. There are a couple tricks we use to do this. We still want to start small so we can figure out the steps as we go. Here is what we would do next. We start by creating a function that will do the steps we did above. We then do replicate independent simulations, using replicate(). Here we write a function to combine several steps. myForLoop &lt;- function(obs.R, years, initial.N) { # select all R at random rR &lt;- sample(obs.R, size=years, replace=TRUE) # create a vector to hold N N &lt;- numeric(years+1) # give it an initial population size N[1] &lt;- initial.N # Do the for-loop for( t in 1:years ) { # project the population one time step N[t+1] &lt;- N[t] * rR[t] } # return the vector of N N } # try it out with different hypothetical R myForLoop(obs.R=0:3, years=5, initial.N=43) ## [1] 43 129 0 0 0 0 Our function seems to work. Next we do ten such projection simulations, each for 50 time steps, using the sparrow data. # specify the number of simulations and for how long sims=10; years=50 set.seed(3) outmat &lt;- replicate(sims, expr=myForLoop(obs.R=obs.R, years=years, initial.N=43) ) Now let’s peek at the results (Fig. 3.9). It is fun to graph our output, but also helps us make sure we are not making a heinous mistake in our code. Note we use log scale to help us see the small populations. matplot(0:years, outmat, type=&quot;l&quot;, log=&quot;y&quot;) Figure 3.9: Using matplot() to plot a matrix vs. a single variable. Our simulated populations sometimes increase and sometimes decrease. # combine columns years, and our output junk &lt;- data.frame(years = 1:(years+1), outmat) names(junk) ## [1] &quot;years&quot; &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; &quot;X4&quot; &quot;X5&quot; &quot;X6&quot; &quot;X7&quot; &quot;X8&quot; ## [10] &quot;X9&quot; &quot;X10&quot; # make sure to load &#39;tidyr&#39; if you did not already load it or tidyverse # library(tidyr) # &quot;gather&quot; many columns into one (except years) out.long &lt;- gather(junk, key=&quot;Run&quot;, value=&quot;N&quot;, -years) ggplot(data=out.long, aes(x=years, y=N, group=Run)) + geom_line() + scale_y_log10() Figure 3.10: Using ggplot() to plot one variable against vs. a single variable, organized by a grouping variable. Our simulated populations sometimes increase and sometimes decrease. # Or for colorful lines # ggplot(data=out.long, aes(x=years, y=N, linetype=Run, colour=Run)) + # geom_line(show.legend=FALSE) + scale_y_log10() What does it mean that the simulation has an approximately even distribution of final population sizes (Fig. )? If we plotted it on a linear scale, what would it look like?^[Plotting it on the log scale reveals that the relative change is independent of population size; this is true because the rate of change is geometric. If we plotted it on a linear scale, we would see that many trajectories result in small counts, and only a few get really big. That is, the median size is pretty small, but a few populations get huge.} Rerunning this simulation, with new \\(R\\) each time, will show different dynamics every time, and that is the point of simulations. Simulations are a way to make a few key assumptions, and then leave the rest to chance. In that sense it is a null model of population dynamics. 3.4.5 A distribution of possible futures Now we are in a position to make an informed prediction, given our assumptions. We will predict the range of possible outcomes and the most likely outcomes, given our set of assumptions. We will simulate the population for 50 years 10,000 times and describe the distribution of final populatin sizes. We use system.time to tell me how long it takes on my computer. sims=1e4; years=50 set.seed(3) ## system.time keeps track of how long processes take. system.time( outmat &lt;- replicate(sims, expr=myForLoop(obs.R=obs.R, years=years, initial.N=43) ) ) ## user system elapsed ## 0.156 0.029 0.205 This tells me how long it took to complete 10,000 simulations. We also check the dimensions of the output, and they make sense. dim(outmat) ## [1] 51 10000 We see that we have an object that is the size we think it should be. We shall assume that everything worked way we think it should. 3.4.6 Analyzing results We extract the last year of the simulations (last row), and summarize it with quartiles (0%, 25%, 50%, 75%, 100%, and also the mean). N.2053 &lt;- outmat[51,] summary(N.2053, digits=6) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 12.1 60.7 1306.3 297.7 2299420.0 hist(log10(N.2053)) Figure 3.11: Distribution of the 10000 final base-10 log population sizes. Note the approximately Normal distribution. The quantile() function allows us to find a form of empirical confidence interval, including, approximately, the central 90% of the observations.21 quantile(N.2053, prob=c(0.05, .95) ) ## 5% 95% ## 1.331579 2862.940808 These quantiles provide an estimate of the most likely range of possible populatin sizes, given our assumptions. 3.4.7 Inferring processes underlying growth rate The above approach relies only on the observed data. That means that the growth rates, while representative, can never be different than what was observed. A different approaach would be to assume that the growth rates can be different than observed, but drawn from the same underlying process that caused the observed rates. The observed rates are simply a visible manifestation of unseen processes. We might summarize these by asserting that the observed growth rates were samples from a continuous distribution distribution, whose prperties we can infer from the sample. For instance, it may be that these processes cause annual rates to follow a Normal, or perhaps log-normal distribution. We can fit a Normal distribution to the logarithms of our observed \\(R\\), and we see that it doesn’t do too bad a job (Fig. 3.12). mu &lt;- mean( log(obs.R) ) sigma &lt;- sd( log(obs.R) ) # a regular sequence for log-R lR &lt;- seq(-1, 1.1, by=0.01) # the probability densities for the log-R dR &lt;- dnorm(lR, m=mu, sd=sigma) hist(log(obs.R), breaks=10, prob=TRUE) lines(lR, dR) Figure 3.12: The logarithms of the observed R seem reasonably approximated by a Normal distribution whose mean and standard deviation are derived from the log-transformed data. Now we will simulate populations just like before, but instead of random draws from the observed data, we do random draws from the inferred distribution. Our new function. myForLoop2 &lt;- function(mu, sigma, years, initial.N) { # select all R at random from lrR &lt;- rnorm(years, m=mu, sd=sigma) rR &lt;- exp(lrR) # create a vector to hold N N &lt;- numeric(years+1) # give it an initial population size N[1] &lt;- initial.N # Do the for-loop for( t in 1:years ) { # project the population one time step N[t+1] &lt;- N[t] * rR[t] } # return the vector of N N } Our new simulations. sims=1e4; years=50 set.seed(3) outmat2 &lt;- replicate(sims, expr=myForLoop2(mu=mu, sigma=sigma, years=years, initial.N=43) ) N2.2053 &lt;- outmat2[51,] quantile(N2.2053, prob=c(0.05, .95) ) ## 5% 95% ## 1.205509 3089.819907 quantile(N.2053, prob=c(0.05, .95) ) ## 5% 95% ## 1.331579 2862.940808 The results are very similar to those based on only the observed \\(R\\). If they were markedly different, we might ask whether our choice of distribution was appropriate. Our conclusions are based on a model of discrete density-independent population growth what assumptions are we making and are they valid? Are our unrealistic assumptions perhaps nonetheless a good approximation of reality? what would you like to add next to make the model a better approximation? Perhaps we might explain some of the variation in annual growth rates to weather patterns in the breeding range (Darrtown, Ohio). If so, we might separate demographic effects vs. environmental effects. That would give us an even better model we could use for explanation and prediction. In this chapter, we have explored the meaning of density-independent population growth. It is a statistically demonstrable phenomenon, wherein the per captia growth rate exhibits no relation with population density. It is a useful starting point for conceptualizing population growth. We have derived discrete geometric and continuous exponential growth and seen how they are related. We have caculated doubling times. We have discussed the assumptions that different people might make regarding these growth models. Last, we have used simulation to explore prediction and inference in a density-independent context. References "],
["DID.html", "4 Density-independent Demography 4.1 A two stage matrix model 4.2 A brief primer on matrices 4.3 Decomposing A 4.4 A three stage model 4.5 Projection 4.6 Analyzing the transition matrix 4.7 R packges for demography 4.8 Exploring a real population", " 4 Density-independent Demography In the preceding chapter, we listed Hastings’ (Hastings 2011) key principles and assumptions of single species population growth. One of the key assumptions is that “all individuals in a population are identical.” In this chapter, we elucidate an important violation of that assumption, population structure. Figure 4.1: Demography of human populations of Mexico and Sweden. Based on 1990 data from US Census Bureau, Population Division, International Programs Center. Populations have structure. Consider the human populations of Mexico and Sweden in 1990. Mexico had a much larger fraction of their population in child bearing age classes or younger (Fig. 4.1). In addition, the age-specific fertility rate was higher in Mexico, especially for younger women (Fig. 4.1). How did this happen, and why did Mexico have so many young people? What were the consequences of this for their culture, their use of resources, their domestic and foreign policies, and their future population growth? How about Sweden? Demography is the study of populations with special attention to their structure (Lincoln, Boxshall, and Clark 1998). Originally, age-based human demography was the provenance of actuaries who helped governments keep track of the number citizens of different ages and thus, for instance, know how many would be available for conscription into the military.22 The reason we model the structure of populations is because various demographic rates vary markedly with these stages. Juveniles produce no offspring. Very few seeds survive an entire year, whereas some large adults survive very well. We use structure when that structure is associated with important differences in demographic rates: survival, fecundity, and growth. The structure to which we refer is simply the organization of populations by some character such as life history stage, age, or size. Sizes and ages are often reduced to categories such as we saw in human populations (e.g., 0–4.9,y, 5–9.9,y,…). Sizes may be based on any reliable and repeatable measure that relates to demographic rates and are similarly binned. Life history stages may include eggs, larvae, metamorphs, juveniles, and adults in amphibians, or seeds, rosettes, and reproductive stems in herbaceous plants. With a variable such as size, we don’t even need to use categories, but rather we can use size as a continuous variable; we address this briefly later in the chapter. Structured population models allow us to intertwine species-specific natural history and quantitative methods. This makes the approach especially appealing for the conservation biology of threatened and endangered species. We use structured population models to improve our better understanding of a population or improve predictions of its future dynamics, or guide the management of the population. We might learn a lot about what controls the abundance of a species if we can test ideas related to different stages, ages, or sizes. What limits the population growth of the Western toad – is it egg survival, or overwintering survival of juveniles? Where should we invest our efforts to control garlic mustard (Alliaria petiolata) – killing the first year rosettes, or the second year adults? Why are cacti generally endangered (Goettsch et al. 2015)—is the smallest size, or the largest size that is most essential to insure long-term survival? We can use structured population models to address such questions. 4.1 A two stage matrix model Figure 4.2: Like all amphibians, the Western toad (Anaxyrus boreas) has a complex life cycle, with several life history stages. Adults breed in early spring, laying eggs in water. The larvae (tadpoles) hatch and develop over the spring and summer, and then metamorphose (become metamorphs), and then juveniles. Juveniles require more than a year to mature. Adults can live up to about a decade. American toads (A. americanus) do the same thing. A matrix model of a structured population consists of stages and transitions. Vonesh and Cruz (2002) used matrix projection to assess the importance of egg mortality for declines in amphibian populations. Their model of the Western toad (Anaxyrus boreas, Fig. 4.2) comprises two stages (juveniles and adults) and four transitions. In all structured popuation models23, a transition is the annual contribution of an individual in stage \\(i\\) at time \\(t\\) to stage \\(j\\) at \\(t+1\\). In Fig. 4.2, the transition from juvenile to adult is the probability that a juvenile survives an entire year and also matures, becoming sexually viable.24 The transition from juvenile to juvenile is the probability that a juvenile survives a year and does not mature. The transition from adult to adult is the probability that an adult survives the year. These three transitions are probabilities. The transition from adult to juvenile (Fig. 4.2) is typically referred to as fecundity, and it is the product of several events. Vonesh and De la Cruz assume that this transition depends on the population sex ratio, the average clutch size of a female, egg survival, larval survival and metamorphosis, and the overwintering survival of metamorphs. They even assume that larval survival depnds on denisty. Thus what we refer to as “fecundity”25 is far more than just average clutch size because it must include all the processes that occur over the year associated with producing a clutch and the survival of that clutch. Structured population models allow us to take advantage of the natural history of our study species. For our study population, at a minimum, we need to (i) identify stages that differ in their demographic rates, and (ii) when individuals tend to breed. Consider the example of the Western toad (Anaxyrus boreas). As with all amphibians, survival and fecundity rates depend heavily or entirely on life history stages of egg, larvae (tadpole), juvenile, and adult. We would know that breeding occurs in early spring, depending on latitude and elevation. If we wanted to model juveniles and adults, we would typically sample a population prior to breeding when juveniles and adults are just starting to become active. The design of a structured population model depends on the sampling or census schedule. These models are typically assume an annual census that occurs just before, and just after seed set, egg laying, or births. We refer to these as pre-breeding or post-breeding census models. Vonesh and Cruz (2002) (Fig. 4.2) use a pre-breeding census model. This is because only juveniles and adults are present in the population at the time of sampling. If the design assumed a post-breeding census (later in the year), it would probably include three stages, with larva (tadpoles) in addition to juveniles and adults. The reasons for using a pre- vs. post-breeding census include our ability to actually identify and sample stages, and parameter estimation. For example, it may be easy to accurately estimate the abundance of juvenile and adult toads, but very difficult to estimate larval density and larval survival. In such a case, we could represent the adult to juvenile transition as a black box, estimated as the total number of new juveniles in year \\(t+1\\) divided by the number of adults in year \\(t\\). We can draw a two different types of life cycle graphs for this two-stage model in just such a population (Fig. 4.3). Some people find one more illuminating than the other. It is useful to be able to use both. Figure 4.3: Two types of life cycle graphs. These both represent an amphibian pre-breeding model. All the stages must be present during the annual census, and each arrow or transition must represent everything that happens over the entire year. Notice the transition from adult toad to juvenile toad (Fig. 4.3) includes egg production, egg survival, tadpole or larva survival and growth, and metamorphosis out of the aquatic stage. This are obviously important events. We make explicit only those stages that we count during our census; all other other events are iplicit within the transitions. Once we have a life cycle diagram (Fig. 4.3), we create a transition or projection matrix that represents mathematically all of the stages and transitions between stages (4.1). This matrix will have one row and one column for each stage, and the columns represent the stages in year \\(t\\) and the rows represent the stages in year \\(t+1\\). We refer to a single column by j and a single row by i. Each column represents stage j in year \\(t\\), and each row represents stage i in year \\(t+1\\). For our amphibian example, the transition matrix will have two rows and two columns. It will be a “two by two”, or \\(2 \\times 2\\) matrix. \\[ \\begin{equation} \\tag{4.1} \\mathbf{A} = \\left( \\begin{array}{cc} p_{11}&amp;F_{12} \\\\ p_{21}&amp;p_{22} \\end{array} \\right) \\end{equation} \\] If (4.1) represents the Western toad (Fig. 4.3), then transition \\(p_{11}\\) is the probability that juveniles survive but fail to mature, \\(p_{21}\\) is the probability that juveniles survive and also mature, \\(p_{22}\\) is the probability that adults survive, and \\(F_{12}\\) is contribution of the average adult to the juvenile stage. In addition to fecundity, survival, maturation or growth from one stage to the next, some organisms undergo regression (Fig. 4.4). Regressing means to transition from a later stage to an earlier stage. For instance, and plant can shrink in size do to physical damage, disease or herbivory. A plant can also return temporarily to a non-reproductive stage after a large bout of reproduction. These are examples of regression (Fig. 4.4). One assumption we are making is that individuals set seed, or give birth, all at once. Therefore, we refer to our model as a birth-pulse model. On the other hand, if we assume that we have continuous reproduction throughout the year, we do things differently, and would refer to this as a birth-flow model. Whether a population is breeding continuously over a year, or whether reproduction is seasonal, will influence how we estimate fecundities. Even for synchronously breeding populations, many models pool years into a single age class or stage. The interested reader should consult an authoritative text such as Caswell (2001). Figure 4.4: A transition matrix, in which each element in the matrix describes the probability that an individual of a given size \\(j\\) at time \\(t\\) appears as size \\(i\\) and time \\(t+1\\). Reproduction typically results in the minimum size, stage, or age. This matrix may be composed of a small number of rows and columns (2-10), or, in the case of integral projection, an infinite number of rows and columns. The number of rows is equal to the number of columns. A life cycle graph (Figs. 4.2, 4.3) and the corresponding transition matrix (4.1) constitute our model. The matrix A for our structured population is directly analogous to \\(\\lambda\\) for our unstructured model of discrete population growth in the previous chapter.26 Later, we will project the population in an analogous way, using \\[\\mathbf{ N_{t+1} = A N_t}\\] and to do that, we need a refresher on matrix multiplication. 4.2 A brief primer on matrices We refer to matrices by their rows and columns. A matrix with three rows and one column is a \\(3 \\times 1\\) matrix (a ``three by one’’ matrix); we always state the number of rows first. Matrices comprise elements; an element of a matrix is signified by its row and column. The element in the second row and first column is \\(a_{21}\\). The dimension of a matrix is its number of rows and columns. To add two matrices, they must have the same dimensions. Consider two matrices, A and B . To add these two matrices we simply add the elements in the same row and column of both matrices, as below. \\[\\begin{align*} \\mathbf{A} &amp;= \\left( \\begin{array}{cc} a &amp; b \\\\ c &amp; d \\end{array} \\right); \\; \\mathbf{B} = \\left(\\begin{array}{cc} m &amp; o\\\\ n &amp; p \\end{array}\\right)\\\\ \\mathbf{A+B} &amp;= \\left( \\begin{array}{cc} \\left( a+m \\right) &amp; \\left(b+o \\right)\\\\ \\left(c+n \\right) &amp; \\left(d+p \\right) \\end{array} \\right) \\end{align*}\\] Multiplying matrices is a little more complicated. To do so, we mutliply elements and then sum them: multiply each row element of the first matrix (A) times each column element of the second matrix (B), sum the element-wise products, and place this sum in the respective element of the final matrix. This process is what we refer to as a dot product or sometimes inner product. When we have two vectors of equal length \\(x\\) and \\(y\\) the dot product is \\[x \\cdot y = x_1y_1 + x_2y_2 + \\ldots + x_n y_n\\] A dot product begins with two equal length vectors and returns a single number (i.e. a scalar). Consider the matrix multiplication in (4.2). We first multiply each element of row 1 of A (\\(a\\; b\\)), times the corresponding elements of column 1 of B (\\(m\\; n\\)), sum these products, and place the sum in the first row, first column of the resulting matrix. We then repeat this for each row of A and each column of B. \\[\\begin{align} \\tag{4.2} \\mathbf{AB} &amp;= \\left( \\begin{array}{cc} \\left( am + bn \\right) &amp; \\left(ao+bp \\right)\\\\ \\left(cm + dn \\right) &amp; \\left(co + dp \\right) \\end{array}\\right) \\end{align}\\] To do this, the number of columns in the first matrix must equal the number of rows in the second matrix. It also means that the resulting matrix will have the same number of rows as the first matrix, and the same number of columns as the second matrix. Multiplying a \\(2 \\times 2\\) matrix by a \\(2 \\times 1\\) results in a \\(2 \\times 1\\). Multiplying a \\(3 \\times 3\\) matrix by a \\(3 \\times 1\\) results in a \\(3 \\times 1\\). We cannot multiply a \\(2 \\times 1\\) matrix by a \\(2 \\times 2\\) because the number of columns in the first matrix (1) does not match the number of rows in the second matrix (2). Let’s define two \\(2 \\times 2\\) matrices, M and N, filling in one by rows, and the other by columns. (M &lt;- matrix( 1:4 , nrow=2, byrow=TRUE)) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 (N &lt;- matrix( c(10, 20, 30, 40), nrow=2)) # byrow=FALSE is the default. ## [,1] [,2] ## [1,] 10 30 ## [2,] 20 40 Adding these matrices is simple. Here we do the first element by hand, and then sum the matrices all at once. # 1 + 10 M[1,1] + N[1,1] ## [1] 11 M + N ## [,1] [,2] ## [1,] 11 32 ## [2,] 23 44 To mulitply M and N, we multiply and then sum the first row of \\(M\\) by the first column of \\(N\\), and make this element \\(a_{11}\\) of the resulting matrix product. # 1*10 + 2*20 M[1,1] * N[1,1] + M[1,2] * N[2,1] ## [1] 50 This is the dot product. In R, we must use %*% to signify that we mean matrix multiplication. M %*% N ## [,1] [,2] ## [1,] 50 110 ## [2,] 110 250 If we multiply M times a \\(2 \\times 1\\) matrix D, what should we get? D &lt;- matrix(c(100, 200), nrow=2) M %*% D ## [,1] ## [1,] 500 ## [2,] 1100 # note that we cannot perform D %*% M Make sure you could write out the multiplication and summation for each element in the resulting matrix. The transpose of M is \\(\\mathbf{M^T}\\) M; t(M) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 We use the transpose of A to calculate reproductive value, later in the chapter. 4.3 Decomposing A A slightly different way to conceptualize our transition matrix is to consider it the three separate matrices, one each for survival, growth, and fecundity, \\[\\mathbf{ A = SG + F }\\]. Survival, S, and growth, G, havethese elements: each stage \\(j\\) has its own probability of survival \\(s_j\\), which describes survival of each stage from \\(t\\) to \\(t+1\\). each survivor in stage \\(j\\) will grow or regress into a different stage, or remain in the same stage, with probability \\(g_{ij}\\). In a demographic context, fecundity, F, is the surviving number of offspring produced by the average individual in stage \\(i\\). Sometimes this is a complete black box, in which we can only count the adults in year \\(t\\) and the offspring in year \\(t+1\\). Other times, we can use estimates of the probability that an individual in stage \\(i\\) reproduces at all, the average number of offspring of an individual that does actually reproduce, and the survival and growth of the adults or the resulting offspring. It all depends on the detail we have of our study system. And, of course, it depends heavily on whether we census our population shortly before reproduction (pre-breeding census), or shortly after reproduction (post-breeding census). For our Western Toad (Fig. 4.2), this results in \\[\\begin{equation*} \\tag{4.3} \\mathbf{A = SG + F} = \\left( \\begin{array}{c} \\sigma_j\\\\ \\sigma_a \\end{array} \\right) \\left( \\begin{array}{cc} 1-\\mathrm{P} &amp; 0 \\\\ \\mathrm{P} &amp; 1 \\end{array} \\right) + \\left( \\begin{array}{cc} 0 &amp; \\rho \\phi \\sigma_e \\sigma_t \\sigma_m \\\\ 0 &amp; 0 \\end{array} \\right) \\end{equation*}\\] where \\(\\sigma_j,\\,\\sigma_a\\) are survival of juveniles and adults, and P is the probability of juvenile growth, given survival. If an adult survives it always remains an adult (\\(g_{22}=1\\)) and never regresses (\\(g_{12}=0\\)). Fecundity occurs only in adults, and it is the product of the sex ratio (\\(\\rho\\)), average clutch size per female (\\(\\phi\\)), egg survival (\\(\\sigma_e\\)), survival of tadpoles (\\(\\sigma_t\\)), and the overwintering survival of the metamorphs (\\(\\sigma_m\\)). Following the rules of matrix multiplication and addition, we now have Vonesh and de la Cruz went further and added density-dependence using a term for the negative effects of high tadpole density on tadpole survival and growth. We do not show that here, but regardless how complex the natural history gets, we can build natural history into our models. \\[\\begin{equation*} \\tag{4.4} \\mathbf{SG} = \\left( \\begin{array}{cc} \\sigma_j (1-\\mathrm{P}) &amp; 0 \\\\ \\sigma_j \\mathrm{P} &amp; \\sigma_a \\end{array} \\right) \\; ; \\; \\mathbf{A} = \\left( \\begin{array}{cc} \\sigma_j (1-\\mathrm{P}) &amp; \\rho \\phi \\sigma_e \\sigma_t \\sigma_m \\\\ \\sigma_j \\mathrm{P} &amp; \\sigma_a \\end{array} \\right) \\end{equation*}\\] 4.4 A three stage model Now we describe a three-stage model of a plant population. Like our amphibian example, it is a pre-breeding model, relying on a pre-breeding census. Nonetheless, it includes a seed stage for the seed bank, where seeds may be more than one year old. Eleanor Pardini et al. (n.d.) and colleagues modeled garlic mustard (Alliaria petiolata), a biennial plant species that is an exotic invasive species in the eastern deciduous forest of the U.S. The stages they choose to represent were those present in May: seeds in the soil seed bank, 1–2 month old immature rosettes, and adults (Fig. @fig:garlic1). Figure 4.5: The life cycle of garlic mustard using a post-breeding census (Pardini et al. 2009). The census takes place in May of each year. Each arrow represents the transition from May to May. Seeds germinate in early spring and become rosettes (basal leaves near the soil surface). The rosettes experience mortality all summer, fall, and winter. Surviving rosettes become reproductive adults the following spring a summer. Adults flower and are pollinated in June, after which the fruits ripen and seeds mature. Seeds overwinter for at least six months before germinating in the spring. Not all seeds germinate, but they may remain viable in the seed bank for several years. Thus, the complete life cycle at least two years. Once the seeds germinate, the plant requires over a year to reach maturity, and produce flowers, fruits and seeds. Let’s work through these probability transitions. \\(s_1\\), a germinated seed survives as a rosette. \\(s_2\\), surviving from May to August as a rosette. \\(s_3\\), surviving from August to early May and becoming a reproductive plant. \\(v\\), a seed is viable (survives and can germinate). \\(g_1\\), a viable seed germinates in the first season, or \\(1-g_1\\) remains un germinated \\(g_2\\), a viable seed germinates in the second season or \\(1-g_2\\) does not. Fecundity, \\(f\\), is the average number of seeds per reproductive plant. The transition matrix A would thus be \\[\\begin{equation*} \\tag{4.5} \\mathbf{A} = \\left( \\begin{array}{ccc} 1-g_2 &amp; 0 &amp; v(1-g_1)f \\\\ g_2 s_1 &amp; 0 &amp; v g_1 s_1 f \\\\ 0 &amp; s_2 s_3 &amp; 0 \\end{array} \\right) \\end{equation*}\\] Put into your own words each of the transition elements. What about the transition from adult to rosette? Did the plant shrink? While perennial plants can get smaller, or regress, that is not what happens here. In this transition, the adult in May gets pollinated, develops fruits, the seeds mature and are deposited on the soil late that summer or fall. Those seeds are survive overwinter, germinate in early spring, and grow into rosettes that summer, and survive until the next census in May. In that way, stage 3 (adult) contributes to stage 2 (rosette) through reproduction plus survival and growth. The transition from adult to seed, \\(p_{13}\\), occurs only when the seeds do not germinate after the first winter, but spend another year in the seed bank in the soil. Once we have the transition matrix, we can use it to characterize many features of the population, including the finite rate of increase (\\(\\lambda\\)), the predicted relative abundances of the various stages, and the relative importance of each separate transition \\(p_{ij}\\) for the long term population growth rate. We will do this in a later section, but first will explore projection. It is frequently useful to acutally project the population, one year at a time, into the future. 4.5 Projection Projection is the modeling of a population through time, for prediction under one or another set of assumptions. In practice, we use matrix multiplication to project stage- or structured populations. Matrix multiplication does all these calculations for us. We let A be our square demographic transition matrix, with one row and one column for each stage. Let \\(\\mathbf{N}_t\\) be a one-column matrix of stage sizes, with one row for each stage. Matrix multiplication allows us to project the population, \\(\\mathbf{A}\\mathbf{N}_t = \\mathbf{N}_{t+1}\\). To project a population for multiple years, we use a for-loop. We used this in the previous chapter for an unstructured population. Here we multiply our transition matrix by the current year’s abundances projecting next year’s abundances. All we need to specify are the transition matrix, starting stage abundances for \\(t=0\\), and the number of years through which we want to project. Here we define a transition matrix, \\(\\mathbf{N}_0\\), and a numbers of time steps to project. A &lt;- matrix( c(.1, 2.0, .3,.4), nrow=2, byrow=T) N0 &lt;- matrix(c(100, 1), nrow=1) years &lt;- 6 To do the for-loop, we need an zeroes matrix to hold \\(n\\) for each of the years of each of the stages, including for our first year. We start by filling our matrix with zeroes and “binding” the rows in \\(\\mathbf{N}_0\\) onto the top of our zeroes matrix as the first row. N.proj1 &lt;- matrix( 0, nrow=years, ncol=nrow(A)) colnames(N.proj1) &lt;- c(&quot;Juv&quot;, &quot;Adult&quot;) N.proj2 &lt;- rbind(N0, N.proj1) Now we perform the iteration with the for-loop and plot the result. Note how we do the multiplication for the current year \\(t\\) and put the result in the next year \\(t+1\\). # Project, then... for(t in 1:years) {N.proj2[t+1,] &lt;- A%*%N.proj2[t,]} # ...rearrange and plot N.proj.data &lt;- data.frame(Year=0:years, N.proj2) npd &lt;- gather(N.proj.data, Stage, Abundance, -Year) ggplot(npd, aes(Year, Abundance, linetype=Stage)) + geom_line() Figure 4.6: Projection of a population showing transient dynamics. In the first seven years, we see the abundances of the two stages bounce around. These are transient dynamics that, in our density-independent models, will fade away over time. 4.6 Analyzing the transition matrix Projection is very important for many reasons, especially for stochastic models or for very complicated models. However, we also get some of our best insights through direct analysis the transition matrix using eigenanalysis (Caswell 2001). The features we learn about a structured population using eigenanalysis are best thought of as predictions, attractors, or long term averages, assuming that the transition elements don’t change. This is a big assumption, but understanding it helps us interpret the analysis appropriately. Once you have obtained the transition matrix, \\(\\mathbf{A}\\), you can analysis it using eigenanalysis to estimate \\(\\lambda\\), the finite rate of increase, stable stage structure, reproductive value, and sensitivities and elasticities. Below, we explain each of these quantities. 4.6.1 Eigenanalysis Eigenanalysis is a mathematical technique that summarizes multivariate data. Ecologists use eigenanalysis frequently, for (i) multivariate statistics such as ordination, (ii) local stability analyses with two or more species, and (iii) analyzing population transition matrices. Eigenanalysis is simply a method to transform a square matrix into independent, or orthogonal, pieces. These pieces are eigenvectors and their corresponding eigenvalues. There are the same number of eigenvalues (and eigenvectors) as there are columns in a matrix. In demography, the two most useful pieces are the dominant right eigenvalue and its corresponding right eigenvector. Eigenanalysis is a technique that finds all the solutions for \\(\\lambda\\) and \\(\\mathbf{w}\\) of \\[\\begin{equation} \\tag{4.6} \\mathbf{Aw}=\\lambda \\mathbf{w} \\end{equation}\\] where \\(\\mathbf{w}\\) is a particular summary of our data. With analysis of a transition matrix, \\(\\mathbf{A}\\) is the projection matrix, \\(\\lambda\\) is an eigenvalue and \\(\\mathbf{w}\\) is an eigenvector. If we write out eq. (4.6) for a \\(2 \\times 2\\) matrix, we would have \\[ \\tag{4.7} \\left( \\begin{array}{ccc} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22}\\\\ \\end{array} \\right) \\left( \\begin{array}[c]{c} w_{11}\\\\ w_{21} \\end{array} \\right) =\\lambda \\left( \\begin{array}[c]{c} w_{11}\\\\ w_{21} \\end{array} \\right) \\] For instance, we can preform eigenanalysis on this transition matrix. (A &lt;- matrix(c(0, .3, 2, .7), nrow=2)) ## [,1] [,2] ## [1,] 0.0 2.0 ## [2,] 0.3 0.7 (eA &lt;- eigen(A)) ## eigen() decomposition ## $values ## [1] 1.2 -0.5 ## ## $vectors ## [,1] [,2] ## [1,] -0.8574929 -0.9701425 ## [2,] -0.5144958 0.2425356 This gives us one eigenvalue per column of our transition matrix (values). Each eigenvalue has a corresponding eigenvector that is the corresponding column of the vectors matrix. Let’s use R to prove to ourselves that eq. (4.6) is what we say it is. Here we see for ourselves whether \\(\\mathbf{A}\\mathbf{w} = \\lambda \\mathbf{w}\\) for \\(i = 1\\). Below, we use the results of the previous eigenanalysis. (lambda1 &lt;- eA$values[1]) ## [1] 1.2 (w1 &lt;- eA$vectors[,1]) ## [1] -0.8574929 -0.5144958 cbind(A %*% w1, lambda1 * w1) ## [,1] [,2] ## [1,] -1.0289915 -1.0289915 ## [2,] -0.6173949 -0.6173949 The first column is \\(\\mathbf{Aw}\\) and the second column is \\(\\lambda \\mathbf{w}\\). Sure enough, they look the same. Try doing the same exercise for the second eigenvalue and the second eigenvector, \\(i=2\\). Typically, the first eigenvalue and its corresponding eigenvector capture the most important features of the transition matrix. We call these the dominant eigenvalue, \\(\\lambda_1\\), and its corresponding eigenvector, \\(w_1\\). While first solution does not capture all of the information about our transition matrix, it is the most useful. There are an infinite number of solutions to this equation because solutions can just be simple multiples of the set found with eigenanalysis. Eigenanalysis finds a set in which the solutions are all independent of each other, and which capture all of the information in \\(\\mathbf{A}\\) in a particularly useful way. In this book, we will not delve into details of eigenanalysis beyond this. What is important for us is how we use the results. Below, we describe how we use eigenanalysis to find (i) the long term asymptotic finite rate of increase \\(\\lambda\\), (ii) the stable stage distribution, and the the reproductive values of each stage. 4.6.2 Finite rate of increase The asymptotic annual growth rate or finite rate of increase is the dominant eigenvalue of the transition matrix. Eigenvalues are always referred to with the Greek symbol \\(\\lambda\\), and provide a solution to (4.6). The dominant eigenvalue of any matrix, \\(\\lambda_{1}\\), is the eigenvalue with the largest magnitude, and it is frequently a complex number. (When we perform eigenanalysis, it is common to get complex numbers, with real and imaginary parts. The magnitude is the sum of the squared parts.) With population transition matrices, \\(\\lambda_{1}\\) will always be positive and real. This will not be the case with other types of matrices we examine in later chapters. The dominant eigenvalue is the biggest one. We can find which one it by asking R to tell us the index position \\(i\\) of the largest absolute value (the modulus) of the eigenvalues. In most cases, it is the first eigenvalue, as it is here. ( dom.pos &lt;- which.max( Mod(eA[[&quot;values&quot;]]) ) ) ## [1] 1 We use that index to extract the largest eigenvalue. We keep the real part, using Re(), dropping the imaginary part. (Note that although the dominant eigenvalue of a real transition matrix will always be real, R will include an imaginary part equal to zero (\\(0i\\)) as a place holder if any of the eigenvalues has a non-zero imaginary part). # extract the dominant eigenvalue and retain only its Real part ( L1 &lt;- Re(eA[[&quot;values&quot;]][dom.pos]) ) ## [1] 1.2 L1 is \\(\\lambda_1\\), the aysmptotic finite rate of increase. This finite rate of increase has the same biological meaning as \\(\\lambda\\) in the previous chapter. 4.6.3 Stable stage distribution The relative abundance of the different life history stages is called the stage distribution, that is, the distribution of individuals among the stages. A property of a stage structured population is that, if all the demographic rates (elements of the transition matrix) remain constant, its stage structure will approach a stable stage distribution, a stage distribution in which the relative number of individuals in each stage is constant. Note that a population can grow, so that the absolute number of individuals increases, but the relative abundances of the stages is constant; this is the stable stage distribution. If the population is not actually growing (i.e. \\(\\lambda=1\\)) and demographic parameters remain constant, then the population is stationary and will achieve a stationary stage distribution, where neither absolute nor relative abundances change. How do we find the stable stage distribution? It also turns out that \\(w_1\\) provides the necessary information. We scale the eigenvector \\(w_1\\) by the sum of its elements because we are interested in the distribution, which is defined by all stages summing to one. Therefore the stable stage distribution is \\[\\begin{equation} \\tag{4.8} \\frac{w_1}{\\sum_{i=1}^s{w_1}} \\end{equation}\\] where \\(s\\) is the number of stages. Once a population reaches its stable stage distribution each of the stages grow exponentially, because \\[\\mathbf{Aw}=\\lambda \\mathbf{w}\\] 4.6.4 Calculating the stable stage distribution The dominant eigenvector, \\(w_1\\), is in the same position as the dominant eigenvalue. We extract \\(w_1\\), keeping just the real part, and divide it by its sum to get the stable stage distribution. # continuing from previous code... w1 &lt;- Re(eA[[&quot;vectors&quot;]][,dom.pos]) ssd &lt;- w1/sum(w1) round(ssd, 3) ## [1] 0.625 0.375 This shows us that if the transition matrix does not change over time, the population will eventually be composed of these relative abundances. We have claimed, without evidence, that with a constant transition matrix, the projected population will eventually reach a stable stage distribution and grow exponentially with a finite rate of increase of \\(\\lambda_1\\). Here we show an example. A &lt;- matrix(c(0, .3, 2, .7), nrow=2) # our tran. matrix N0 &lt;- c(Juveniles=1,Adults=10) # initial population steps &lt;- 8 # number of time steps # combine the stages of our initial population, and a zero matrix # with a column for each stage and a row for each time step N &lt;- rbind(N0, matrix(0, ncol=2, nrow=steps) ) # use a for-loop to project the population each year and store it. for(t in 1:steps) {N[t+1,] &lt;- A%*%N[t,]} # Sum the stages to get the total N N.total &lt;- rowSums(N) # For each year, divide each stage by the total to get # relative abundances and thus the distribution. proportions &lt;- N/N.total SD &lt;- data.frame(Year=0:steps, proportions) sdg &lt;- gather(SD, Stage, Proportion, -Year) # plot the distributions for succeeding years. ggplot(sdg, aes(Stage, Proportion)) + geom_col() + facet_wrap(~Year, nrow=3, ncol=3) Using total population sizes, we can calculate an annual rate of increase \\(N_{t+1}/N_t\\). # using &quot;-&quot; in an index removes that element lambda.t &lt;- N.total[-1]/N.total[-(steps+1)] qplot(x=1:steps, y=lambda.t, geom=c(&quot;line&quot;, &quot;point&quot;)) + annotate(&quot;point&quot;, x = steps , y=L1, pch=1, size=3) Figure 4.7: Calculating an annual finite rate of increase from a projected population shows that the population will approach asymptotically a constant geometric rate of increase. Solid points are annual growth rate; the circular point is lambda(1) from eigen analyysis. Iterating the transition matrix to approximate \\(\\lambda_1\\) and \\(w_1\\) is actually called the power iteration method for eigenanalysis. 4.6.5 Reproductive value If the stage structure gives us one measure of the importance of a stage (its relative abundance), then the reproductive value gives us one measure of the importance of an individual in each stage. Reproductive value is the expected contribution of each individual to future reproduction. We characterize all individuals in a stage using the same expected reproductive value. We find each stage’s reproductive value by solving for the dominant left eigenvector \\(\\mathbf{v}\\), where \\[\\begin{equation} \\tag{4.9} \\mathbf{vA}=\\lambda\\mathbf{v} \\end{equation}\\] Like the relation between the dominant right eigenvector and the stable stage distribution, this vector is actually proportional to the reproductive values. Unlike the stable stage distribution, we scale it so that all reproductive values are relative to that of the first stage (e.g. juveniles or seeds). \\[ \\tag{4.10} \\frac{v}{v_{1}} \\] We find the left eigenvalues and -vectors by performing eigenanalysis on the transpose of the transition matrix. Transposition flips rows and columns. We perform eigenanalysis, extracting just the dominant left eigenvector; we then scale it, so the stage 1 has a reproductive value of 1.0. etA &lt;- eigen(t(A)) dom.pos &lt;- which.max(Mod(etA$values)) v1 &lt;- Re(etA$vectors[,dom.pos]) ( rv &lt;- v1/v1[1] ) ## [1] 1 4 Here we see that reproductive value, \\(rv\\), increases with age or stage. This means that the expected reproductive value of an individual in the second stage is 4 times as great as that of an individual in the first stage. In general, reproductive value of individuals in a stage increases with increasing probability of reaching fecund stages. 4.6.6 Sensitivity and elasticity Sensitivity and elasticity tell us the relative importance of each transition (i.e. each arrow of the life cycle graph or element of the matrix) in determining \\(\\lambda\\). They do so by combining information on the stable stage structure and reproductive values. The stage structure and reproductive values each in their own way contribute to the importance of each stage in determining \\(\\lambda\\). The stable stage distribution provides the relative abundance of individuals in each stage. Reproductive value provides the expected contribution to future population growth of individuals in each stage. Sensitivity and elasticity combine these to tell us the relative importance of each transition in determining \\(\\lambda_1\\). Almost always, sensitivities of a transition matrix are the sensistivities of \\(\\lambda_1\\) to the elements of the transition matrix. They are the direct contributions of each transition to determining \\(\\lambda_1\\). The sensitivity for the element \\(a_{ij}\\) of a transition matrix is the change in \\(\\lambda_1\\) that occurs when we change \\(a_{ij}\\) a small amount, or \\(\\delta \\lambda / \\delta a_{ij}\\). It isn’t surprising, then, these are derived from the stable stage distribution and the reproductive values. Specifically, the sensitivities are calculated as \\[ \\frac{\\delta \\lambda}{\\delta a_{ij}}=\\frac{v_{i}w_{j}}{v\\cdot w} \\tag{4.11} \\] where \\(v_{i}w_{j}\\) is the product of each pairwise combination of elements of the dominant left and right eigenvectors, \\(v\\) and \\(w\\). Specifically, the numerator is generated by the reproductive value of the target stage and the stable stage distribution of the source stage, \\[ \\begin{pmatrix} v_1\\\\v_2 \\end{pmatrix} \\begin{pmatrix} w_1 &amp; w_2 \\end{pmatrix}= \\begin{pmatrix} v_1 w_1 &amp; v_1 w_2 \\\\ v_2 w_1 &amp; v_2 w_2 \\end{pmatrix} \\] In the denominator, the dot product, \\(\\mathbf{v} \\cdot \\mathbf{w}\\), is the sum of the pairwise products of each vector element. Dividing \\(v_{i}w_{j}\\) by this sum causes the sensitivities to be relative to the magnitudes of \\(v\\) and \\(w\\). Let’s calculate sensitivities now. Because the stable stage distribution and the reproductive values are merely scaled versions of the dominant right and left eigenvectors, it doesn’t matter whether we use the unscaled eigenvectors, or the scaled stable stage distribution and reproductive values. vw &lt;- matrix(rv, nr=2, nc=1) %*% matrix(ssd, nr=1, nc=2) # or vw &lt;- v1 %*% t(w1); the numerator dot.prod &lt;- sum(rv*ssd) # or dot.prod &lt;- sum(v1 * w1) denominator (s &lt;- vw/dot.prod) ## [,1] [,2] ## [1,] 0.2941176 0.1764706 ## [2,] 1.1764706 0.7058824 These are the sensitivities, \\(\\delta \\lambda / \\delta a_{ij}\\), of each corresponding transition element. You will always get a sensitivity for every position in this matrix, even when the transition is zero, \\(a_{ij}=0\\). We just ignore those, but by convention, include them. These sensitivities are the relative change in \\(\\lambda_1\\) for an absolute change in the element. For instance, it is the relative effect of increasing \\(a_{21}\\) from 0.3 to 0.33, or \\(a_{22}\\) from 0.7 to 0.73. The largest of these sensitivities is that for is \\(p_{21}\\), surviving from the first stage to the second stage. This means that small changes to \\(p_{21}\\) have large effects on \\(\\lambda_1\\) than do small changes to \\(F_2\\) or \\(p_{22}\\). As \\(\\delta \\lambda / \\delta a_{ij}\\), the sensitivities are the slope of the line relating the magnitude of \\(\\lambda_1\\) to the matrix element. Figure 4.8: Sensitivities of lambda to transition elements are slopes. The dotted line is the calculated sensitivity of lambda to A[2,1], because it is the slope evaluated at A[2,1]. Elasticities are sensitivities that have been weighted by the transition probabilities. We may, however, be interested in how a proportional change in a transition element influences \\(\\lambda_1\\)—how does a 10% increase in seed production, or a 10% decline in juvenile survival influence \\(\\lambda_1\\)? For these answers, we need to adjust sensitivities to account for the relative magnitudes of the transition elements, and this provides the elasticities, \\(e_{ij}\\). Elasticities are relative sensitivities, and are defined as \\[ e_{ij}=\\frac{\\delta \\lambda/\\lambda}{\\delta a_{ij}/a_{ij}} = \\frac{a_{ij}}{\\lambda}\\frac{\\delta \\lambda}{\\delta a_{ij}}. \\] These are also equal to \\[e_{ij}= \\frac{\\delta \\log \\lambda}{\\delta \\log a_{ij}}.\\] Like sensitivities, we can think of elasticities as slopes. Calculating these in R is easy. e &lt;- (A/L1) * s (round(e, 3)) ## [,1] [,2] ## [1,] 0.000 0.294 ## [2,] 0.294 0.412 Now we see that survival by adults (\\(p_{22}\\)) has the biggest effect on \\(\\lambda_1\\). Why the difference between sensitivities and elasticities? Because the elasticities reflect the effect of a proportional change (e.g., 1%) of the elements, whereas sensitivities reflect the effect of a change by a constant amount (e.g., 0.01) of each element. Note that elasticities are relative to each other, in that they sum to 1.0. This is an especially nice features of elasticities because it makes it easier to compare elasticities among differ matrices and even different organisms. Once we have the sensitivities and elasticities, we can really begin to see what is controlling the growth rate of a stage (or age) structured population. Although these values do not tell us which stages and transitions will be influenced by natural phenomona or management practices, they provide us with the predicted effects on \\(\\lambda_1\\) of a proportional change in a demographic rate. This is particularly important in the management of invasive (or endangered) species where we seek to have the maximum impact for the minimum amount of effort and resources (Caswell 2001; Ellner and Guckenheimer 2006). All of these details can get very confusing, and smart people don’t always get it right. Therefore, get expert advice (Caswell 2001; Ellner and Guckenheimer 2006), and remember that the stages of life cycle graph and matrix are the stages that you collect at one point in time, and an arrow or transition element has to include everything that happens from one census period to the next. What do we do if our stages seem like arbitrary categories along a continuous scale? What if size is easy to measure accurately and is a really good predictor of demographic rates, and these rates vary continuously with size? How do we decide on size classes? How do we draw seemingly arbitrary divisions between different sizes? With integral projection, as opposed to matrix projection, we no longer have to worry about a particular number of stages or age classes. We select an individual-level state variable, such as body mass, length, stem diameter, or even location, that is a useful predictor of demographic rates. We use this continuously varying state variable in place of stages or age classes. We rely on statistical methods, such as linear regression, to describe the relations between the individual-level state variable and survival, growth, and fecundity. We then combine those relations to model how individuals in a population are likely to change from one generation to the next. In many organisms, size is often a relatively good predictor of survival and fecundity. Within a poulation, size is often associated with age, learning, and resource acquisition. Size is also related to the onset of reproductive maturity and initiation of specific reproductive structures. Among reproductive individuals, size is often strongly correlated with per capita reproductive output. Integral projection can use size to model size-dependent demography. Whereas matrix projection models use matrix algebra to project populations. Matrix algebra is mutliplication of discrete classes and summation of discrete products. Integral projection models (IPMs) use multiplication and the calculus of integration across continuous variables toward the same end. 4.7 R packges for demography Caswell (2001) is a definitive reference text for matrix models. Ellner and Guckenheimer (2006) provides an excellent introduction, and I am sure there are many other excellent texts as well. Stubben and Milligan (2007) and Stott, Hodgson, and Townley (2018), and de la Cruz (2019) provide R packages for stage-based matrix models with methods that go well beyond this text. In addition, there are several other packages for analyzing life tables and human demography. Ellner, Childs, and Rees (2016) is probably the best source for understanding IPMs. The R package IPMpack (Metcalf et al. 2014) is an excellent package to implement and analyze IPMs. 4.8 Exploring a real population Crouse, Crowder, and Caswell (1987) performed a demographic analysis of an endangered sea turtle species, the loggerhead (Caretta caretta). Management of loggerhead populations seemed essential for their long term survival, and a popular management strategy had been and still is to protect nesting females, eggs, and hatchlings. The ground breaking work by Crouse27 and her colleagues compiled data to create a stage-based projection matrix to analyze quantitatively which stages are most important and least important in influencing long-term growth rate. This work led to US Federal laws requiring that US shrimp fishermen use nets that include Turtle Excluder Devices (TEDs, https://www.fisheries.noaa.gov/southeast/bycatch/turtle-excluder-device-regulations ). Crouse et al. determined a transition matrix, A, for their loggerhead population: \\[\\begin{array}{c} H\\\\J_s\\\\J_l \\\\sub \\\\ B_1 \\\\B_2 \\\\ M \\end{array} \\left( \\begin{array}{cccccccc} 0&amp; 0&amp; 0&amp; 0&amp; 127&amp; 4&amp; 80\\\\ 0.6747&amp; 0.7370&amp; 0&amp; 0&amp; 0&amp; 0&amp; 0\\\\ 0&amp; 0.0486&amp; 0.6610&amp; 0&amp; 0&amp; 0&amp; 0\\\\ 0&amp; 0&amp; 0.0147&amp; 0.6907&amp; 0&amp; 0&amp; 0\\\\ 0&amp; 0&amp; 0&amp; 0.0518&amp; 0&amp; 0&amp; 0\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 0.8091&amp; 0&amp; 0\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 0&amp; 0.8091&amp; 0.8089 \\end{array} \\right)\\] Draw by hand two different types of life cycle graphs for this loggerhead population. Include the matrix elements associated with each transition. Use eigenanalysis to determine \\(\\lambda_1\\). Explain what this tells us about the population, including any assumptions regarding the stable stage distribution. Use eigenanalysis to determine the stable stage distribution. Use eigenanalysis to determine the elasticities. Which transition(s) are most influential in determining growth rate? What is the predicted long-term relative abundance of all stages? What do we call this? If your interest is to maximize long-term growth rate, in which stage(s) should you invest protection measures? Which stages are least likely to enhance long-term growth rate, regardless of protective measures? Start with \\(\\mathbf{N} = \\left(1000\\,10\\,10\\,10\\,10\\,10\\,10\\right)\\) and graph dynamics for all stages for 20 years. References "],
["references.html", "5 References", " 5 References "]
]
