

The function first makes a new vector containing $N_0$, uses a for-loop to implement eq. \@ref(eq:3dd) for each time step, and then returns the vector $N$.


With discrete logistic growth, if we start with a small population ($N<<K$), we will see the population rise and gradually approach $K$ or $1/\alpha$ (Fig. \@ref(fig:dlg)). We refer to $K$ as an *\index{attractor}attractor* because $N$ moves in a deterministic fashion toward $K$. We explore the meanings of *attractor* and related terms throughout the book.

  \begin{figure}[ht]
    \centering
    \includegraphics[width=.5\linewidth]{DLG1.pdf}
    \caption{Discrete logistic growth with $r_d=1$, $\alpha = 0.01$. }
    \label{fig:dlg}
  \end{figure}

\medskip \noindent
\begin{boxedminipage}{\linewidth}{\footnotesize
\paragraph{Graphing Population Size}
We can use the function created above, `dlogistic`, with default settings, to generate a population projection.
Now we plot the projection, and put in a dotted line for $1/\alpha$ or $K$.
```{r DLG1}
t <- 15; a <- 0.01
Nts <- dlogistic(alpha=a, t=t)
qplot(0:t, Nts) + geom_hline(yintercept =1/a, lty = 3)
```

### Relations between growth rates and density
We already know a lot about one description of density dependent growth, the discrete logistic model. In particular, we know that with a constant per capita negative effect, $\alpha$,  the population size at which growth falls to zero is $K$. Let us explore further how the *per capita* growth increment, and the *population* growth increment, vary with population size.

Casual examination of Fig. \@ref(fig:dlg) suggests that the total population growth increment ($\Delta N_t = N_{t+1}-N_t$) starts out small when both $t$ and $N$ are small, accelerates as $N$ grows, and then over time, slows down and reaches an asymptote of $K$. Is this changing rate of population growth a function of time, or a function of density? Let us first consider the growth increment as a function of $N$.

First consider the relation between the population growth increment and population size (Fig. \@ref(pgi)). We see it increase as $N$ grows, and then decrease as $N$ approaches $K$. The pattern is fairly symmetric. That is, it increases and decreases at about the same rates. 

Next consider the per capita growth increment ($\Delta N_t / N_t$; Fig. \@ref(ppgi)). There is a direct linear relation between the per capita growth increment and the size of the population --- this is *linear density dependence*. This linear dependence on $N$ comes from our assumption that the per capita negative effect is a constant, $\alpha$.

\medskip \noindent
\begin{boxedminipage}{\linewidth}{\footnotesize
\paragraph{(Per Capita) Population Growth Increment $vs$. $N$ (Fig. \ref{fig:DGI1})}
Using the previous projection, we now capture both the total and the per capita growth increment per unit time, from $t$ to $t+1$. We graph these versus $N_t$, population size at $t$.
<<>>=
total.incr <- Nts[1:t+1] - Nts[1:t] 
per.capita.incr <- total.incr/Nts[1:t]
<<PGI, fig=true, include=false>>=
plot(Nts[1:t], total.incr)
<<PPGI, fig=true, include=false>>=
plot(Nts[1:t], per.capita.incr)
@ 
}\end{boxedminipage} \medskip

\begin{figure}[ht]
  \centering
  \subfloat[Population Growth Increment]{\includegraphics[width=.47\linewidth]{PGI.pdf} \label{pgi}}
  \subfloat[Per capita Growth Increment]{\includegraphics[width=.47\linewidth]{PPGI.pdf} \label{ppgi}}
  \caption{Relations between the total and per capita discrete growth increments and population size.}
  \label{fig:DGI1}
\end{figure}

Let's use a simple analytical approach to understand Figs. \ref{pgi} and \ref{ppgi} a little better. Take a look at eq. \ref{eq:3dd}. First let's rearrange the expression so that we can set the \emph{increment} of change equal to zero. How would you do that? Take a minute and try it, before you look at the answer below. 

Setting the increment of change equal to zero, we rearrange with the population growth increment on the left, and then set it to zero.
\begin{align}
N_{t+1}-N_{t} &= r_{d}N_{t}\left(1-\alpha N_{t}\right) \label{eq:pgi}\\
N_{t+1}-N_{t} &= r_{d}N_{t} - r_d\alpha N_{t}^2 \label{eq:quad}\\
0 &= r_{d}N_{t} - r_d\alpha N_{t}^2 \label{eq:quad0}
\end{align}
What do we notice? One observation we could make is that we have a \index{quadratic equation}quadratic equation,\footnote{$ax^2+bx+c=0$} where the intercept is zero. This tells us that perhaps Fig. \ref{pgi} is symmetric because it is a quadratic expression in terms of $N$. 

What would satisfy this quadratic expression (eq. \ref{eq:quad0}), that is, cause the growth increment to equal zero? Well, if $r_{d}$ or $N_{t}$ equal zero, those would yield potentially interesting solutions. Assuming neither $r_{d}$ nor $N_{t}$ equal zero, we can divide each side by these, and we are left with the solution we found in eq. \ref{eq:2add3}, that the growth increment will be zero when $N_{t}=\frac{1}{\alpha}=K$.\label{sec:per-capita-popul}

Now let us examine the per capita growth increment (Fig. \ref{ppgi}). If we start with the population growth increment  eq. \ref{eq:pgi}, all we need is to divide through by $N_t$ to get
\begin{equation}
\frac{N_{t+1}-N_{t}}{N_t} = r_{d} - r_d\alpha N_{t}. \label{eq:ppgi}
\end{equation}
With $r_d$ and $\alpha$ being constants, and $N$ varying, what is this expression? It is the expression for a straight line,\footnote{$y=mx+b$.} just like we observe (Fig. \ref{ppgi}). When $N_t=0$, the per capita increment equals $r_d$, and when $N_t=1/\alpha$, the per capita increment is zero. This is precisely where we started when we began the motivation for discrete logistic growth. 

\subsection{Effect of initial population size on growth dynamics}
What will be the effect of differences in initial population size? 
We could approach such a question in at least two ways \cite{Case2000}. 
For some of us, the simplest way is to play games with numbers, trial and error, plugging in a variety of initial population sizes, and see what happens to the dynamics. If we do this systematically, we might refer to this as a simulation approach. For very complicated models, this may be the \emph{only} approach.  Another approach, which is often used in concert with the simulation approach, is the analytical approach. We used this above, when we set the growth equation equal to zero and solved for $N$. In general, this analytical approach can sometimes give us a definitive qualitative explanation for \emph{why} something happens. This has been used as a justification for using simple models that actually have analytical solutions --- they can provide answers \cite{May2001}.

For an analytical approach, first consider the endpoint solutions to the discrete logistc model eq. \ref{eq:quad}. The population will stop changing when $N_t=K$. Note that it does not appear to matter what the initial population size was. The only thing that matters is $\alpha$. Recall also that the population would not grow if for any reason $N_t=0$ --- the population will be stuck at zero. Based on these analyses, it appears that the only thing that matters is whether the initial population size is zero, or something greater than zero. If the latter, then initial population size appears to have no effect on the eventual population size.

It always pays to check our analytical answer with a brute force numerical approach, so we will use a little simple simulation to see if we are right. In this case, we can vary systematically the initial population size, and see what happens (Fig. \ref{fig:init}). What our approach shows us is that regardless of the initial conditions (except zero), $N$ converges on $K$ --- $K$ is an attractor. We also might notice that sometimes when $N_0>K$, it crashes below $K$ before converging on $K$ in the end (more on that later). Last, because there is a qualitative shift in the behavior of the population when $N=0$, we might want to investigate what happens when $N$ gets very very close to zero. However, in this situation, the analytical solution is so straightforward that it seems convincing that as long as $N>0$, it will grow toward $K$.

\begin{figure}[ht]
  \centering
  \subfloat[Variation in $N_0$]{\includegraphics[width=.48\linewidth]{DLGinit.pdf} \label{fig:init}}
  \subfloat[Variation in $\alpha$]{\includegraphics[width=.48\linewidth]{DLGalpha.pdf}\label{fig:a}}
  \caption[Confirmation of analytical solutions for discrete logistic growth]{\subref{fig:init} Dynamics due to different initial $N$ (zero was also specifically included, and $\alpha=0.01$).  \subref{fig:a} Dynamics due to different $\alpha$.   All $N$, except $N=0$, converge on $K=1/\alpha$, regardless of the particular value of $\alpha$ ($r_{d}=1$).}
\end{figure}


\medskip \noindent
\begin{boxedminipage}{\linewidth}{\footnotesize
\paragraph{Numerical Evaluation of Initial Conditions  (Fig. \ref{fig:init})}
Here we draw randomly 30 $N_0$ from a uniform distribution between zero and $1.2K$. We also include zero specifically. We then use \texttt{sapply} to run \texttt{dlogistic} for each $N_0$, using defaults for the other arguments.
<<>>=
N0s <- c(0, runif(30) * 1.1 * 1/a)
N <- sapply(N0s, function(n) dlogistic(N0=n) )
<<DLGinit, fig=TRUE>>=
matplot(0:t,N, type="l", lty=1, lwd=.75, col=1)
text(t, 1/a, expression(italic("K") == 1/alpha), adj=c(1,0))
@ 
A serious simulation might include a much larger number of $N_0$. 
}\end{boxedminipage}
\medskip

\subsection{Effects of $\alpha$}

Our conclusions thus far have been based on specific values of $\alpha$ and $r_{d}$. Have we been premature? Just to be on the safe side, we should probably vary these also.

What will happen if $\alpha$ varies? This seems easy. First, when $N_t$ is zero, the population growth increment eq. \ref{eq:pgi} is zero, regardless of the magnitude of $\alpha$. However, when $N_t > 0$, $N$ will increase until it reaches $1/ \alpha$ ($K$; Fig. \ref{fig:a}).  The outcome seems pretty clear --- by decreasing the negative effect of individuals on each other (i.e. decrease $\alpha$) then the final $N$ increases, and $\alpha$ determines the final $N$.

\medskip \noindent
\begin{boxedminipage}{\linewidth}{\footnotesize
\paragraph{Numerical Evaluation of $\alpha$  (Fig. \ref{fig:a})}
Here we draw 30 random $K$ from a uniform distribution from 50 to 1000, and convert these to $\alpha$. We use \texttt{sapply} to run \texttt{dlogistic} for each $\alpha$.
<<>>=
a.s <- 1/runif(30, min=50, max=1000)
N <- sapply(a.s, function(a) dlogistic(alpha=a, t=15) )
@ 
We next plot all populations, and use some fancy code to add some informative text in the right locations.
<<DLGalpha, fig=TRUE>>=
matplot(0:t, N, type="l", ylim=c(0,1000), lty=1, lwd=.75, col=1)
text(8, 1/min(a.s), bquote(italic(alpha) == .(round(min(a.s),3))), 
     adj=c(1,0.5))
text(10, 1/max(a.s), bquote(italic(alpha) == .(round(max(a.s),3))), 
     adj=c(0,1.2))
@ 
Note that we use the minimum and maximum of \texttt{a.s} to both position the text, and provide the values of the smallest and largest $\alpha$.
}\end{boxedminipage}
\medskip


\subsection{Effects of $r_{d}$} 
What will variation in $r_{d}$ do? \index{logistic growth!effect of $r_d$}Probably nothing unexpected, if our exploration of geometric growth is any guide. Our analytical approach indicates that it should have no effect on $K$ (sec. \ref{sec:per-capita-popul}). Nonetheless, let us be thorough and explore the effects of $r_{d}$ by varying it systematically, and examining the result.

Yikes --- what is going on in Fig. \ref{fig:DDGr1}? Perhaps it is a good thing we decided to be thorough. These wild dynamics are real --- let's go back and look more carefully at $r_{d}$. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=.67\linewidth]{DLGrd.pdf}
  \caption{The variety of population dynamics resulting from different values of $r_d$ for the discrete logistic growth model ($r_d = 1,1.2, \ldots, 3$, $\alpha=0.01$). See Fig. \ref{fig:DDGr2} for a more informative view.}
  \label{fig:DDGr1}
\end{figure}

\medskip \noindent
\begin{boxedminipage}{\linewidth}{\footnotesize
\paragraph{Simple Numerical Evaluation of $r_d$ (Fig. \ref{fig:DDGr1})}
Here we vary $r_d$ by creating a short systematic sequence $r_d = 1.3,1.6, \ldots, 2.8$. We set $t=50$, and use \texttt{dlogistic} to create a trajectory for each of the six $r_d$.
<<>>=
rd.v <- seq(1.3, 2.8, by=.3)
t <- 15
Ns <- data.frame(sapply(rd.v, function(r) dlogistic(rd=r, t=t) ))
<<DLGrd, fig=TRUE>>=
matplot(0:t, Ns, type="l", col=1)
@ 
Note that many populations do not seem to settle down at $K$.
}\end{boxedminipage}
\medskip

If we examine each projection separately, we see a cool pattern is emerging (Fig. \ref{fig:DDGr2}). At the lowest $r_{d}$, the population grows gradually toward its carrying capacity, $K$, and stays there. Once $r_{d}=1.6-1.9$ it  overshoots $K$ just a bit, creating oscillations; these oscillations, however, dampen back down to $K$. When $r_{d}=2.2$, however, the populations seem to bounce back and forth between two values. When $r_d=2.5$, $N$ bounces around, but now it bounces around between four different values.  When $r_{d}=2.8$, however, it seems to bounce back and forth around $K$, but at values that vary every time. This model is just about as simple as a model can be, and includes no random numbers. What is going on?

\begin{figure}[ht]
  \centering
  \includegraphics[width=.9\linewidth]{DDGr2.pdf}
  \caption{A more informative view of the effects of variation in $r_{d}$ on population dynamics.}
  \label{fig:DDGr2}
\end{figure}

\medskip \noindent
\begin{boxedminipage}{\linewidth}{\footnotesize
\paragraph{Presentation of Limit Cycles (Fig. \ref{fig:DDGr2})}
First we make a data frame with the six $r_d$ values in the first column, and the respective populations in rows, using \texttt{t()} to transpose \texttt{Ns}. This puts the data in \emph{wide} format, with a different time step in each column. (This might, for instance, be how you record data in a field experiment with repeated measurements through time).
<<>>=
tmp <- data.frame(rd=as.factor(rd.v), t(Ns))
@
Next, we reshape the data to \emph{long} format, were all $N$ are in the second column, and each is associated with a time step and its $r_d$ value (cols. 3 and 4).
<<results=hide>>=
Ns2 <- reshape(tmp, varying=list(2:ncol(tmp)), idvar="rd", v.names="N", 
               direction="long")
str(Ns2)
@ 
\dots (output omitted) We plot each trajectory separately using \texttt{xyplot} in a different graphics package, \texttt{lattice}. Known as a \emph{conditioning} plot, \texttt{xyplot} graphs  $y~vs.~x$ conditional on $g$ (\texttt{y $\sim$ x | g}).
<<DDGr2, fig=TRUE, width=7, height=4>>=
library(lattice)
print( xyplot(N ~ time|rd, data=Ns2, type="l", layout=c(3,2,1), col=1 ) )
@
}\end{boxedminipage} \medskip


What is going on is the emergence of \emph{\index{stable limit cycles}stable limit cycles}, and \emph{\index{chaos}chaos}.\footnote{Not the evil spy agency featured in the 1960's US television show, \emph{Get Smart}.} At low $r_d$, we have simple asymptotic approach to $K$. As $r_d$ increases, we see the population overshoot the carrying capacity and exhibit \emph{\index{damped oscillations}damped oscillations}. When $2 < r_d < 2.449$, the population is attracted to two-point limit cycles. In this case, \emph{these two points are stable attractors}. Regardless where the population starts out, it is attracted to the same two points, for a given $r_d$. As $r_d$ increases further, the number of points increases to a four-point limit cycle (e.g., at $r_d=2.5$), then an eight-point cycle, a 16-point limit cycle, and so on. These points are stable attractors. As $r_d$ increases further , however, stable limit cycles shift into \emph{chaos} ($r_d>2.57$). Chaos is \emph{a non-repeating, deterministic fluctuating trajectory, that is bounded, and sensitive to initial conditions}.

Robert May \cite{May:1974fk} shocked the ecological community when he first demonstrated stable limit cycles and chaos using this model. His groundbreaking work, done on a hand calculator, showed how very complicated, seemingly random dynamics emerge as a result of very simple deterministic rules. Among other things, it made population biologists wonder whether prediction was possible at all. In general, however, chaos seems to require very special circumstances, including very high population growth.

Is there a biological interpretation of these fluctuations? Consider some simple environment, in which small vegetation-eating animals with high reproductive rates eat almost all the vegetation in one year. The following year, the vegetation will not have recovered, but the animal population will still be very high. Thus the high growth rate causes a disconnect between the actual population size, and the negative effects of those individuals comprising the population. \emph{The negative effects of the actions of individuals (e.g., resource consumption) are felt by the offspring of those individuals, rather than the individuals themselves}. We won't belabor the point here, but it is certainly possible to extend this delayed density dependence to a wide variety of populations. The discrete logistic model has a built in delay, or \emph{\index{time lag}time lag}, of one time step, because the growth increment makes a single leap of one time step. This delay is missing from the analogous continuous time model because the growth increment covers an infinity small time step, thanks to the miracles of calculus.\footnote{A time lag can be explicitly built in to a continuous time model, with only a small effort.}

\subsubsection{Bifurcations}
Up until now, we have examined $N$ as a function of time. We have
graphed it for different $\alpha$ and $N_0$, but time was always on
the X-axis. Now we are going to examine $N$ as a function of $r_d$, so
$r_d$ is on the X-axis. Specifically, we will plot the stable limits
or attractors $vs$. $r_d$ (Fig. \ref{fig:bifurcation}). 
 What does it mean? For $r_{d} < 2$, there is only a single $N$. This is what we mean by a stable point equilibrium, or point attractor --- as long as $r_d$ is small, $N$ always converges to a particular point.\footnote{It need not be the same $N$ for each $r_d$, although in this case it is.} When $2<r_{d}<2.45$, then all of a sudden there are two different $N$; that is, there is a two-point stable limit cycle. Note that when $r_d\approx 2$ these oscilliations between the two point attractors around $K$ are small, but as we increase $r_d$, those two points are farther apart. The point at which the limit cycle emerges, at $r_d=2$, is called a \emph{\index{bifurcation}bifurcation}; it is a splitting of the single attractor into two attractors. At $r_{d}\approx 2.45$, there is another bifurcation, and each the two stable attractors split into two, resulting in a total of  four unique $N$. At $r_{d}\approx 2.53$, there are eight $N$. All of these points are \emph{periodic} attractors\index{attractor!periodic} because $N$ is drawn to these particular points at regular intervals. As $r_d$ increases the number of attractors will continue to double, growing geometrically. Eventually, we reach a point when there becomes an infinite number of unique points, \emph{that are determined by $r_{d}$}.\footnote{They are also determined by the initial $N$, but we will get to that later.} This completely deterministic, non-repeating pattern in $N$ is a property of \emph{chaos}. Chaos is not a random phenomenon; rather it is the result of deterministic mechanisms generating non-repeating patterns.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.67\linewidth]{Bifurcation}
  \caption{Illustration of the long term dynamics of discrete logistic population growth. When a small change in a continuous parameter results in a change in the number of attractors (e.g. a single point equilibrium to a stable 2-point limit cycle), we call this a bifurcation.}
  \label{fig:bifurcation}
\end{figure}

\medskip \noindent
\begin{boxedminipage}{\linewidth}{\footnotesize
\paragraph{Bifurcation Plot: Attractors as a Function of $r_d$ (Fig. \ref{fig:bifurcation})}
Here we perform more comprehensive simulations, and plot the point and periodic attractors \emph{vs}. $r_d$. First we pick some constraints for the simulation: the number of different $r_d$, the sequence of $r_d$ values, and the number of time steps.
<<eval=true>>=
num.rd <- 201
rd.s <- seq(1,3, length=num.rd)
t <- 400
<<echo=false, results=hide, eval=FALSE>>=
num.rd <- 1001
rd.s <- seq(1,3, length=num.rd)
t <- 1000
@ 
Next we use \texttt{sapply} for the simulations (If \texttt{num.rd} and \texttt{t} are large, this could take a while).
<<>>=
tmp <- sapply(rd.s, function(r) dlogistic(rd=r, N0=99, t=t) ) 
@ 
Next we convert the output to a data frame and stack up the $N$ in one column. We also rename each of the stacked columns, and add new columns for the respective $r_d$ and time steps.
<<>>=
tmp.s <- stack( as.data.frame(tmp) )
names(tmp.s) <- c("N", "Old.Column.ID")
tmp.s$rd <- rep(rd.s, each=t+1)
tmp.s$time <- rep(0:t, num.rd)
@  
We save just the later dynamics in order to focus on the $N$ after they have converged on the periodic attractors. Here we select the last 50\% of the time steps. (Your figure will look a little different than Fig. \ref{fig:bifurcation} because I used more $r_d$ and time steps.)
<<Bifurcation,fig=TRUE>>=
N.bif <- subset(tmp.s, time > 0.5 * t )
plot(N ~ rd, data = N.bif, pch=".", xlab=quote("r"["d"]))
@ 
}\end{boxedminipage} \medskip

There has been a great deal of effort expended trying to determine whether a particular model or real population exhibits true \index{chaos}chaos. In any practical sense, it may be somewhat unimportant whether a population exhibits true chaos, or merely a higher order periodic attractor \cite{Ellner:2005fk}. The key point here is that very simple models, and therefore potentially simple mechanisms, can generate very complex dynamics.

@
\subsubsection{Sensitivity to initial conditions}
Another very important characteristic feature of chaotic populations is that they are very sensitive to initial conditions. Thus emerges the idea that whether a \index{butterflies}butterfly in Sierra Leone flaps its wings twice or thrice may determine whether a hurricane hits the southeastern United States in New Orleans, Louisiana, or in Galveston, Texas.\footnote{Clearly, this suggests that the solution to increased storm severity due to global warming is to kill all butterflies.}

If we generate simulations where we vary initial population size by a single individual, we find that this can have an enormous impact on the similarity of two populations' dynamics, and on our ability to predict future population sizes (Fig. \ref{fig:DDGChaosInitN}). Note how the populations start with similar trajectories, but soon diverge so that they experience different sequences of minima and maxima (Fig. \ref{fig:DDGChaosInitN}). This is part of what was so upsetting to ecologists about May's 1974 paper --- perhaps even the simplest deterministic model could create dynamics so complex that we could not distinguish them from random \cite{May:1974fk}. Over time, however, we came to learn that (i) we could distinguish random dynamics from some chaos-like dynamics, and (ii) the hunt for chaos could be very exciting, if most frequently disappointing \cite{Becks2005, Constantino1995,Kendall:1998ys}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.67\linewidth]{DDGChaosInitN.pdf}
  \caption{Effects of differences in initial population size on the short term and long term dynamics, and their correspondence, of three populations.}
  \label{fig:DDGChaosInitN}
\end{figure}

\medskip \noindent
\begin{boxedminipage}{\linewidth}{\footnotesize
  \paragraph{Sensitivity to Intitial Conditions}
We start with three populations, all very close in initial abundance. We then propogate with a $r_d$ to generate chaos for 100 time steps.
<<>>=
N.init <- c(97,98,99)
t <- 30
Ns <- sapply(N.init, function(n0) dlogistic(rd=2.7, N0=n0, t=t) )
@ 
Now we would like to graph them over the first 12 times, and look at the correlations between $N_1$ and the other two populations.
<<DDGChaosInitN, fig=TRUE>>=
matplot(0:t,Ns, type="l", col=1)
@
}\end{boxedminipage} \medskip

\subsubsection{Boundedness, and other descriptors}
One last issue that we should note is the extent to which our populations are \emph{\index{chaos!boundedness}bounded}. A population may have complex dynamics, but we may be able to characterize a given population by its upper and lower bounds. In spite of the differences created by the initial conditions, the upper and lower bounds of our chaotic populations were very similar (Fig. \ref{fig:DDGChaosInitN}). Note also as $r_d$ increases (Fig. \ref{fig:bifurcation}) the oscillations increase very systematically. 

In general, we can describe many characteristics of populations, even if we cannot predict exactly what the population size will be five years hence. For instance, we can describe the shape of density dependence (linear, nonlinear), and characteristics of $N$, such as the average, the variance, the periodicity, the upper and lower bounds, and the color (i.e. degree of temporal auto-correlation). Indeed, these characteristics may vary among types of organisms, body sizes, or environments.



## To err(or) is human
Most of this book describes and explains models. Models are explicit manifestations of theory with which we can organize our thoughts and and compare with our data. Here we bring our attnetion to two layers of uncertainty when we compare models and data: *process error* and *observation error*. These types of error are not mistakes, but differences from "actual" values. These terms can be confusing if we think that "error" means "mistake". They aren't; they are just differences between reality and our guesses of reality.

Observation error is the uncertainty associated with drawing inferences about a population based on only samples of the population. We can never know the actual abundance of tuna in the sea because we can only estimate it with uncertainty. We refer to the uncertainty due to our observation methods as *observation error*. It is not error in the colloquial sense of making a mistake, but simply a difference between an actual population, and our sample of it.

Process error is not error at all, in the colloquial sense. It is uncertainty in the very process we are studying. We can use the logistic growth model to predict the dynamics of a population, but the underlying processes of reproduction and density-dependence will vary for mechanistic reasons that we cannot fathom, or which we choose judiciously to ignore. Process error is the some underlying mechanistic difference between our model and the actual ecological process.

Any time we examine a time series of data, we need to envision both observation and porocess error underlying the dynamics. Often, we don't do that, and assume either only observation error or process error. 

Here we explore the consequences of process and observation error. This is inspired by @Bolker2008 (Ch. 11). Following this, we then move on to wax poetic about elephants in the rain and heat [@Chamaille-Jammes2008].

## Geometric growth with observation and process error

If we believe that a population of brown rats grows geometrically with $\lambda =1$, that means we predict that it doesn't change size. If we observe and sample this population we will sample it with error. We could describe the population, $N$, and our observations of it thus,
$$N_0 = a$$
$$N_{t+1} = \lambda N_t$$
$$N_{obs,t} \sim \mathrm{Normal}(N_t, \sigma^2_{obs})$$
where we say the observed $N$ at time $t$ is a Normally distributed random variable with a mean of the actual $N$ and a variance of $\sigma^2_{obs}$. Thus we estimate $N_t$ with uncertainty or observation error.

Now we model it.

```{r obs1}
# set lamba, number of time steps (nt), a vector to hold N, and N0
lambda <- 1
nt <- 100
N <- numeric(nt)
Nobs.o <- numeric(nt)
N0 = 100
N[1] <- N0

# decide on an amount of uncertainty in our estimate
# standard deviation

sd.obs <- 20

# now we project the population and model observation error
for (t in 1:(nt - 1)) {
  ## observation error
     Nobs.o[t] = rnorm(1, mean = N[t], sd = sd.obs)
     ## mechanistic projection
     N[t+1]=lambda*N[t]
}
# finish up adding error to the last observation
Nobs.o[nt] = rnorm(1, mean = N[nt])

## plot it
 qplot(1:nt, Nobs.o, geom=c("line", "point"))
```


---------------------------

So why is the continuous version so boring, while the discrete version is so complex? Remind yourself what is going on with the discrete version. The model could only take steps from one generation to the next. The step into generation $t+1$ was a function of $N$ at $t$, and not a function of $N_{t+1}$. Therefore the rate of change in $N$ was not influenced by a contemporaneous value of $N$. There was a delay between $N$ and the effect of $N$ on the population growth rate. For many organisms, this makes sense because they undergo discrete reproductive events, reproducing seasonally, for instance. In contrast, the continuous logistic population growth is always influenced by a value of $N$ that is updated continuously, instantaneously. That is the nature of simple differential equations. They are instantaneous functions of continuous variables. We can, if we so choose, build in a delay in the density dependence of continuous logistic growth. This is referred to as ``delayed density dependence'' or ``time-lagged logistic growth''



\begin{equation}
\label{clogisticDDD}
\frac{\D{N}}{\D{t}} = rN \left( 1 - \alpha N_{t-\tau} \right)
\end{equation}
where $\tau$ is the degree of the delay, in time units associated with the particular model. Just as with the discrete model, the dynamics of this continuous model can get very complicated, with sufficient lag and sufficient $r$.

\medskip \noindent
\begin{boxedminipage}{\linewidth}{\footnotesize
  \paragraph{Plotting Random Populations ( (Fig. \ref{c2}))}
 We use the above function to create 20 populations with different traits. We start with an empty matrix, and then for each of the populations, we draw random $N_0$ and $r$, run the ODE solver, keeping just the column for $N$. Last we plot the output.
```{r}
t.s <- 0:50
outmat <- matrix(NA, nrow=length(t.s), ncol=20)
for(j in 1:20) outmat[,j] <- { y <- runif(n=1, min=0, max=120)
                          prms <- c(r=runif(1, .01,2), alpha=0.01 )
                           ode(y, times=t.s, clogistic, prms)[,2] }
```
<<ContDyn2, fig=true, include=false>>=
matplot(t.s, outmat, type='l', col=1, ylab="All Populations")
@ 
}\end{boxedminipage} \medskip



<!--chapter:end:05-DDgrowth_untranslated.Rmd-->

# Multiple Basins of Attraction

## Introduction
Historically, simple models may have helped to lull some ecologists into thinking either that (i) models are useless because they do not reflect the natural
world, or (ii) the natural world is highly predictable. Here we investigate how simple models can create unpredictable outcomes, in models of Lotka--Volterra competition, resource competition, and intraguild predation. In all cases, we
get totally different outcomes, or alternative stable states, depending on
different, stochastic, initial conditions.

## Alternative stable states
*Alternative stable states* (ASS), *are a set of two or more possible stable attractors that can occur given a single set of external environmental conditions*.  For a single list of species, there may exist more than one set of abundances that provide stable equilibria. One key to assessing alternative stable states is that *the stable states must occur with the same external environmental conditions*. If the external conditions differ, then the system is merely governed by different conditions. A very important complication is that if an abiotic factor is coupled dynamically to the biotic community, then it becomes by definition an internal part of the system and no longer external to it.

If stable attractors exist, how does the system shift from one attractor to another? The system can be shifted in a variety of ways, but the key is that the system (i.e., the set of species abundances) gets shifted into a different part of the state space, and then is attracted toward another state. System shifts may occur due to demographic stochasticity, the random variation in births and deaths that may be especially important when populations become small. System shifts may also occur due to different assembly sequences. For instance the outcome of succession may depend upon which species arrive first, second, third, etc. System shifts might also arise via a physical disturbance that causes mortality. Different abundances may also arise from the gradual change and return of an environmental factor, and the resulting set of alternative equilibria is known as *hysteresis*, and below we examine a case related to resource competition. 

<!--chapter:end:08-MBA.Rmd-->

---
title: "Primer of Ecology with R"
author: "Hank Stevens"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
delete_merged_file: true
output:
    bookdown::tufte_book2:
      includes:
        in_header: preamble.tex
#     bookdown::gitbook
documentclass: book
bibliography: [/Volumes/GoogleDrive/My Drive/zlibrary.bib, /Volumes/GoogleDrive/My Drive/library.bib, book.bib, packages.bib]
biblio-style: plain
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "A Primer of Ecology"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, results='markup', warning=FALSE,
                      fig.path = "figs/", knitr.graphics.auto_pdf = TRUE,
                      message=FALSE)

library(primer)
library(lattice)
library(latticeExtra)
library(ggplot2)
theme_set(theme_minimal())
cbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
               "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
library(diagram)
library(knitr)
library(tufte)
library(reshape2)
library(data.table)
library(magrittr)
library(rsvg)
library(DiagrammeR)
library(DiagrammeRsvg)
library(phaseR)
```

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```


# Preface {-}

\paragraph{Goals and audience}
In spite of the presumptuous title, my goals for this book are modest. I wrote it as 
\begin{itemize}
  \item the manual I wish I had in graduate school, and
\item a primer for our graduate course in Population and Community Ecology at Miami University\footnote{Miami University is located in the Miami River valley in Oxford, Ohio, USA; the region is home to the Myaamia tribe that dwelled here prior to European occupation.}
\end{itemize}

It is my hope that readers can enjoy the \emph{ecological content} and ignore the R code, if they care to.  Toward this end, I tried to make the code easy to ignore, by either putting boxes around it, or simply concentrating code in some sections and keeping it out of other sections.

It is also my hope that ecologists interested in learning R will have a rich yet gentle introduction to this amazing programming language. Toward that end, I have included some useful functions in an R package called \texttt{primer}. Like nearly all R packages, it is available through the R projects repositories, the CRAN mirrors. See the Appendix for an introduction to the R language.

I have a hard time learning something on my own, unless I can \emph{do} something with the material. Learning  ecology is no different, and I find that my students and I learn theory best when we write down formulae, manipulate them, and explore consequences of rearrangement. This typically starts with copying down, verbatim, an expression in a book or paper. Therefore, I encourage readers to take pencil to paper, and fingers to keyboard, and copy expressions they see in this book. After that, make sure that what I have done is correct by trying some of the same rearrangements and manipulations I have done. In addition, try things that aren't in the book --- have fun.

\paragraph{A pedagogical suggestion}
For centuries, musicians and composers have learned their craft in part by \emph{copying by hand} to works of others. Physical embodiment of the musical notes and their sequences helped them learn composition. I have it on great authority that most theoreticians (and other mathematicians) do the same thing --- they start by copying down mathematical expressions. This physical process helps get the content under their skin and through their skull. I encourage you to do the same. Whether otherwise indicated or not, let the first assigned problem at the end of each chapter be to copy down, with a pencil and paper, the mathematical expression presented in that chapter. In my own self-guided learning, I have often taken this simple activity for granted and have discounted its value --- to my own detriment. I am not surprised how often students also take this activity for granted, and similarly suffer the consequences. \emph{Seeing} the logic of something is not always enough --- sometimes we have to actually \emph{recreate} the logic for ourselves.

Using a text editor (such as the simple one in the R GUI) is one of the huge benefits of using R. Using a text editor to write code and to make extensive comments to yourself (and others!) makes R \emph{truly} interactive. You interact with yourself by putting your thoughts on (virtual) paper, highlighting concepts and details you learn along the way, and documenting clearly what it is you have done. The code and the comments are two halves of the whole. Your comments also allow you to interact with others in a very complete way. You cannot realistically and reliably interact with others unless you put something in writing. If you use an application that is a pure GUI, you would have to describe the buttons you click and be absolutely certain that you leave nothing out and that you can actually communicate what you did. By writing and commenting code, you can send your scripts to anyone (your students, employees, boss, regulatory agency, collaborators\ldots). 

\paragraph{Comparison to other texts}
It may be useful to compare this book to others of a similar ilk. This book bears its closest similarities to two other wonderful primers: Gotelli's \emph{A Primer of Ecology}, and Roughgarden's \emph{Primer of Theoretical Ecology}. I am more familiar with these books than any other introductory texts, and I  am greatly indebted to these authors for their contributions to my education and the discipline as a whole. 

My book, geared toward graduate students, includes more advanced material than Gotelli's primer, but most of the ecological topics are similar. I attempt to start in the same place (e.g., ``What is geometric growth?''), but I develop many of the ideas much further. Unlike Gotelli, I do not cover life tables at all, but rather, I devote an entire chapter to \emph{demographic matrix models}. I include a chapter on community structure and diversity, including \emph{multivariate distances}, \emph{species-abundance distributions}, \emph{species-area relations}, and \emph{island biogeography}, as well as \emph{diversity partitioning}. My book also includes code to implement most of the ideas, whereas Gotelli's primer does not.

This book also differs from Roughgarden's primer, in that I use the Open Source R programming language, rather than Matlab\textregistered, and I do not cover physiology or evolution. My philosphical approach is similar, however, as I tend to ``talk'' to the reader, and we fall down the rabbit hole together\footnote{From \emph{Alice's Adventures in Wonderland} (1865), L. Carroll (C. L. Dodgson).}.

Aside from Gotelli and Roughgarden's books, this book bears similarity in content to several other wonderful introductions to mathematical ecology or biology. I could have cited repeatedly (and in some places did so) the following: Ellner and Guckenheimer (2006), Gurney and Nisbet (1998), Kingsland (1985), MacArthur (1972), Magurran (2004), May (2001), Morin (1999), Otto and Day (2006),  and Vandermeer and Goldberg (2007). Still others exist, but I have not yet had the good fortune to dig too deeply into them.

\paragraph{Acknowledgements}
I am indebted to Scott Meiners and his colleagues for their generous sharing of data, metadata, and statistical summaries from the Buell-Small Succession Study (http://www.ecostudies.org/bss/), a 50$+$ year study of secondary succession (supported in part by NSF grant DEB-0424605) in the North American temperate deciduous forest biome. I would like to thank Stephen Ellner for Ross's Bombay death data and for R code and insight over the past few years.  I am also indebted to Tom Crist and his colleagues for sharing some of their moth data (work supported by The Nature Conservancy Ecosystem Research Program
NSF DEB-0235369).

I am grateful for the generosity of early reviewers and readers, each of whom has contributed much to the quality of this work: Jeremy Ash, Tom Crist, David Gorchov, Raphael Herrera-Herrera, Thomas Petzoldt, James Vonesh, as well as several anonymous reviewers, and the students of our Population and Community Ecology class. I am also grateful for the many conversations and emails shared with four wonderful mathematicians and theoreticians: Jayanth Banavar, Ben bolker, Stephen Ellner, Amit Shukla, and Steve Wright --- I never have a conversation with these people without learning something. I have been particularly fortunate to have team-taught Population and Community Ecology at Miami University with two wonderful scientists and educators, Davd Gorchov and Thomas Crist. Only with this experience, of working closely with these colleagues, have I been able to attempt this book. It should go without saying, but I will emphasis, that the mistakes in this book are mine, and there would be many more but for the sharp eyes and insightful minds of many other people.

I am also deeply indebted to the R Core Development Team for creating, maintaining and pushing forward the R programming language and environment \cite{R2009}. Like the air I breathe, I cannot imagine my (professional) life without it. I would especially like to thank Friedrich Leisch for the development of \texttt{Sweave}, which makes literate programming easy \cite{Leisch:2002sw}. Because I rely on Aquamacs, ESS, \LaTeX, and a host of other Open Source programs, I am deeply grateful to those who create and distribute these amazing tools.

A few other R packages bear special mention. First, Ben Bolker's text \cite{Bolker:2008rr} and packages for modeling ecological data (\texttt{bbmle} and \texttt{emdbook}) are broadly applicable. Second, Thomas Petzoldt's and Karsten Rinke's \texttt{simecol} package provides a general computational architecture for ecological models, and implements many wonderful examples \cite{Petzoldt:2007jo}. Much of what is done in this primer (especially in chapters 1, 3--6, 8) can be done with \texttt{simecol}, and sometimes done better. Third, Robin Hankin's \texttt{untb} package is an excellent resource for exploring ecological neutral theory (chapter 10) \cite{Hankin:2007zm}. Last, I relied heavily on the \texttt{deSolve} \cite{Soetaert:mw} and \texttt{vegan} packages \cite{Oksanen:2008}.

Last, and most importantly, I would like to thank those to whom this book is dedicated, whose love and senses of humor make it all worthwhile.

\vspace{1cm}
\begin{flushright}\noindent
{\it Martin Henry Hoffman Stevens}\\
Oxford, OH, USA, Earth\\
February, 2009\\
\end{flushright}




<!--chapter:end:0thoroughindex.Rmd-->

# Disease
Here we discuss epidemiological disease models. Pathogens cause diseases, and are typically defined as microorganisms (fungi, bacteria, and viruses) with some host specificity, and which undergo population growth within the host. 

Our simplest models of disease are funny, in that they don't model pathogens (the enemy) at all. These famous models, by @Kermack:1927fk, keep track of different types of hosts, primarily those with and without diseas that model all $N$ hosts by keeping track of 

* **Susceptible hosts** Individuals which are not infected, but could become infected,
* **Infected hosts** Individuals which are already infected, and
* **Resistant hosts** Individuals which are resistant to the disease, typically assumed to have built up an immunity through past exposure,

where $N=S+I+R$. It is important to note that $N$, $S$, $I$, and $R$ are *densities*. That is, we track numbers of individuals in a fixed area. This is important because it has direct consequences for the spread, or transmission, of disease [@McCallum2001].

Disease spreads from infected individuals to susceptible individuals. The rate depends to some extent on the number or alternatively, on the fraction of the population that is infected. Resistant individuals are not typically considered vectors of the pathogens, and so increased abundance of resistant individuals slow the transmission rate by diluting the other two groups. 

## Constant population size

A good starting place is a simple SIR model for a population of constant size, with no immigration or emigration,  [@Ellner:2006qe, @Kermack:1927fk]. 
\begin{align*}
  (\#eq:SIR)
  \frac{d S}{d t} &= -\beta IS\\
  \frac{d I}{d t} &=\beta IS  - \gamma I\\
  \frac{d R}{d t} &= \gamma I
\end{align*}


```{r sir}
# Here we create the function for the simple SIR model.
SIR <- function(t, y, p) {
  with( as.list(c(y, p)), {
    dS.dt <- -B*I*S
    dI.dt <- B*I*S - g*I
    dR.dt <- g*I
   return( list(c(dS.dt, dI.dt, dR.dt)) )
  } )
}
```

### Density-dependent transmission
In this model, the *transmission coefficient*, $\beta$, describes the instantaneous rate at which the number of infected hosts increases per infected individual. It is directly analogous to the attack rate of type I prey-dependent models. Recall that it is based on the law of mass action, borrowed from physics and chemistry. It assumes that the rate at which events occur (new infections) is due to complete and random mixing of the reactants ($S,\,I$), and the rate at which the reactants collide and react can be described by a single constant, $\beta$. As density of either type of molecule increases, so too does the rate of interaction. In prey-dependent predation, we referred to $aN$ as a linear functional response; here we refer to $\beta S$ as the *density-dependent transmission function*. The *transmission rate* is the instantaneous rate for the number of *new infections* or cases per unit time [@McCallum2001].

Resistant individuals might be resistant for one of two reasons. They may die, or they may develop immunities. In either case, we assume they cannot catch the disease again, nor spread the disease. As this model assumses  a constant population  size, we continue to count all $R$ individuals, regardless of whether they become immune or die.

The individuals become resistant to this disease at the constant per capita rate, \index{$\gamma$}$\gamma$. The rate $\gamma$ is also the inverse of the mean \index{residence time}residence time, or \emph{\index{duration|see{residence time}}duration}, of the disease\footnote{This is an interesting phenomenon of exponential processes --- the mean time associated with the process is equal to the inverse of the rate. This is analogous to turnover time or residence time for a molecule of water in a well-mixed lake.}.  

Disease \emph{\index{incidence}incidence} is the number of new infections or cases occurring over a defined time interval. This definition makes incidence a discrete-time version of transmission rate. \emph{\index{prevalence}Prevalence} is the fraction of the population that is infected $I/N$.


A common question in disease ecology is to ask under what conditions will an outbreak occur. Another way of asking that is to ask what conditions cause $\dot{I}>0$. We can set $dI/dt > 0$ and solve for something interesting about what is required for an outbreak to occur.
\begin{align}
  0 & < \beta IS - \gamma I\notag\\
  \frac{\gamma}{\beta} &< S
\end{align}
What does this tell us? First, because we could divide through by $I$, it means that if no one is infected, then an outbreak can't happen --- it is the usual, but typically unstable equilibrium at 0. Second, it tells us that an outbreak will occur if the absolute density of susceptibles^[$S$ is the absolute density, whereas $S/N$ is the relative density.] is greater than $\gamma / \beta$. If we consider the pre-outbreak situation where $S \approx N$, then simply making the population size (and density) low enough can halt the spread of disease. This is why outbreaks tend to occur in high density populations, such as agricultural hosts (e.g., cattle), or historically in urban human populations, or in schools. 

Vaccinations are a way to reduce $S$ without reducing $N$. If a sufficient number of individuals in the population are vaccinated to reduce $S$ below $\gamma / \beta$, this tends to protect the unvaccinated individuals as well.

Another common representation of this is called the *force of infection* or }*basic reproductive rate of the disease.* If we assume that in a large population $S \approx N$, then rearranging eq. \@ref(eq:R0) gives us 
\begin{equation}
(\#eq:R0)
  R_0=\frac{\beta N}{\gamma}
\end{equation}
where \index{R@$R_0$|see{basic reproductive rate}}$R_0$ is the basic reproductive rate of the disease. If $R_0 > 1$, then an outbreak (i.e., disease growth) is plausible. This is analogous to the finite rate of increase of a population where $\lambda>1$.

Here we model the outbreak of a nonlethal disease (e.g.,  a new cold virus in winter on a university campus). We assume that the disease is neither life-threatening, and nor is anyone resistant, thus $R_{t=0}=0$. We can investigate the SIR model by pretending that, as is often the case, we begin with a largely uninfected population and $t=0$, so $I_0=1$ and $S_0\approx N$.  We first set parameters. Let's imagine that a 
```{r cold}
N <- 10^4; I <- R <- 1; S <- N - I - R
y <- c(S=S, I=I, R=R)
parms <- c(B=.01, g=4)
# Next we integrate for three months.
months <- seq(0,3, by=0.01)
out1 <- data.frame( ode(y, months, SIR, parms) )
out2 <- pivot_longer(out1, cols=S:R, names_to="Host", values_to="N")
ggplot(out2, aes(time, N, linetype=Host)) + geom_line() + labs(x="Months")
```
```{r ash}
# Emerald ash borer?
N <- 10^4; I <- R <- 1; S <- N - I - R
y <- c(S=S, I=I, R=R)
# ash death takes about 4 years to die, 
# meaning it is infected for about 4 years
duration <- 4
gamma = 1/duration
# question = how to estimate beta?
beta = .0005
parms <- c(B=beta, g=gamma)
years <- seq(0,10, by=0.01)
out1 <- data.table( ode(y, years, SIR, parms)  )
out2 <- pivot_longer(out1, cols=S:R, names_to="Host", values_to="N")
ggplot(out2, aes(time, N, linetype=Host)) + geom_line() + labs(x="Years")
```
It is important at this point to reiterate a point we made above --- *these conclusions apply when S, I, and R are densities* [@McCallum2001]. If you increase population size but also the area associated with the population, then you have not changed density. If population size only increases, but density is constant, then interaction frequency does not increase. Some populations may increase in density as they increase is size, but some may not. Mass action dynamics are the same as type I functional response as predators --- there is a constant linear increase in per capita infection rate as the number of susceptible hosts increases.

### Frequency-dependent transmission
In addition to density-dependent transmission, investigators have used other forms of density dependence. One the most common is typically known as *frequency--dependent* transmission, where transmission depends on the *prevalence*, or the frequency of infecteds in the population, $I/N$.
\begin{align}
  \frac{d S}{d t} &= - \beta \frac{SI}{N} \#eq:SIRfd1\\
  \frac{d I}{d t} &=   \beta \frac{SI}{N} - \gamma I \#eq:SIRfd2\\
  \frac{d I}{d t} &=   \gamma I. \#eq:SIRfd2
\end{align}

Here we create the function for the system of ODEs in eq. \ref{eq:SIR}.
```{r}
SIRf <- function(t, y, p) {
  N <- sum(y)
  with( as.list(c(y,p)), {
    dS.dt <- -B*I*S/N
    dI.dt <- B*I*S/N - g*I
    dR.dt <- g*I
   return( list(c(dS.dt, dI.dt, dR.dt)) )
  } )
}
```
The proper form of the transmission function depends on the mode of transmission  [@McCallum2001]. Imagine two people are on an elevator, one sick (infected), and one healthy but susceptible, and then the sick individual sneezes [@Ellner:2006qe]. This results in a particular probability, $\beta$, that the susceptible individual gets infected. Now imagine resistant individuals get on the elevator --- should adding resistant individuals change the probability that the susceptible individual gets infected? Note what has and has not changed. First, with the addition of a resistant individual, $N$ has increased, and prevalence, $I/N$, has decreased. However, the densities of $I$ and $S$ remain the same (1 per elevator). What might happen? There are at least two possible outcomes:

1. If sufficient amounts of the virus spread evenly throughout the elevator, adding a resistant individual does *not* change the probability of the susceptible becoming sick, and the rate of spread will remain dependent on the densities of $I$ and $S$ --- the rate will not vary with declining prevalence.
2.  If only the immediate neighbor gets exposed to the pathogen, then the probability that the neighbor is susceptible declines with increasing $R$, and thus the rate of spread *will* decline with declining prevalence.

It is fairly easy to imagine different scenarios, and it is very important to justify the form of the function.

```{r}
R <- 0; S <- I <- 1000; Ss <- Is <- seq(1, S, length=11); N <- S+I+R
betaD <- 0.1; betaF <- betaD*N

# sapply will calculate the transmission functions for each combination of
# the values of $I$ and $S$.
mat1 <- sapply(Is, function(i) betaD * i * Ss)
mat2 <- sapply(Is, function(i) betaF * i * Ss / (i + Ss + R) )
# Now we plot these matrices. 

{
  layout(matrix(1:2, nr=1))
persp(mat1, theta=20, phi=15, r=10, zlim=c(0,betaD*S*I), 
      main="Density Dependent",
      xlab="I", ylab="S", zlab="Transmission Rate")
persp(mat2, theta=20, phi=15, r=10, zlim=c(0,betaF*S*I/N), 
      main="Frequency Dependent",
      xlab="I", ylab="S", zlab="Transmission Rate")
}

``` 


Elaborate on this stuff?

Antonovic et al. Metapopulation disease dynamics

*Silene alba* and *Microbotryum violacea*
Proportion of subpopulations that have an infected plant increases with the size of the subpopulation 

Disease prevalence declines with subpopulation size. This is characteristic of frequency-dependent transmission. 

Lyme Disease

Van Buskirk and Ostfeld 1995 Eco. Appl.
*Borrelia burgdorferi* spirochete
Blacklegged tick *Ixodes scapularis*
*Peromyscus leucopus* Whitefooted mouse
Whitetailed deer *Oedicoilius virginiana*
spring 1: eggs
summer 1: larva on intermediate host - small mammal or bird
spring 2: nymph a new intermediate host small mammal or bird 
summer 2: drops off host 2, seeks definitive host (tyupically deer); most dangerous to humans
fall 2: adult on deer (low risk to humans)

host competence: whitefooted mice vs. chipmunks and birds

<!--chapter:end:10-disease.Rmd-->

# Diversity

## Background
```{r BSsuc,fig=TRUE, fig.width=2.5, fig.height=2.5, echo=FALSE, results='hide', message=FALSE, fig.cap="Empirical rank--abundance distributions of successional plant communities (old-fields) within the temperate deciduous forest biome of North America. ``Year'' indicates the time since abandonment from agriculture.  Data from the Buell-Small succession study (http://www.ecostudies.org/bss/)"}
bsdiv <- read.csv('C3rank.csv')
bs <- melt(bsdiv, id="Rank", variable.name="Year", value.name="Percent_cover")
str(bs)
ggplot(bs, aes(Rank, Percent_cover, linetype=Year, colour=Year)) + geom_line() + scale_y_log10()
```

It seems easy, or at least tractable, to compare the abundance of a
single species in two samples. In this chapter, we introduce concepts
that ecologists use to compare entire communities in two samples. We
focus on two quantities: *species composition*, and
*diversity*. We also discuss several issues related to this,
including species--abundance distributions,  ecological neutral
theory, diversity partitioning, and species--area relations. Several packages in R include functions for dealing specifically with these topics. 
Please see the ``Environmetrics'' link within the ``Task Views'' link at any CRAN website for downloading R packages. Perhaps the most comprehensive (including both diversity and composition) is the `vegan` package, but many others include important features as well.

## Neutral theory

Species area immigration curves
```{r}
d <- .4
i <- 1:26
g <- d*(1-d)^(i-1)
geodist <- g/sum(g)
comm <- sample(letters, 1e4, replace=TRUE, prob=geodist)
plot( table(comm) )
```

Gamma function
```{r}

gamma(3) # 2*1
gamma(4) # 3*2*1
gamma(3.5) # something in between. :)
```

<!--chapter:end:10-diversity.Rmd-->

# Food webs

A food web is a real or a model of a \emph{set of feeding relations among species or functional groups}. This chapter has two foci, (i) a very brief introduction to multi-species webs as networks, and (ii) a re-examination of old lessons regarding the effect of food chain length on a web's dynamical properties.


```{r web, echo=FALSE, fig.cap="A food web is a map of feeding relations. Here, snails (S) and invertebrates (I) feed on algae, and fish, F, feed on invertebrates and algae. We can represent this as a graph or a matrix.", out.width="40%", fig.show='hold', fig.ncol=2, fig.height=5, fig.width=5}
gid <- graph(c('S', 'A', 'A', 'S', 
               'I', 'A', 'A', 'I', 
               'I','F', 'F','I',
               'P','A', 'A','P'))
signs <- c("-",'+', "-",'+', "-",'+', "-",'+')
plot(gid, layout=matrix(c(0, .5, 1, .6, 0.5, 0.1, 0.5, .85), nc=2), edge.curved=.5,      edge.label=signs, edge.label.font=2,  edge.label.cex=1.5, 
     vertex.size=60, 
     margin=c(0,0,0,0), rescale=TRUE)
```
\begin{tabular}{c| c c c c }
& \multicolumn{4}{c}{\emph{~Consumers}}\\
\emph{~Resources~} & ~A~ & ~I~ & ~S~ & ~F~\\
\hline
A&0&$-$&$-$&$-$\\
I&$+$&0&0&$-$\\
S&$+$&0&0&0\\
F&$+$&$+$&0&0\\
\end{tabular}

## Food Web Characteristics
\index{food web characteristics}
We need language to describe the components of a food web, such as *links* and \emph{\index{nodes}nodes}, and we also need language to describe the properties of a web as a whole. Generally speaking, networks such as food webs have \emph{emergent properties} [@May:2006lr], such as the number of nodes in the network. Emergent properties are typically considered to be nothing more than characteristics which do not exist at simpler levels of organization. For instance, one emergent property of a population is its density, because population density cannot be measured in an individual; that is why density is an \index{emergent property}emergent property.^[If we assume no supernatural or spectral interference, then we can also assume that density arises mechanistically from complicated interactions among individuals and their environments, rather than via magic. Other scientists will disagree and say that properties like density that are merely additive aggregates do not qualify for the lofty title of "emergent property."] While all food web models are based on simple pairwise interactions, the resulting emergent properties of multispecies webs quickly become complex due to indirect interactions and coupled oscillations [@Berlow:2004yq;@Vandermeer:2006fj].  In addition, any extrinsic factor (e.g., seasonality) that might influence species interactions may also influence the emergent properties of food webs. 


A few important \index{network}network descriptors and emergent properties include,

* **Node** A point of connection among links, a.k.a. \index{trophospecies}trophospecies; each node in the web may be any set of organisms that share sufficiently similar feeding relations; in Fig. \@ref(fig:web), $P$ may be a single population of one species, or it may be a suite of species that all feed on both $B$ and $R$. 
* **Link** A feeding relation; a connection between nodes or trophospecies; may be directed (one way) or undirected. A directed link is indicated by an arrow, and is the effect ($+,\,-$) of one species on another. An undirected link is indicated by a line (no arrow head), and is merely a connection, usually with positive and negative effects assumed, but not quantified. 
* **Connectance** The proportion of possible links realized. Connectance may be based on either directed, $C_{D}$, or undirected, $C_{U}$, links. For Fig. \@ref(fig:web) these would be
\begin{gather*}
C_{D}=\frac{L}{S^{2}}=\frac{8}{16}=0.5 \\
C_{U}=\frac{L}{\left(S^2-S\right)/2}=\frac{4}{6}=0.67
\end{gather*}
where $S$ is the number of species or nodes.
* **Degree distribution**, $\mathrm{Pr}(i)$ The probability that a randomly chosen node will have degree $i$, that is, be connected to $i$ other nodes [@May:2006lr]. In Fig. \@ref(fig:web), $A$ is of degree 1 (i.e., is connected to one other species).  $P$ and $B$ are of degree 2, and $R$ is of degree 3. If we divide through by the number of nodes (4, in Fig. \@ref(fig:web)), then the \emph{degree distribution} consists of the probabilities $\mathrm{Pr}\left(i\right)=\{0.25,\,0.5,\,0.25\}$. As webs increase in size, we can describe this distribution as we would a statistical distribution. For instance, for a web with randomly placed connections, the degree distribution is the binomial distribution [@Cohen1990d].
* **Characteristic path length** Sometimes defined as the average shortest path between any two nodes [@Dunne2002]. For instance, for Fig. \@ref(fig:web), the shortest path  between $P$ and $R$ is 1 link, and between $A$ and $P$ is 2 links. The average of all pairwise shortest paths is $(1+1+1+1+2+2)/6=1.\bar{3}$. It is also sometimes defined as the average of \emph{all paths} between each pair of nodes.
 
* **Modularity, Compartmentation** The degree to which subsets of species are highly connected or independent of other species. This tends to arise in consumer-resource interactions [@Thebault2010]
  
To calculate \index{compartmentation}compartmentation in a food web, first assume each species interacts with itself. Next, calculate the proportion of shared interactions, $p_{ij}$ for each pair of species, by comparing the lists of species with which each species in a pair interacts. The numerator of $p_{ij}$ is the number of species with which both of the pair interact. The denominator is the total number of different species with which either species interacts. \medskip

As an example, let's calculate this for the above food web (Fig. \@ref(fig:web)). $A$ interacts with $A$ and $R$, $B$ interacts with $B$, $R$, and $P$. Therefore, $A$ and $B$ both interact with only $R$, whereas, together, $A$ and $B$ interact with $A$, $B$, $R$, and $P$. The proportion, $p_{ij}$, therefore is $1/4 = 0.25$. We do this for each species pair. \smallskip

Next we sum the proportions, and  divide the sum by the maximum possible number of undirected links, $C_U$.
\begin{table}[h]
\begin{minipage}[c]{.48\linewidth}
  \centering
\begin{tabular}{l| c c c }
Species & S & I & F\\
\hline
A&2/4&3/4&3/4\\
S&  &1/4&1/4\\
I&  &  &3/3 
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}[c]{.48\linewidth}
\begin{align*}
  (\#eq:ci)
C_{I}&=\frac{\sum_{i=1}^{S-1}\sum_{j=i+1}^{S}p_{ij}}{\left(S^2-S\right)/2}\\
    &=3.5/6\\
    &=0.58
\end{align*}
  \end{minipage}
\end{table}
To reiterate: For any pair of species, $i$ and $j$ ($i\neq j$), $p_{ij}$ is the proportion of shared interactions, calculated from the number of species that interact with \emph{both} species
$i$ and $j$, divided by the number of species that interact
with \emph{either} species $i$ or species $j$. As above, $S$ is
the number of species or nodes in the web.
* **Nestedness** the degree to which specialists make connections that generalists do too. A completely nested network is one in which we can rank species by the number of interactions, and find that species that interact with fewer other species are always interacting with species that generalists also interact with. This tends to arise in plant-pollinator networks [@Thebault2010].
* **Motif** A repeating pattern; most commonly, a unique set of unidirectional patterns of consumption (Fig. \@ref(fig:motifs)). A motif *profile* of a species is the variety of motifs that that species is part of, and the relative frequency with which they play each role. 
* **Small world architecture** The degree to which characteristic path length, $L$, grows with the logarithm of the number of nodes in the web, $N$, 
$$L \propto log(N)$$
This occurs when nodes are connected via a small number of links through a node with a large number of connections. This occurs in social networks when individuals are connected via shared acquaintances.

```{r motifs, echo=FALSE, fig.cap="There are 13 different possible motifs for three species. These links record only consumption. For instance, S1 is a simple linear food chain and S4 is a single species that is consumed by two other species. motifs D1-D8 all include mutual consumption. This is likely to occur between species in the same guild, and when adults of one feed on juveniles of another."}
include_graphics("figs/motifs")
```

* **Trophic level, trophic position** may simply be categorized as basal, intermediate or top trophic positions. Basal positions are those in which the trophospecies feed on no other species. The top positions are those in which the trophospecies are fed upon by nothing. One can also calculate a quantitative measure of trophic \emph{level}. This is important in part because omnivory, something rampant in real food webs, complicates identification of a trophic level. We can calculate \index{trophic level}trophic level for the top predator, $P$ (Fig. \@ref(fig:web)), and let us assume that $P$ gets two-thirds of what it needs from $B$, and gets one-third from $A$. $B$ itself is on the second trophic level, so given that, the trophic level of $P$ is calculated as
\begin{equation*}
T_{i}=1+\sum_{j=1}^{S}T_{j}p_{ij}=1+\left(2\left(0.67\right)+1\left(0.33\right)\right)=2.67
\end{equation*}
where $T_{i}$ is the trophic level of species $i$,
$T_{j}$ is the trophic level of prey species $j$,
and $p_{ij}$ is the proportion of the diet of predator
$i$ consisting of prey $j$.
* **Omnivory**Feeding on more than one \emph{trophic} level ($v>0$, Fig. \@ref(fig:web)); it is \emph{not} merely feeding on different species or resources.
* **Intraguild predation** A type of omnivory in which predation occurs between consumers that share a resource; in Fig. \@ref(fig:web) $P$ and $B$ share prey $R$. When $P$ gets most of its energy from $B$, we typically refer to that as omnivory ($v<0.5$); when $P$ gets most of its energy from $R$, we typically refer to that as intraguild predation ($v>0.5$).

This list of food web descriptors is a fine start but is by no means exhaustive.

## Food chain length --- an emergent property
There are many interesting questions about food webs that we could address; let us address one that has a long history, and as yet, no complete answer: What determines the length of a \index{food chain length}food chain? Some have argued that chance plays a large role [@Cohen1990d;@Hubbell2001;
@Williams2000a],  and others have shown that area
[@MacArthur1967; @Rosenzweig1995] or ecosystem size [@Post:2002bs] may
play roles. The explanation that we will focus on here is *dynamical stability*. Communities with more species had been hypothesized to be less stable, and therefore less likely to persist and be observed in nature. Stuart Pimm and John Lawton [@Pimm1977] extended this work by testing whether *food chain
length* could be limited by the instability of long food chains.

### Multi-species Lotka--Volterra notation
A many-species \index{Lotka--Volterra!food web}Lotka--Volterra model can be represented in a very compact form,
\begin{equation}
(\#eq:compact)
\frac{d X_{i}}{d t}=X_{i}\left(b_{i}+\sum_{j=1}^{S}a_{ij}X_{j}\right)
\end{equation}
where $S$ is the number of species in the web, $b_i$ is the intrinsic rate of increase of species $i$ (i.e., $r_i$), and $a_{ij}$ is a per capita effect of species $j$ on species $i$. 

When $i=j$, $a_{ij}$ refers to an *intra*specific effect, which is typically negative. Recall that in our earlier chapters on competition, we used $\alpha_{ii}$ to represent intraspecific per capita effects. Here for notational convenience, we leave $i$ and $j$ in the equation, realizing that $i=j$ for intraspecific interactions. Further, we let $a_{ij}$ be any sign, either positive or negative, and sum the effects. If we let $X=N$, $b=r$, and $a=r \alpha$, then the following are equivalent:
\begin{gather*}
  \dot{N_1} =r_1N_1\left(1-\alpha_{11}N_1 - \alpha_{12}N_2\right)\\
  \dot{X} =X_1\left(b_1+a_{11}X_1+a_{12}X_2 \right)
\end{gather*} 
The notation in \@ref(eq:compact) is at once both simple and flexible. When $a_{ij}$ is negative, it may represent competition or the effect of a predator, $j$, on its prey, $i$. When $a_{ij}$ is positive, it may represent mutualism or the effect of prey~$j$ on a predator~$i$.

### Background
In the early and mid-1970's, Robert May and others
 demonstrated that important predictions could be made with relatively simple Lotka--Volterra models [@May1973ab], and this work still comprises an important compendium of lessons for ecologists today [@May:2006lr]. May used simple Lotka--Volterra models to show that increasing the number of species in a food web tended to make the food web less stable [@May1973ab; @May1972]. In species-rich webs, species were more likely to become extinct. Further, he showed that the more connections there were among species in the web (higher connectance), and the stronger those connections (higher interaction strength), the \emph{less} stable the web. At the time, this ran counter to a prevailing sentiment that more diverse ecosystems were more stable, and led to heated discussion. 
```{r PLABE, echo=FALSE, fig.cap="Three of the six webs investigated by Pimm and Lawton (1977). Left to right, these correspond to Pimm and Lawton (1977) Figs. 1A, E, and B. All basal species exhibit negative density dependence.", out.width=c("20%","55%","20%"), fig.show='hold', fig.ncol=3, fig.height=10, fig.width=5}
resc <- FALSE
gA <- graph(c(1,1, 1,2,2,1, 2,3,3,2, 3,4,4,3))
signs <- c("-", '+',"-", '+',"-", '+',"-")
plot(gA, layout=matrix(c(.5,.5,.5,.5, 0,.33,.67,1), nc=2), edge.curved=.5,      edge.label=signs, edge.label.font=1,  edge.label.cex=1, 
     vertex.size=20, edge.loop.angle=pi/4,
     margin=c(0,0,0,0), rescale=resc)

gE <- graph(c(1,1, 2,2, 3,3, 1,4,4,1, 2,4,4,2, 3,4,4,3))
signs <- c("-","-","-", '+',"-", '+',"-", '+',"-")
plot(gE, layout=matrix(c(0,.5,1,.5, 0,0,0,.5), nc=2), edge.curved=.5,      edge.label=signs, edge.label.font=1,  edge.label.cex=1, 
     vertex.size=30, edge.loop.angle=pi/4,
     margin=c(0,0,0,0), rescale=resc)

gB <- graph(c(1,1, 1,2,2,1, 
              2,3,3,2, 
              3,4,4,3, 
              2,4,4,2))
signs <- c("-", '+',"-", 
           '+',"-", 
           '+',"-", 
           '+',"-")
plot(gB, layout=matrix(c(.3,.5,.7,.3, 0,.33,.67,1), nc=2), 
     edge.curved=.5,edge.label=signs, edge.label.font=1,  edge.label.cex=1, 
     vertex.size=30, edge.loop.angle=pi/4,
     margin=c(0,0,0,0), rescale=resc)
```


## Implementing Pimm and Lawton's Methods}
Here we use R code to illustrate how one might replicate, and begin to extend, the work of Pimm and Lawton [@Pimm1977].

In previous chapters, we began with explicit time derivatives, found partial derivatives and solved them at their equilibria. Rather than do all this, Pimm  and Lawton bypassed these steps and went straight to the evaluated Jacobian matrix. They inserted  random estimates for the elements of the Jacobian into each non-zero element in the food web matrix. These estimates were constrained within (presumably) reasonable limits, given large less abundant predators and small more abundant prey. 

Their methods followed this approach.

1. Specify a food web interaction matrix.^[In the original publication, webs E and D seem to be represented incorrectly.]
2. Include negative density dependence for basal species only. 
3. Set upper and lower bounds for the effects of predators on prey ($0$ to
$-10$) and prey on predators($0$ to $+0.1$); these are the Jacobian elements.
4.  Generate a large number of random Jacobian matrices and perform linear
stability analysis.
5. Determine qualitative stability (test $\lambda_1<0$), and return time for each random matrix. Use these to examine key features of the distributions of return times (e.g., average return time).
6.  Compare the stability and return times among different food web configurations that varied systematically in food chain length and the presence
of omnivory, but hold the number of species constant.

It is worth discussing briefly the \index{Jacobian elements}Jacobian elements. @May1973ab defined interaction strength as the Jacobian element of a matrix, which represents the *total effect of one individual on the  population growth rate of another species*. Think about how you calculate the Jacobian --- as the partial derivative of one species' growth rate with respect to the size of the other population. It is the instantaneous change in the population growth rate per unit change in the population of another, at the equilibrium. The units chosen for the Jacobian elements thus mean that individual predators have relatively much larger effects on the population growth rates of prey than \emph{vice versa}.

Let's build a function that does what Pimm and Lawton did. There are an infinite number of ways to do this, but this will suffice. First, we'll create a matrix that represents qualitatively the simplest longest food chain where each species feeds only on one other species and where no prey are fed upon by more than one consumer. Initially, we will use the values of greatest magnitude used by Pimm and Lawton. 
```{r}
Aq = matrix(c(
  -1,   -10,   0,     0,
  0.1, 0,   -10,  0,
  0,    0.1,   0,    -10,
  0,      0,    0.1,   0),
  nrow=4, byrow=TRUE)
```
Note that this matrix indicates a negative effect of the basal species on itself, large negative effects ($-10$) of each consumer on its prey, and small positive effects of each prey on its consumer.

For subsequent calculations, it is convenient to to find out from the matrix itself how big the matrix is, that is, how many species, $S$, are in the web.
```{r}
S <- nrow(Aq)
```
Next, we create a random realization of this matrix by multiplying each element times a unique random number between zero and 1. For this matrix, that requires $4^2$ unique numbers.
```{r}
M <- Aq * runif(S^2)
```
Next we perform eigenanalysis on it, retaining the eigenvalues.
```{r}
eM <- eigen(M)[["values"]]
```
Pimm and Lawton tested whether the dominant eigenvalue was greater than 0 (unstable) and if less than zero, they calculated return time. We will simply record the dominant eigenvalue (the maximum of the real parts of the eigenvalues).
```{r}
deM <- max( Re(eM) )
```
Given the stabilizing effect of the intraspecific negative density dependence, we will hang on to that as well.
```{r}
intraNDD <- sqrt(sum(diag(M)^2)/S)
```
Given lessons from May's work, we might also want to calculate the average interaction strength, not including the intraspecific interactions. Here we set the diagonal interactions equal to zero, square the remaining elements, find their average, and take the square root.
```{r}
diag(M) <- 0
IS <- sqrt( sum(M^2)/(S*(S-1)) )
```
Recall that weak omnivory is supposed to stabilize food webs [@McCann1997]. For webs that include omnivory, we will calculate the interaction strength of omnivory in the same way we do for other interactions, as the square root of the average of the squared $a_{ij}$ \@ref(eq:IS).

We can wrap all this up in a function where we specify the $i,\,j$ of one of the directed omnivorous links. It does not matter which we specify, either the $ij$ or the $ji$.
```{r PL2, echo=FALSE, results='hide'}
pimmlawton <- function(mat, N=1, omni.i=NA, omni.j=NA, omega=NULL){
  S <- nrow(mat)
  if(is.na(omni.i)) {
  out <- matrix(NA, nrow=N, ncol=4)
    colnames(out) <- c("DomEig", "Im", "IntraDD", "I")
  for(n in 1:N) out[n,] <- {
  M = runif(S^2) * mat
  ## Do eigenanalysis
  eigs <- eigen(M)[["values"]]
  mx <- which.max(Re(eigs))
  deM = Re(eigs)[mx]
  deMi = Im(eigs)[mx]
  intraNDD <- sqrt(sum(diag(M)^2)/S)
  diag(M) <- 0
IS = sqrt( sum(M^2)/(S*(S-1)) )
 c(deM, deMi, intraNDD, IS)
  } } else {
  out <- matrix(NA, nrow=N, ncol=5)
    colnames(out) <- c("DomEig", "Im", "IntraDD", "I", "I.omni")
  for(n in 1:N) out[n,] <- {
  M = runif(S^2) * mat
  ## Adjust for McCann type omnivory
  if(!is.null(omega))  {M[omni.i,omni.j] <- omega*M[omni.i+1,omni.j]
                      M[omni.i+1,omni.j] <- (1-omega)*M[omni.i+1,omni.j]
                      M[omni.j,omni.i] <- omega*M[omni.j,omni.i+1]
                      M[omni.j,omni.i+1] <- (1-omega)*M[omni.j,omni.i+1]}
  ## Do eigenanalysis
  eigs <- eigen(M)[["values"]]
  mx <- which.max(Re(eigs))
  deM = Re(eigs)[mx]
  deMi = Im(eigs)[mx]
  intraNDD <- sqrt(sum(diag(M)^2)/S)
  diag(M) <- 0
IS = sqrt( sum(M^2)/(S*(S-1)) )
    omnivory <- sqrt(mean(c(M[omni.i,omni.j],M[omni.j,omni.i])^2))
 c(deM, deMi,intraNDD, IS, omnivory)
  }
}
    return(as.data.frame(out))
}
```

See the arguments in the function.
```{r} 
args(pimmlawton)
```

Now we can check this function for a single simulation for our first web,
```{r}
set.seed(1)
pimmlawton(Aq)
```
Now let's do it 2000 times, as Pimm and Lawton did. Each row will be an independent randomization, and the columns will be the dominant eigenvalue, the intraspecific density dependence, and the average interaction strength.
```{r}
out.A <- pimmlawton(Aq, N=2000) 
```
We might like to look at basic summary statistics of the information we collected --- what are their minima and maxima and mean? 
```{r}
summary(out.A)
```
We see that out of 2000 random food chains, the largest dominant eigenvalue is still less than zero ($\lambda_1<0$). What does that mean? It means that all of the chains are qualitatively stable, and that the return times are greater than zero ($-1/\lambda_1>0$).\footnote{Recall that a negative return time indicates that any ``perturbation'' at the equilibrium would have been closer to zero at some time in the past, i.e., that the perturbation is growing.}

May's work showed that stability is related to interaction strength. Let's examine how the dominant eigenvalue is related to interaction strength.\footnote{Recall that if we think of stability analysis as the analysis of a small perturbation at the equilibrium, then the dominant eigenvalue is the growth rate of that perturbation.}

May used specific quantitative definitions of all of his ideas. He defined connectance as the proportion of interactions in a web, given the total number of all possible directed interactions (i.e., directed connectance). Thus a linear food chain with four species (Fig.  \@ref(A)), and intraspecific competition in the basal (bottom) species would have a connectance of $4/16=0.25$. May's definition of \index{interaction strength, quantified}interaction strength was the square root of the average of all $a_{ij}^2$ ($i \neq j$),
\begin{equation}
  (\#eq:IS)
  I =\sqrt{ \frac{\sum_{i=1}^S\sum_{j=1, i \neq j}^S a_{ij}}{S^2-S}}.
\end{equation}
Squaring the $a_{ij}$ focuses on magnitudes, putting negative and positive values on equal footing.

An important component of May's work explored the properties of *randomly* connected food webs. At first glance this might seem ludicrous, but upon consideration, we might wonder where else one could start. Often, simpler (in this case, random) might be better. The conclusions from the \index{random connection models}random connection models act as null hypotheses for how food webs might be structured; deviations from May's conclusions might be explained by deviations from his assumptions. Since this work, many ecologists have studied the particular ways in which webs in nature appear non-random.

One conclusion May derived was a threshold between stability and instability for random webs, defined by the relation
\begin{equation}
  (\#eq:1)
  I\left( S C_D \right)^{1/2} = 1
\end{equation}
where $I$ is the average \index{interaction strength, average}interaction strength, $S$ is the number of species, and $C_D$ is directed connectance.  If $I\left( S C \right)^{1/2} > 1$, the system tended to be unstable  (Fig. \@ref(fig:may1)). Thus, if we
increase the number of species, we need to decrease the average interaction
strength if we want them to persist. The larger and more tightly connected (larger $I$, $S$, and $C_D$) the
web, the more likely it was to come crashing down. Therefore, if longer food
chains were longer by virtue of having more species, they would be less stable
because of the extra species, if for no other reason.
```{r may1, fig.cap="Relation between the average interaction strength and the number of species able to coexist (here directed connectance is $C_D = 0.3$). The line represents the maximum number of species that are predicted to be able to coexist at equilibrium. Fewer species could coexist, but, on average, more species cannot coexist at equilibrium.", fig.width=5, fig.height=5, results='hide', echo=FALSE}
S <- 40; C=.3
par(mar=c(3,3,0,0), mgp=c(2,.5, 0))
curve(C*x^ -2, .1, 1, ylab=expression("Number of Species ("*italic(S)*")"),
      xlab = expression("Interaction Strength ("*italic(I)*")") )
text(.6, 20, "Unstable Webs", srt=45)
text(.2, 3.75, "Stable Webs" , srt=-45, cex=.8)
```

Pimm and Lawton felt that it seemed reasonable that long chains might be less
stable also because predator-prey dynamics appear inherently unstable, and a
connected series of unstable relations seemed less likely to persist than
shorter chains. They tested whether food chain length
\emph{per se}, and not the number of species, influenced the stability of food
chains. Another question they addressed concerned omnivory. At the time, surveys of naturally occurring
food webs had indicated that omnivory was rare [@Pimm1978]. Pimm and Lawton tested whether omnivory stabilized or destabilized food webs [@Pimm1977;@Pimm1978]. 

Like May, Pimm and Lawton [@Pimm1977] used Lotka--Volterra models to
investigate their ideas. They designed six different food web configurations
that varied food chain length, but held the number of species constant (Fig.
\@ref(fig:PLABE)). For each food web configuration, they varied randomly interaction strength and tested whether an
otherwise randomly structured food web was stable. Their food webs included
negative density dependence only in the basal species. 

Pimm and Lawton concluded that (i) shorter chains were more stable than longer chains, and (ii) omnivory destabilized food webs (Fig. \@ref(fig:RTHist)). While these conclusions have stood the test of time, Pimm and Lawton failed to highlight another aspect of their data --- that omnivory shortened return times for those webs that were qualitatively stable (Fig. \@ref(fig:histsAB)). Thus, omnivory could make more stable those webs that it didn't destroy. Subsequent work has elaborated on this, showing that weak omnivory is very likely to stabilize food webs  [@McCann1998a]. 

\section{Implementing Pimm and Lawton's Methods}
Here we use R~code to illustrate how one might replicate, and begin to extend, the work of Pimm and Lawton \cite{Pimm1977}.

In previous chapters, we began with explicit time derivatives, found partial derivatives and solved them at their equilibria. Rather than do all this, Pimm  and Lawton bypassed these steps and went
straight to the evaluated Jacobian matrix. They inserted  random estimates for
the elements of the Jacobian into each non-zero element in the food web matrix.
These estimates were constrained within (presumably) reasonable
limits, given large less abundant predators and small more abundant prey. 

Their methods followed this approach.

1. Specify a food web interaction matrix.\footnote{In the original publication, webs E and D seem to be
represented incorrectly.}
2. Include negative density dependence for basal species only. 
3. Set upper and lower bounds for the effects of predators on prey ($0$ to
$-10$) and prey on predators($0$ to $+0.1$); these are the Jacobian elements.
4. Generate a large number of random Jacobian matrices and perform linear
stability analysis.
5. Determine qualitative stability (test $\lambda_1<0$), and return time for each random matrix. Use these to examine key features of the distributions of return times (e.g., average return time).
6. Compare the stability and return times among different food web configurations that varied systematically in food chain length and the presence
of omnivory, but hold the number of species constant.


It is worth discussing briefly the \index{Jacobian elements}Jacobian elements. @May1973ab defined interaction strength as the Jacobian element of a matrix, which represents the total effect of one individual on the  population growth rate of another species. Think about how you calculate the Jacobian --- as the partial derivative of one species' growth rate with respect to the size of the other population. It is the instantaneous change in the population growth rate per unit change in the population of another, at the equilibrium. The units chosen for the Jacobian elements thus mean that individual predators have relatively much larger effects on the population growth rates of prey than *vice versa*.

Let's build a function that does what Pimm and Lawton did. There are an
infinite number of ways to do this, but this will suffice. First, we'll create a
matrix that represents qualitatively the simplest longest food chain  where each species feeds only on one other species and where no prey are fed upon by more than one consumer. Initially, we will use the values of greatest magnitude used by Pimm and
Lawton. 
```{r}
Aq = matrix(c(
  -1,   -10,   0,     0,
  0.1, 0,   -10,  0,
  0,    0.1,   0,    -10,
  0,      0,    0.1,   0),
  nrow=4, byrow=TRUE)
```
Note that this matrix indicates a negative effect of the basal species on itself, large negative effects ($-10$) of each consumer on its prey, and small positive effects of each prey on its consumer.

For subsequent calculations, it is convenient to to find out from the matrix itself how big the matrix is, that is, how many species, $S$, are in the web.
```{r}
S <- nrow(Aq)
```
Next, we create a random realization of this matrix by multiplying each element times a unique random number between zero and 1. For this matrix, that requires $4^2$ unique numbers.
```{r}
M <- Aq * runif(S^2)
```
Next we perform eigenanalysis on it, retaining the eigenvalues.
```{r}
eM <- eigen(M)[["values"]]
```
Pimm and Lawton tested whether the dominant eigenvalue was greater than 0 (unstable) and if less than zero, they calculated return time. We will simply record the dominant eigenvalue (the maximum of the real parts of the eigenvalues). 
```{r}
deM <- max( Re(eM) )
```
Given the stabilizing effect of the intraspecific negative density dependence, we will hang on to that as well.
```{r}
intraNDD <- sqrt(sum(diag(M)^2)/S)
```
Given lessons from May's work, we might also want to calculate the average interaction strength, not including the intraspecific interactions. Here we set the diagonal interactions equal to zero, square the remaining elements, find their average, and take the square root.
```{r}
diag(M) <- 0
IS <- sqrt( sum(M^2)/(S*(S-1)) )
```
Recall that weak omnivory is supposed to stabilize food webs \cite{McCann1997}. For webs that include omnivory, we will calculate the interaction strength of omnivory in the same way we do for other interactions, as the square root of the average of the squared $a_{ij}$ (eq. \@ref(eq:IS).

We can wrap all this up in a function where we specify the $i,\,j$ of one of the directed omnivorous links.\footnote{It does not matter which we specify, either the $ij$ or the $ji$.}
```{r PL, echo=FALSE, results='hide', eval=FALSE}
pimmlawton <- function(mat, N=1, omni.i=NA, omni.j=NA, omega=NULL){
  S <- nrow(mat)
  if(is.na(omni.i)) {
  out <- matrix(NA, nrow=N, ncol=4)
    colnames(out) <- c("DomEig", "Im", "IntraDD", "I")
  for(n in 1:N) out[n,] <- {
  M = runif(S^2) * mat
  ## Do eigenanalysis
  eigs <- eigen(M)[["values"]]
  mx <- which.max(Re(eigs))
  deM = Re(eigs)[mx]
  deMi = Im(eigs)[mx]
  intraNDD <- sqrt(sum(diag(M)^2)/S)
  diag(M) <- 0
IS = sqrt( sum(M^2)/(S*(S-1)) )
 c(deM, deMi, intraNDD, IS)
  } } else {
  out <- matrix(NA, nrow=N, ncol=5)
    colnames(out) <- c("DomEig", "Im", "IntraDD", "I", "I.omni")
  for(n in 1:N) out[n,] <- {
  M = runif(S^2) * mat
  ## Adjust for McCann type omnivory
  if(!is.null(omega))  {M[omni.i,omni.j] <- omega*M[omni.i+1,omni.j]
                      M[omni.i+1,omni.j] <- (1-omega)*M[omni.i+1,omni.j]
                      M[omni.j,omni.i] <- omega*M[omni.j,omni.i+1]
                      M[omni.j,omni.i+1] <- (1-omega)*M[omni.j,omni.i+1]}
  ## Do eigenanalysis
  eigs <- eigen(M)[["values"]]
  mx <- which.max(Re(eigs))
  deM = Re(eigs)[mx]
  deMi = Im(eigs)[mx]
  intraNDD <- sqrt(sum(diag(M)^2)/S)
  diag(M) <- 0
IS = sqrt( sum(M^2)/(S*(S-1)) )
    omnivory <- sqrt(mean(c(M[omni.i,omni.j],M[omni.j,omni.i])^2))
 c(deM, deMi,intraNDD, IS, omnivory)
  }
}
    return(as.data.frame(out))
}
```
```{r}
args(pimmlawton)
```
Now we can check this function for a single simulation for our first web,
```{r}
set.seed(1)
pimmlawton(Aq)
```
Now let's do it 2000 times, as Pimm and Lawton did. Each row will be an independent randomization, and the columns will be the dominant eigenvalue, the intraspecific density dependence, and the average interaction strength.
```{r}
out.A <- pimmlawton(Aq, N=2000) 
```
We might like to look at basic summary statistics of the information we collected --- what are their minima and maxima and mean? 
```{r}
summary(out.A)
```
We see that out of 2000 random food chains, the largest dominant eigenvalue is still less than zero ($\lambda_1<0$). What does that mean? It means that all of the chains are qualitatively stable, and that the return times are greater than zero ($-1/\lambda_1>0$).\footnote{Recall that a negative return time indicates that any ``perturbation'' at the equilibrium would have been closer to zero at some time in the past, i.e., that the perturbation is growing.}

May's work showed that stability is related to interaction strength. Let's examine how the dominant eigenvalue is related to interaction strength.\footnote{Recall that if we think of stability analysis as the analysis of a small perturbation at the equilibrium, then the dominant eigenvalue is the growth rate of that perturbation.}
```{r pairsA, fig.cap="Perturbations at the equilibrium tend to dissipate more rapidly (more negative dominant eigenvalues) with greater intraspecific negative density dependence (IntraDD) and greater interspecifiic interaction strength (I). This graph also demonstrates the independence of IntraDD and I in these simulations.", f.igheight=8, fig.width=8}
pairs(out.A)
```


The results of our simulation (Fig. \@ref(fig:pairsA) show that the dominant eigenvalue can become more negative with greater intraspecific negative density dependence (\textsf{IntraDD}) and greater intersepcifiic interaction strength (\textsf{I}). Recall what this means --- the dominant eigenvalue is akin to a perturbation growth rate at the equilibrium and is the negative inverse of return time. Therefore, stability can increase and return time decrease with increasing interaction strengths.

Note also (Fig. \@ref(fig:pairsA)) that many eigenvalues seem very close to zero --- what does this mean for return times?
The inverse of a very small number is a very big number, so it appears that many return times will be very, very large, and rendering the webs effectively unstable. Let's calculate return times and examine a summary.
```{r}
RT.A <- -1/out.A[["DomEig"]]
summary(RT.A)
```
We find that the maximum \index{return time}return time is a very large number, and even the median is fairly large (```r round( median(RT.A) )```). In an ever-changing world, is there any meaningful difference between a return time of 1000 generations *vs.* neutral stability? 

Pimm and Lawton addressed this by picking an arbitrarily large number (150) and recording the percentage of return times greater than that. This percentage will tell us the percentage of webs that are not effectively stable.
```{r}
sum( RT.A > 150 ) / 2000
```
Now let's extract the return times that are less than or equal to
150 and make a histogram with the right number of divisions or bins to allow it to look like the one in the original [@Pimm1977].
```{r histA}
A.fast <-RT.A[RT.A < 150]
histA <- hist(A.fast, breaks=seq(0,150, by=5), main=NULL)
```
This histogram (Fig. \@ref(hA)) provides us with a picture of the stability for a food chain like that in Fig. \@ref(A). Next, we will compare this to other webs.


### Shortening the Chain
Now let's repeat all this (more quickly) for a shorter chain, but with the same number of species (Fig. \@ref(E). So, we first make the web function. Next we run the 2000 simulations, and check a quick summary.
```{r}
Eq = matrix(c(
  -1,   0,   0, -10,
  0,   -1,   0, -10,
  0,    0,   -1, -10,
  0.1, 0.1, 0.1,  0),
  nrow=4, byrow=TRUE)

out.E <- pimmlawton(Eq, N=2000)
summary(out.E)
```
The summary shows that, again, that all webs are stable ($\lambda_1<0$). A histogram of return times also shows very short return times (Fig. \@ref(hE). Plots of $\lambda_1$ \emph{vs.} interaction strengths show that with this short chain, and three basal species that the role of intraspecfic density dependence becomes even more important, and the predator-prey interactions less important in governing $\lambda_1$. 
```{r E1, fig.cap="For a food chain with two levels, and three basal species, perturbation growth rate (lambda_1) declines with increasing intraspecific negative density dependence (IntraDD) and is unrelated to predator-prey interaction strengths.", out.width=9}
layout(matrix(1:2,nr=1))
plot(DomEig ~ IntraDD, data=out.E)
plot(DomEig ~ I, data=out.E)
```

Note that with the shorter food chain, a greater proportion of the $\lambda_1$ are more negative (farther away from zero) than in the four level food chain. Clearly then, shortening the web stabilizes it, in spite of still having the same \emph{number} of species.

Let us again categorize these as having long and short \index{return time}return times, and graph the distribution of the short ones.
```{r histE}
RT.E <- -1/out.E[["DomEig"]]
E.fast <-RT.E[RT.E < 150]
histE <- hist(E.fast, breaks=seq(0,150, by=5), main=NULL)
```

### Adding Omnivory
Real webs also have \index{omnivory}omnivory --- feeding on more than one trophic level. A nagging question, then and now, concerns the effect of omnivory on food web dynamics. Pimm and Lawton compared food web dynamics with and without omnivory. Let's now create the web (Fig. \@ref(B)) that they used to compare directly with their linear food chain (Fig. \@ref(A)). Next we run the 2000 simulations, and check a quick summary.
```{r}
Bq = matrix(c(
  -1, -10,   0,     0,
  0.1,  0,   -10,   -10,
  0,    0.1,   0,    -10,
  0,    0.1,    0.1,   0),
  nrow=4, byrow=TRUE)

out.B <-  pimmlawton(Bq, N=2000, omni.i=2, omni.j=4)
summary(out.B)
```
With omnivory, we now see that most webs have $\lambda_1>0$, and thus are \emph{unstable}. This was one of the main points made by Pimm and Lawton. Let's look at the data.
```{r}
pairs(out.B)
oBs <- as.data.frame(scale(out.B))
summary(oBs)
mB <- lm(DomEig ~ IntraDD + I + I.omni, data=oBs)
plot(mB)
summary.aov(mB)
summary(mB)
```
It means that most of the randomly constructed webs were not stable point equilibria. To be complete, let's graph what Pimm and Lawton did.
```{r histB}
RT.B <- -1/out.B[["DomEig"]]
B.fast <- RT.B[RT.B < 150 & RT.B>0 ]
out.B.fast <- out.B[RT.B < 150 & RT.B>0 ,]
out.B.stab <- out.B[RT.B>0 ,]
histB <- hist(B.fast, breaks=seq(0,150, by=5),
              main=NULL)
```

### Comparing Chain A versus B
Now let's compare the properties of the two chains, without, and with, omnivory, chains \textbf{A} and \textbf{B} (Figs. \@ref(A), \@ref(B)). Because
these are stochastic simulations, it means we have \emph{distributions} of
results. For instance, we have a distribution of return times for chain
\textbf{A} and a distribution for return times for chain \textbf{B}. That is,
we can plot histograms for each of them. Pimm and Lawton compared their webs in
common sense ways. They compared simple summaries, including
\begin{itemize}
\item the proportion of random webs that were stable (positive return times),
  \item the proportion of stable random webs with return times greater than
150.
\end{itemize}

Now let's try graphical displays. Rather than simply chopping off the long return times, we use base 10 logarithms of return times because the
distributions are so right-skewed. We create a histogram\footnote{Note that now we use probabilities for the $y$-axis, rather than counts. The probability associated with any particular return time is the product of the height of the column and the width of the column (or bin).} of the return times for chain \textbf{A}, and nonparametric density functions for both chain \textbf{A} and \textbf{B}.\footnote{These density smoothers do a good job describing empirical distributions of continuous data, often better than histograms, which have to create discrete categories or ``bins'' for continuous data.}
```{r histsAB, fig.cap="Comparing the distributions of return times for chain **A** and \textbf{B}. "Density" is probability density. The distribution of return times for chain \textbf{A} is the solid line, and the distribution of return times for chain \textbf{B} is the dashed line."}
hist(log(RT.A,10), probability=T, ylim=c(0,1),
     main=NULL, 
     xlab=expression(log[10]("Return Time"))
     )
lines(density(log(RT.A,10)) )
lines(density(log(RT.B[RT.B>0],10)), lty=2, lwd=2 )
legend("topright", c("Chain A", "Chain B"), lty=1:2, 
       lwd=1:2,  bty='n')
```

By overlaying the density function of web B on top of web A return times (Fig. \@ref(fig:histsAB)), we make an interesting observation. The omnivorous webs with positive return times (those plotted) actually tended to have shorter return times than the linear chain. Pimm and Lawton noted this, but did not emphasize it. Rather, they sensibly focused on the more obvious result, that over 90\% of the omnivorous webs had negative return times, indicating an absence of a stable point equilibrium.

## Re-evaluating Take-Home Messages
The primary messages made by @Pimm1977 were that 
\begin{itemize}
\item shorter webs are more stable than long chains,
  \item omnivory destabilized webs.
\end{itemize}

These conclusions were a major part of the lively debate surrounding these issues. It was consistent with the apparent observation of the time, that empirical food webs revealed little omnivory [@Pimm1977;@Pimm1978], and that food chains in nature seemed to be much shorter than could occur, if \index{primary productivity}primary productivity (green plants and algae) was channeled upwards into a linear chain. 

Let's consider their assumptions.

First, Pimm and Lawton made the argument, as many others have (including us), that systems with stable point equilibria are more likely to persist than systems with oscillations, such as stable limit cycles. That is, we presume a strong correlation between the tendency to oscillate, and the tendency for species to become extinct (i.e., the system to collapse). It is easy to show that a system can be pushed from a stable equilibrium into oscillations which eventually become so big as to drive an element of the system to extinction. This is a very reasonable assumption, but one which is not challenged enough. Other measures of system stability could be used, such as the minimum that occurs in a long series of fluctuations [@Huxel1998;@McCann1997]. 

Second, Pimm and Lawton ignored the effects of self-regulated basal species. By exhibiting negative density dependence, the basal species stabilized the web. When Pimm and Lawton made a shorter web, they also added more self-regulated populations. Thus, they necessarily confounded chain length with the number of species with negative density dependence. Which change caused the observed differences among webs? We don't know.

Third, they assumed that the effect of web topology (i.e., short \emph{vs.} long chain) was best evaluated with the \emph{average} properties of the topology, rather than the maximum properties of the topology. By these criteria, webs without omnivory were clearly better. On average, webs without omnivory were more often stable than chains with omnivory, even if some of their return times tended to be quite long. Therefore, one might argue that if a web assembles in nature, it is more likely to persist (i.e., be stable) if it lacks omnivory. 

However, let us consider this preceding argument further. The world is a messy place, with constant insults and disturbances, and resources and environmental conditions fluctuating constantly. In addition, there is a constant rain of propagules dropping into communities, and species abundances are changing all the time. In a sense then, communities are being constantly perturbed. The only webs that can persist in the face of this onslaught are the \emph{most} stable ones, that is the ones with the shortest return times. We just showed that Pimm and Lawton's own analyses showed that \emph{the most stable webs tended to be those with \index{omnivory, stabilizing}omnivory}. Subsequent work supports this claim that omnivory is rampant in nature \cite{Polis1991}, and this is supported by theory that shows weak interactions, including omnivory, stabilize food webs [@McCann1998;@McCann1997].

Pimm and Lawton made very important contributions to this lengthy debate, and we are finally figuring out how to interpret their results.



<!--chapter:end:10-foodwebs.Rmd-->

# Food webs

A food web is a real or a model of a \emph{set of feeding relations among species or functional groups}. This chapter has two foci, (i) a very brief introduction to multi-species webs as networks, and (ii) a re-examination of old lessons regarding the effect of food chain length on a web's dynamical properties.


```{r web, echo=FALSE, fig.cap="A food web is a map of feeding relations. Here, snails (S) and invertebrates (I) feed on algae, and fish, F, feed on invertebrates and algae. We can represent this as a graph or a matrix.", out.width="40%", fig.show='hold', fig.ncol=2, fig.height=5, fig.width=5}
gid <- graph(c('S', 'A', 'A', 'S', 
               'I', 'A', 'A', 'I', 
               'F','I', 'I', 'F',
               'F','A', 'A', 'F'))
signs <- c("-",'+', "-",'+', "-",'+', "-",'+')
plot(gid, layout=matrix(c(0, .5, 1, .6, 0.5, 0.1, 0.5, .85), nc=2), edge.curved=.5,      edge.label=signs, edge.label.font=2,  edge.label.cex=1.5, 
     vertex.size=60, 
     margin=c(0,0,0,0), rescale=TRUE)
```
\begin{tabular}{c| c c c c }
& \multicolumn{4}{c}{\emph{~Consumers}}\\
\emph{~Resources~} & ~A~ & ~I~ & ~S~ & ~F~\\
\hline
A&0&$-$&$-$&$-$\\
I&$+$&0&0&$-$\\
S&$+$&0&0&0\\
F&$+$&$+$&0&0\\
\end{tabular}

## Food Web Characteristics
\index{food web characteristics}
We need language to describe the components of a food web, such as *links* and \emph{\index{nodes}nodes}, and we also need language to describe the properties of a web as a whole. Generally speaking, networks such as food webs have \emph{emergent properties} [@May:2006lr;@Strogatz:2001gf], such as the number of nodes in the network. Emergent properties are typically considered to be nothing more than characteristics which do not exist at simpler levels of organization. For instance, one emergent property of a population is its density, because population density cannot be measured in an individual; that is why density is an \index{emergent property}emergent property.^[If we assume no supernatural or spectral interference, then we can also assume that density arises mechanistically from complicated interactions among individuals and their environments, rather than via magic. Other scientists will disagree and say that properties like density that are merely additive aggregates do not qualify for the lofty title of "emergent property."] While all food web models are based on simple pairwise interactions, the resulting emergent properties of multispecies webs quickly become complex due to indirect interactions and coupled oscillations [@Berlow:2004yq;@Vandermeer:2006fj].  In addition, any extrinsic factor (e.g., seasonality) that might influence species interactions may also influence the emergent properties of food webs. 


A few important \index{network}network descriptors and emergent properties include,

* **Node** A point of connection among links, a.k.a. \index{trophospecies}trophospecies; each node in the web may be any set of organisms that share sufficiently similar feeding relations; in Fig. \@ref(fig:web), $P$ may be a single population of one species, or it may be a suite of species that all feed on both $B$ and $R$. 
* **Link** A feeding relation; a connection between nodes or trophospecies; may be directed (one way) or undirected. A directed link is indicated by an arrow, and is the effect ($+,\,-$) of one species on another. An undirected link is indicated by a line (no arrow head), and is merely a connection, usually with positive and negative effects assumed, but not quantified. 
* **Connectance** The proportion of possible links realized. Connectance may be based on either directed, $C_{D}$, or undirected, $C_{U}$, links. For Fig. \@ref(fig:web) these would be
\begin{gather*}
C_{D}=\frac{L}{S^{2}}=\frac{8}{16}=0.5 \\
C_{U}=\frac{L}{\left(S^2-S\right)/2}=\frac{4}{6}=0.67
\end{gather*}
where $S$ is the number of species or nodes.
* **Trophic level, trophic position** may simply be categorized as basal, intermediate or top trophic positions. Basal positions are those in which the trophospecies feed on no other species. The top positions are those in which the trophospecies are fed upon by nothing. One can also calculate a quantitative measure of trophic \emph{level}. This is important in part because omnivory, something rampant in real food webs, complicates identification of a trophic level. We can calculate \index{trophic level}trophic level for the top predator, in Fig. \@ref(fig:web). Let us assume that the top predator, $F$, gets two-thirds of its energy from $I$, and gets one-third from $A$. $I$ itself is on the second trophic level, so given that, the trophic level of $F$ is calculated as
\begin{equation*}
T_{F}=1+\sum_{j=1}^{S}T_{j}p_{Fj}=1+\left(2\left(0.67\right)+1\left(0.33\right)\right)=2.67
\end{equation*}
where $T_{F}$ is the trophic level of $F$, $T_{j}$ is the trophic level of prey species $j$, and $p_{ij}$ is the proportion of the diet of predator $i$ consisting of prey $j$.
* **Omnivory** Feeding on more than one \emph{trophic} level ($\nu>0$, Fig. \@ref(fig:web)); it is \emph{not} merely feeding on different species or resources.
* **Intraguild predation** A type of omnivory in which predation occurs between consumers that share a resource; in Fig. \@ref(fig:web) $F$ and $I$ share prey $A$. When $F$ gets most of its energy from $I$, we typically refer to that as omnivory ($\ nu<0.5$); when $F$ gets most of its energy from $A$, we typically refer to that as intraguild predation ($\nu >0.5$).

* **Degree distribution**, $\mathrm{Pr}(i)$ The probability that a randomly chosen node will have degree $i$, that is, be connected to $i$ other nodes [@May:2006lr]. In Fig. \@ref(fig:web), $A$ is of degree 1 (i.e., is connected to one other species).  $P$ and $B$ are of degree 2, and $R$ is of degree 3. If we divide through by the number of nodes (4, in Fig. \@ref(fig:web)), then the \emph{degree distribution} consists of the probabilities $\mathrm{Pr}\left(i\right)=\{0.25,\,0.5,\,0.25\}$. As webs increase in size, we can describe this distribution as we would a statistical distribution. For instance, for a web with randomly placed connections, the degree distribution is the binomial distribution [@Cohen1990d].
* **Characteristic path length** Sometimes defined as the average shortest path between any two nodes [@Dunne2002]. For instance, for Fig. \@ref(fig:web), the shortest path  between $P$ and $R$ is 1 link, and between $A$ and $P$ is 2 links. The average of all pairwise shortest paths is $(1+1+1+1+2+2)/6=1.\bar{3}$. It is also sometimes defined as the average of \emph{all paths} between each pair of nodes.
* **Modularity, Compartmentation** The degree to which subsets of species are highly connected or independent of other species. This tends to arise in consumer-resource interactions [@Thebault2010]
  
As one way calculate \index{compartmentation}modularity in a food web, first assume each species interacts with itself. Next, calculate the proportion of shared interactions, $p_{ij}$ for each pair of species, by comparing the lists of species with which each species in a pair interacts. The numerator of $p_{ij}$ is the number of species with which both of the pair interact. The denominator is the total number of different species with which either species interacts. \medskip

As an example, let's calculate this for the above food web (Fig. \@ref(fig:web)). $S$ interacts with $S$ and $A$, $I$ interacts with $I$, $S$, and $F$. Therefore, $S$ and $I$ both interact with only $A$, whereas, together, $S$ and $I$ interact with $S$, $I$, $A$, and $F$. The proportion, $p_{ij}$, therefore is $1/4 = 0.25$. We do this for each species pair. \smallskip

Next we sum the proportions, and  divide the sum by the maximum possible number of undirected links, $C_U$.
\begin{table}[h]
\begin{minipage}[c]{.48\linewidth}
  \centering
\begin{tabular}{l| c c c }
Species & S & I & F\\
\hline
A&2/4&3/4&3/4\\
S&  &1/4&1/4\\
I&  &  &3/3 
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}[c]{.48\linewidth}
\begin{align*}
  (\#eq:ci)
C_{I}&=\frac{\sum_{i=1}^{S-1}\sum_{j=i+1}^{S}p_{ij}}{\left(S^2-S\right)/2}\\
    &=3.5/6\\
    &=0.58
\end{align*}
  \end{minipage}
\end{table}
To reiterate: For any pair of species, $i$ and $j$ ($i\neq j$), $p_{ij}$ is the proportion of shared interactions, calculated from the number of species that interact with \emph{both} species
$i$ and $j$, divided by the number of species that interact
with \emph{either} species $i$ or species $j$. As above, $S$ is
the number of species or nodes in the web.

* **Nestedness** the degree to which specialists make connections that are subsets of the connections that generalists make. A completely nested network is one in which we can rank species by the number of interactions, and find that species that interact with fewer other species are always interacting with species that generalists also interact with. This tends to arise in plant-pollinator networks [@Thebault2010].
* **Motif** A repeating pattern; most commonly, a unique set of unidirectional patterns of consumption (Fig. \@ref(fig:motifs)). A motif *profile* of a species is the variety of motifs that that species is part of, and the relative frequency with which they play each role. 
```{r motifs, echo=FALSE, fig.cap="There are 13 different possible motifs for three species. These links record only consumption. For instance, S1 is a simple linear food chain and S4 is a single species that is consumed by two other species. motifs D1-D8 all include mutual consumption. This is likely to occur between species in the same guild, and when adults of one feed on juveniles of another.", message=FALSE}
include_graphics("figs/motifs")
```
* **Random, scale-free, and small world, architecture** These refer to how connections are arranged within the network, and slightly different definitions exist for each. Random networks are often defined by connections between nodes occurring at random, given a constant probability of connection between any two nodes. A small-world network is one in which characteristic path length, $L$, grows with the logarithm of the number of nodes in the web, $L \propto log(N)$ and where nodes are connected via a small number of links through a node with a large number of connections. This occurs in social networks when a few individuals are connected to many people and to the study of 'degress of separation'. A scale-free network is one in which the degree distribution $Pr(i)$ of nodes $i$ follows a power law distribution, where $Pr(i) \backsim d^{-z}$; if you rank the nodes from lweast to most to least connected, the plot of the logarithm of th number of connections will fall on a straight line with slope $-z$.

This list of food web descriptors is a fine start but is by no means exhaustive.

## Food chain length --- an emergent property
There are many interesting questions about food webs that we could address; let us address one that has a long history, and as yet, no complete answer: What determines the length of a \index{food chain length}food chain? Some have argued that chance plays a large role [@Cohen1990d;@Hubbell2001;
@Williams2000a],  and others have shown that area
[@MacArthur1967; @Rosenzweig1995] or ecosystem size [@Post:2002bs] may
play roles. The explanation that we will focus on here is *dynamical stability*. Communities with more species had been hypothesized to be less stable, and therefore less likely to persist and be observed in nature. Stuart Pimm and John Lawton [@Pimm1977] extended this work by testing whether *food chain
length* could be limited by the instability of long food chains.

### Multi-species Lotka--Volterra notation
A many-species \index{Lotka--Volterra!food web}Lotka--Volterra model can be represented in a very compact form,
\begin{equation}
(\#eq:compact)
\frac{d X_{i}}{d t}=X_{i}\left(b_{i}+\sum_{j=1}^{S}a_{ij}X_{j}\right)
\end{equation}
where $S$ is the number of species in the web, $b_i$ is the intrinsic rate of increase of species $i$ (i.e., $r_i$), and $a_{ij}$ is a per capita effect of species $j$ on species $i$. 

When $i=j$, $a_{ij}$ refers to an *intra*specific effect, which is typically negative. Recall that in our earlier chapters on competition, we used $\alpha_{ii}$ to represent intraspecific per capita effects. Here for notational convenience, we leave $i$ and $j$ in the equation, realizing that $i=j$ for intraspecific interactions. Further, we let $a_{ij}$ be any sign, either positive or negative, and sum the effects. If we let $X=N$, $b=r$, and $a=r \alpha$, then the following are equivalent:
\begin{gather*}
  \dot{N_1} =r_1N_1\left(1-\alpha_{11}N_1 - \alpha_{12}N_2\right)\\
  \dot{X} =X_1\left(b_1+a_{11}X_1+a_{12}X_2 \right)
\end{gather*} 
The notation in \@ref(eq:compact) is at once both simple and flexible. When $a_{ij}$ is negative, it may represent competition or the effect of a predator, $j$, on its prey, $i$. When $a_{ij}$ is positive, it may represent mutualism or the effect of prey~$j$ on a predator~$i$.

### Background
In the early and mid-1970's, Robert May and others
 demonstrated that important predictions could be made with relatively simple Lotka--Volterra models [@May1973ab], and this work still comprises an important compendium of lessons for ecologists today [@May:2006lr]. May used simple Lotka--Volterra models to show that increasing the number of species in a food web tended to make the food web less stable [@May1973ab; @May1972]. In species-rich webs, species were more likely to become extinct. Further, he showed that the more connections there were among species in the web (higher connectance), and the stronger those connections (higher interaction strength), the *less* stable the web. At the time, this ran counter to a prevailing sentiment that more diverse ecosystems were more stable, and led to heated discussion.
```{r PLABE, echo=FALSE, fig.cap="Three of the six webs investigated by Pimm and Lawton (1977). Left to right, these correspond to Pimm and Lawton (1977) Figs. 1A, E, and B. All basal species exhibit negative density dependence.", out.width=c("17%","55%","20%"), fig.show='hold', fig.ncol=3, fig.height=10, fig.width=5}
resc <- FALSE
gA <- graph(c(1,1, 1,2,2,1, 2,3,3,2, 3,4,4,3))
signs <- c("-", '+',"-", '+',"-", '+',"-")
plot(gA, layout=matrix(c(.5,.5,.5,.5, 0,.33,.67,1), nc=2), edge.curved=.5,      edge.label=signs, edge.label.font=1,  edge.label.cex=1, 
     vertex.size=10, edge.loop.angle=pi/4,
     margin=c(0,0,0,0), rescale=resc)

gE <- graph(c(1,1, 2,2, 3,3, 1,4,4,1, 2,4,4,2, 3,4,4,3))
signs <- c("-","-","-", '+',"-", '+',"-", '+',"-")
plot(gE, layout=matrix(c(0,.5, 1,.5, 0,0,0,.5), nc=2), edge.curved=.5,      edge.label=signs, edge.label.font=1,  edge.label.cex=1, 
     vertex.size=10, edge.loop.angle=pi/4,
     margin=c(0,0,0,0), rescale=resc)

gB <- graph(c(1,1, 1,2,2,1,  2,3,3,2, 3,4,4,3, 2,4,4,2))
signs <- c("-", '+',"-",  '+',"-",  '+',"-", '+',"-")
plot(gB, layout=matrix(c(.5,.5,.5,.4,  0,.33,.67,1), nc=2), edge.curved=.5, 
     edge.label=signs, edge.label.font=1,  edge.label.cex=1, 
     vertex.size=10, edge.loop.angle=pi/4,
     margin=c(0,0,0,0), rescale=resc)
```

May defined connectance as the proportion of interactions in a web, given the total number of all possible directed interactions (i.e., directed connectance). Thus a linear food chain with four species (Fig. \@ref(fig:PLABE)), and intraspecific competition in the basal species would have a connectance of $7/16=0.25$. May's definition of \index{interaction strength, quantified}interaction strength was the geometric mean of all interspecific interactions, i.e. the square root of the average of all $a_{ij}^2$ ($i \neq j$),
\begin{equation}
  (\#eq:IS)
  I =\sqrt{ \frac{\sum_{i=1}^S\sum_{j=1, i \neq j}^S a_{ij}}{S^2-S}}.
\end{equation}
Squaring the $a_{ij}$ focuses on magnitudes, putting negative and positive values on equal footing.

An important component of May's work explored the properties of *randomly* connected food webs. At first glance this might seem ludicrous, but upon consideration, we might wonder where else one could start. Often, simpler (in this case, random) might be better. The conclusions from the \index{random connection models}random connection models act as null hypotheses for how food webs might be structured; deviations from May's conclusions might be explained by deviations from his assumptions. Since this work, many ecologists have studied the particular ways in which webs in nature appear non-random.

One conclusion May derived was a threshold between stability and instability for random webs, defined by the relation
\begin{equation}
  (\#eq:1)
  I\left( S C \right)^{1/2} = 1
\end{equation}
where $I$ is the average \index{interaction strength, average}interaction strength, $S$ is the number of species, and $C$ is directed connectance.  If $I\left( S C \right)^{1/2} > 1$, the system tended to be unstable  (Fig. \@ref(fig:may1)). Thus, if we increase the number of species, we need to decrease the average interaction strength if we want them to persist. The larger and more tightly connected (larger $I$, $S$, and $C_D$) the web, the more likely it was to come crashing down. Therefore, if longer food chains were longer by virtue of having more species, they would be less stable because of the extra species, if for no other reason.
```{r may1, fig.cap="Relation between the average interaction strength and the number of species able to coexist (here directed connectance is $C = 0.3$). The line represents the maximum number of species that are predicted to be able to coexist at equilibrium. Fewer species could coexist, but, on average, more species cannot coexist at equilibrium.", fig.width=5, fig.height=5, results='hide', echo=FALSE}
S <- 40; C=.3
par(mar=c(3,3,0,0), mgp=c(2,.5, 0))
curve(C*x^ -2, .1, 1, ylab=expression("Number of Species ("*italic(S)*")"),
      xlab = expression("Interaction Strength ("*italic(I)*")") )
text(.6, 20, "Unstable Webs", srt=45)
text(.2, 3.75, "Stable Webs" , srt=-45, cex=.8)
```

Pimm and Lawton felt that it seemed reasonable that long chains might be less
stable also because Lotka-Volterra predator-prey dynamics appear inherently unstable, and a connected series of unstable relations seemed less likely to persist than shorter chains. They tested whether food chain length \emph{per se}, and not the number of species, influenced the stability of food chains. Another question they addressed concerned omnivory. At the time, surveys of naturally occurring food webs had indicated that omnivory was rare [@Pimm1978]. Pimm and Lawton tested whether omnivory stabilized or destabilized food webs. 

Like May, @Pimm1977 used Lotka--Volterra models to
investigate their ideas. They designed six different food web configurations
that varied food chain length, but held the number of species constant (Fig.
\@ref(fig:PLABE)). For each food web configuration, they varied randomly interaction strength and tested whether an
otherwise randomly structured food web was stable. Their food webs included
negative density dependence only in the basal species. 

Pimm and Lawton concluded that (i) shorter chains were more stable than longer chains, and (ii) omnivory destabilized food webs. While these conclusions have stood the test of time, Pimm and Lawton failed to highlight another aspect of their data---that, for those webs that were qualitatively stable, omnivory shortened return times  (Fig. \@ref(fig:histsAB)). Thus, omnivory could make more stable those webs that it didn't destroy. Subsequent work has elaborated on this, showing that weak omnivory is very likely to stabilize food webs  [@McCann1998a]. 

## Implementing Pimm and Lawton's Methods
Here we use R code to illustrate how one might replicate, and begin to extend, the work of @Pimm1977.

In previous chapters, we began with explicit time derivatives, found partial derivatives and solved them at their equilibria. Rather than do all this, Pimm  and Lawton bypassed these steps and went straight to the evaluated Jacobian matrix. They inserted  random estimates for the elements of the Jacobian into each non-zero element in the food web matrix. These estimates were constrained within (presumably) reasonable limits, given large less abundant predators and small more abundant prey. 

Their methods followed this approach.

1. Specify a food web interaction matrix.^[In the original publication, webs E and D seem to be represented incorrectly.]
2. Include negative density dependence for basal species only. 
3. Set upper and lower bounds for the effects of predators on prey ($0$ to $-10$) and prey on predators($0$ to $+0.1$); these are the Jacobian elements.
4.  Generate a large number of random Jacobian matrices and perform linear stability analysis.
5. Determine qualitative stability (test $\lambda_1<0$), and return time for each random matrix. Use these to examine key features of the distributions of return times (e.g., average return time).
6.  Compare the stability and return times among different food web configurations that varied systematically in food chain length and the presence of omnivory, but hold the number of species constant.

It is worth discussing briefly the \index{Jacobian elements}Jacobian elements. @May1973ab defined interaction strength as the Jacobian element of a matrix, which represents the *total effect of one individual on the  population growth rate of another species*. Think about how you calculate the Jacobian --- as the partial derivative of one species' growth rate with respect to the size of the other population. It is the instantaneous change in the population growth rate per unit change in the population of another, at the equilibrium. The units chosen for the Jacobian elements thus mean that individual predators have relatively much larger effects on the population growth rates of prey than \emph{vice versa}.

Let's build a function that does what Pimm and Lawton did. There are an infinite number of ways to do this, but this will suffice. First, we'll create a matrix that represents qualitatively the simplest longest food chain where each species feeds only on one other species and where no prey are fed upon by more than one consumer. Initially, we will use the values of greatest magnitude used by Pimm and Lawton. 
```{r}
Aq = matrix(c(
  -1,   -10,   0,     0,
  0.1, 0,   -10,  0,
  0,    0.1,   0,    -10,
  0,      0,    0.1,   0),
  nrow=4, byrow=TRUE)
Aq
```
Note that this matrix indicates a negative effect of the basal species on itself, large negative effects ($-10$) of each consumer on its prey, and small positive effects of each prey on its consumer.^[For what sorts of species does this make sense...or nonsense?]

For subsequent calculations, it is convenient to to find out from the matrix itself how big the matrix is, that is, how many species, $S$, are in the web.
```{r}
S <- nrow(Aq)
```
Next, we create a random realization of this matrix by multiplying each element times a unique random number between zero and 1. For this matrix, that requires $4^2$ unique numbers.
```{r}
M <- Aq * runif(S^2)
```
Next we perform eigenanalysis on it, retaining the eigenvalues.
```{r}
eM <- eigen(M)[["values"]]
```
Pimm and Lawton tested whether the dominant eigenvalue was greater than 0 (unstable) and if less than zero, they calculated return time. We will simply record the dominant eigenvalue as the maximum of the real parts of the eigenvalues.
```{r}
deM <- max( Re(eM) )
```
Given the typically stabilizing effect of the intraspecific negative density dependence, we will hang on to that as well, as the geomtric mean of all the intraspecific negative density dependencies.
```{r}
intraNDD <- sqrt(sum(diag(M)^2)/S)
```
Given lessons from May's work, we might also want to calculate the average interspecific interaction strength. Here we set the diagonal interactions equal to zero, and calculate the geometric mean of the square the remaining elements.
```{r}
diag(M) <- 0
IS <- sqrt( sum(M^2)/(S*(S-1)) )
```
Twenty years later, @McCann1997 showed that weak omnivory can stabilize food webs. For webs that include omnivory, we will calculate the interaction strength of omnivory in the same way we do for other interactions, as the square root of the average of the squared $a_{ij}$ \@ref(eq:IS).

```{r PL2, echo=FALSE, results='hide'}
pimmlawton <- function(mat, N=1, omni.i=NA, omni.j=NA, omega=NULL){
  S <- nrow(mat)
  if(is.na(omni.i)) {
    out <- matrix(NA, nrow=N, ncol=4)
    colnames(out) <- c("DomEig", "Im", "IntraDD", "I")
    for(n in 1:N) out[n,] <- {
      M = runif(S^2) * mat
      ## Do eigenanalysis
      eigs <- eigen(M)[["values"]]
      mx <- which.max(Re(eigs))
      deM = Re(eigs)[mx]
      deMi = Im(eigs)[mx]
      intraNDD <- sqrt(sum(diag(M)^2)/S)
      diag(M) <- 0
      IS = sqrt( sum(M^2)/(S*(S-1)) )
      c(deM, deMi, intraNDD, IS)
    } } else {
      out <- matrix(NA, nrow=N, ncol=5)
      colnames(out) <- c("DomEig", "Im", "IntraDD", "I", "I.omni")
      for(n in 1:N) out[n,] <- {
        M = runif(S^2) * mat
        ## Adjust for McCann type omnivory
        if(!is.null(omega))  {M[omni.i,omni.j] <- omega*M[omni.i+1,omni.j]
        M[omni.i+1,omni.j] <- (1-omega)*M[omni.i+1,omni.j]
        M[omni.j,omni.i] <- omega*M[omni.j,omni.i+1]
        M[omni.j,omni.i+1] <- (1-omega)*M[omni.j,omni.i+1]}
        ## Do eigenanalysis
        eigs <- eigen(M)[["values"]]
        mx <- which.max(Re(eigs))
        deM = Re(eigs)[mx]
        deMi = Im(eigs)[mx]
        intraNDD <- sqrt(sum(diag(M)^2)/S)
        diag(M) <- 0
        IS = sqrt( sum(M^2)/(S*(S-1)) )
        omnivory <- sqrt(mean(c(M[omni.i,omni.j],M[omni.j,omni.i])^2))
        c(deM, deMi,intraNDD, IS, omnivory)
      }
    }
  return(as.data.frame(out))
}
```
The `primer` package has a function for all this; see the arguments in the function.
```{r} 
args(pimmlawton)
```
Now we can check this function for a single simulation for our first web,
```{r}
set.seed(1)
pimmlawton(Aq)
```
Now let's do it 2000 times, as Pimm and Lawton did. Each row will be an independent randomization, and the columns will be the dominant eigenvalue, the intraspecific density dependence, and the average interaction strength.
```{r}
out.A <- pimmlawton(Aq, N=1000) 
```
We might like to look at basic summary statistics of the information we collected --- what are their minima and maxima and mean? 
```{r}
summary(out.A[,c(1,3,4)])
```
We see that out of 2000 random food chains, the largest dominant eigenvalue is still less than zero ($\lambda_1<0$). What does that mean? It means that all of the chains are qualitatively stable, and that the return times are greater than zero ($-1/\lambda_1>0$).^[A negative return time indicates that any 'perturbation' at the equilibrium would have been closer to zero at some time in the past.]

May's work showed that stability is related to interaction strength. Let's examine how the dominant eigenvalue is related to interaction strength.^[If we think of stability analysis as the analysis of a small perturbation at the equilibrium, then the dominant eigenvalue is the growth rate of that perturbation.]
```{r pairsA, fig.cap="Perturbations at the equilibrium tend to dissipate more rapidly (more negative dominant eigenvalues) with greater intraspecific negative density dependence (IntraDD) and greater interspecifiic interaction strength (I). This graph also demonstrates the independence of IntraDD and I in these simulations.", fig.height=8, fig.width=8}
pairs(out.A[,c(1,3:4)])
```

The results of our simulation (Fig. \@ref(fig:pairsA)) show that the dominant eigenvalue can become more negative, i.e. more resilient, with greater intraspecific negative density dependence (`IntraDD`) and greater intersepcifiic interaction strength (`I`). We see here that stability can increase with increasing interaction strengths, contrary to May's original general conclusions.

Note also (Fig. \@ref(fig:pairsA)) that many eigenvalues seem very close to zero --- what does this mean for return times?
The inverse of a very small number is a very big number, so it appears that many return times will be very, very large, and rendering the webs effectively unstable. Let's calculate return times and examine a summary.
```{r}
RT.A <- -1/out.A[["DomEig"]]
summary(RT.A)
```
We find that the maximum \index{return time}return time is a very large number, and even the median is fairly large (```r round( median(RT.A) )```). In an ever-changing world, is there any meaningful difference between a return time of 1000 generations *vs.* neutral stability? 

Pimm and Lawton addressed this by picking an arbitrarily large number (150) and recording the percentage of return times greater than that. This percentage will tell us the percentage of webs that are not effectively stable.
```{r}
sum( RT.A > 150 ) / 2000
```
Now let's extract the return times that are less than or equal to
150 and make a histogram with the right number of divisions or bins to allow it to look like the one in the original [@Pimm1977].
```{r histA, fig.cap="The number (y-axis) of different return times (x-axis) resulting from simulated food chains with four species.",   fig.width=5, fig.height=4}
A.fast <-RT.A[RT.A < 150]
histA <- hist(A.fast, breaks=seq(0,150, by=5), main=NULL)
```
This histogram (Fig. \@ref(fig:histA)) provides us with a picture of the stability for a food chain like that in Fig. \@ref(fig:PLABE)(left chain). Next, we will compare this to other webs.

### Shortening the Chain
Now let's repeat all this (more quickly) for a shorter chain, but with the same number of species (Fig. \@ref(fig:PLABE), center chain) So, we first make the web matrix, and then we run the 2000 simulations, and check a quick summary.
```{r}
Eq = matrix(c(
  -1,   0,   0, -10,
  0,   -1,   0, -10,
  0,    0,   -1, -10,
  0.1, 0.1, 0.1,  0),
  nrow=4, byrow=TRUE)
Eq
out.E <- pimmlawton(Eq, N=1000)
summary(out.E)
```
The summary shows that, again, that all webs are stable ($\lambda_1<0$). A histogram of return times also shows very short return times (Fig. \@ref(fig:histE)). Plots of $\lambda_1$ \emph{vs.} interaction strengths show that with this short chain, and three basal species that the role of intraspecfic density dependence becomes even more important, and the predator-prey interactions less important in governing $\lambda_1$. 
```{r E1, fig.cap="For a food chain with two levels, and three basal species, perturbation growth rate (lambda_1) declines with increasing intraspecific negative density dependence (IntraDD) and is unrelated to predator-prey interaction strengths.", fig.show='hold', fig.ncol=2, out.width="40%", fig.width=3.5, fig.height=3.5}
par(mar=c(5,4,0.5,1))
plot(DomEig ~ IntraDD, data=out.E)
plot(DomEig ~ I, data=out.E)
```

Note that with the shorter food chain, a greater proportion of the $\lambda_1$ are more negative (farther away from zero) than in the four level food chain. Clearly then, shortening the web or adding more negative density dependence stabilizes it, in spite of still having the same \emph{number} of species.

Let us again categorize these as having long and short \index{return time}return times, and graph the distribution of the short ones.
```{r histE, fig.cap="The number (y-axis) of different return times (x-axis) resulting from simulated food webs with three basal species and one consumer.", fig.width=5, fig.height=4}
RT.E <- -1/out.E[["DomEig"]]
E.fast <-RT.E[RT.E < 150]
histE <- hist(E.fast, breaks=seq(0,150, by=5), main=NULL)
```

### Adding Omnivory
Real webs also have \index{omnivory}omnivory --- feeding on more than one trophic level. A nagging question, then and now, concerns the effect of omnivory on food web dynamics. Pimm and Lawton compared food web dynamics with and without omnivory. Let's now create the web (Fig. \@ref(fig:PLABE), right chain) that they used to compare directly with their linear food chain. Next we run the 2000 simulations, and check a quick summary.
```{r}
Bq = matrix(c(
  -1, -10,   0,     0,
  0.1,  0,   -10,   -10,
  0,    0.1,   0,    -10,
  0,    0.1,    0.1,   0),
  nrow=4, byrow=TRUE)
Bq
out.B <-  pimmlawton(Bq, N=1000, omni.i=2, omni.j=4)
summary(out.B[,c(1,3:5)])
```
With omnivory, we now see that most webs have $\lambda_1>0$, and thus are \emph{unstable}. This was one of the main points made by Pimm and Lawton. Let's view scatterplots of the data.
```{r pairsB, fig.width=6, fig.height=6}
pairs(out.B[,c(1,3:5)])
```


It means that most of the randomly constructed webs were not stable point equilibria. To be complete, let's graph what Pimm and Lawton did.
```{r histB, fig.cap="The number (y-axis) of different return times (x-axis) resulting from simulated food chains with basal species and omnivory by the top consumer on the second consumer.", fig.width=5, fig.height=4}
RT.B <- -1/out.B[["DomEig"]]
B.fast <- RT.B[RT.B < 150 & RT.B>0 ]
out.B.fast <- out.B[RT.B < 150 & RT.B>0 ,]
out.B.stab <- out.B[RT.B>0 ,]
histB <- hist(B.fast, breaks=seq(0,150, by=5),
              main=NULL)
```

### Comparing Chain A versus B
Now let's compare the properties of the two chains, without, and with, omnivory, chains \textbf{A} and \textbf{B} (Fig. \@ref(fig:PLABE)). Because
these are stochastic simulations, it means we have *distributions* of
results. For instance, we have a distribution of return times for chain
\textbf{A} and a distribution for return times for chain \textbf{B}. That is,
we can plot histograms for each of them. Pimm and Lawton compared their webs in
common sense ways. They compared simple summaries, including
\begin{itemize}
\item the proportion of random webs that were stable (positive return times),
  \item the proportion of stable random webs with return times less than 150.
\end{itemize}

Now let's try graphical displays. Rather than simply chopping off the long return times, we use base 10 logarithms of return times because the
distributions are so right-skewed. We create a histogram of the return times for chain **A**, and nonparametric density functions for both chain \textbf{A} and **B**.
```{r histsAB, fig.cap="Comparing the distributions of return times for chain \textbf{A} and **B**. The y-axis is probability density. The distribution of return times for chain **A** is the solid line, and the distribution of return times for chain **B** is the dashed line. These density smoothers do a good job describing empirical distributions of continuous data, often better than histograms, which have to create discrete categories, or bins, for continuous data.", fig.width=5, fig.height=4}
hist(log(RT.A,10), probability=T, ylim=c(0,1),
     main=NULL, 
     xlab=expression(log[10]("Return Time"))
     )
lines(density(log(RT.A,10)) )
lines(density(log(RT.B[RT.B>0],10)), lty=2, lwd=2 )
legend("topright", c("Food chain A", "With omnivory (B)"), lty=1:2, 
       lwd=1:2,  bty='n')
```

By overlaying the density function of web B on top of web A return times (Fig. \@ref(fig:histsAB)), we make an interesting observation. The omnivorous webs with positive return times (those plotted) actually tended to have shorter return times than the linear chain. Pimm and Lawton noted this, but did not emphasize it. Rather, they sensibly focused on the more obvious result, that over 90\% of the omnivorous webs had negative return times, indicating an absence of a stable point equilibrium.

*Further analysis* If we wanted to measure the effects of variation in the strength of intraspecific density dependence, average interaction strength, and omnivory, we could regress $\lambda_1$ against those variables. As a quick and dirty evaluation, we can rescale all the variables to a mean of zero and a standard deviation of 1, and do a simple multiple regression.
```{r}
# generate a bigger sample
out.B <-  pimmlawton(Bq, N=1e4, omni.i=2, omni.j=4)
# rescale variables
oBs <- as.data.frame(scale(out.B))
# do the regression
mB <- lm(DomEig ~ IntraDD + I + I.omni, data=oBs)
## check diagnostics - which would tell us we should use a different model ;-)
# plot(mB)
```

Because we rescaled the variables the way we did, the regression coefficients tell us the direction and magnitudes of the effects of intraspecific density dependence, average interaction strength, and omnivory. If we had met all the assumptions of the model, these are equal to the correlation coefficients of the independent effects of X on Y.
```{r}
print( coef( summary(mB) ), digits=3)
## To display the independent effects of X on Y, we could use partial regression plots,
## aka added variable plots, available in the 'car' package
# library(car)
# avPlots(mB)
```
These coefficients show us that the dominant eigenvalue increases with increasing  average intra- and interspecific interaction strengths, and decreases with increasing strength of omnivory.


## Re-evaluating Take-Home Messages
The primary messages made by @Pimm1977 were that 
\begin{itemize}
\item shorter webs are more stable than long chains,
  \item omnivory destabilizes webs.
\end{itemize}

These conclusions were a major part of the lively debate surrounding these issues. It was consistent with the apparent observation of the time, that empirical food webs revealed little omnivory [@Pimm1977;@Pimm1978], and that food chains in nature seemed to be much shorter than could occur, if \index{primary productivity}primary productivity (green plants and algae) was channeled upwards into a linear chain. 

Let's consider their assumptions.

First, Pimm and Lawton made the argument, as many others have (including us), that systems with stable point equilibria are more likely to persist than systems with oscillations, such as stable limit cycles. That is, we presume a strong correlation between the tendency to oscillate, and the tendency for species to become extinct (i.e., the system to collapse). It is easy to show that a system can be pushed from a stable equilibrium into oscillations which eventually become so big as to drive an element of the system to extinction. This is a very reasonable assumption, but one which is not challenged enough. Other measures of system stability could be used, such as the minimum that occurs in a long series of fluctuations [@Huxel1998;@McCann1997]. 

Second, Pimm and Lawton ignored the effects of self-regulated basal species. By exhibiting negative density dependence, the basal species stabilized the web. When Pimm and Lawton made a shorter web, they also added more self-regulated populations. Thus, they necessarily confounded chain length with the number of species with negative density dependence. Which change caused the observed differences among webs? We can't tell from these simulations.

Third, they assumed that the effect of web topology (i.e., short \emph{vs.} long chain) was best evaluated with the \emph{average} properties of the topology, rather than the minimum or maximum properties of the topology. By these criteria, webs without omnivory were clearly better. On average, webs without omnivory were more often stable than chains with omnivory, even if some of their return times tended to be quite long. Therefore, one might argue that if a web assembles in nature, it is more likely to persist (i.e., be stable) if it lacks omnivory. 

However, the world is a messy place, with constant insults and disturbances, and resources and environmental conditions fluctuating constantly. There is also a constant rain of propagules dropping into communities, and species abundances are changing all the time. Communities are being constantly perturbed. The only webs that can persist in the face of this onslaught are the \emph{most} stable ones, perhaps the ones with the shortest return times. We just showed that Pimm and Lawton's own analyses showed that \emph{the most resilient webs tended to be those with \index{omnivory, stabilizing}omnivory}. It didn't take too long for new work to show that omnivory is rampant in nature [@Polis1991], and this is supported by theory that shows weak interactions, including omnivory, stabilize food webs [@McCann1998;@McCann1997].

Pimm and Lawton made very important contributions to this lengthy debate, and we are finally figuring out how to interpret their results.



<!--chapter:end:10-foodwebsNoPairs.Rmd-->

# Appendix
[I added at the END the derivation of lambda in a simple 2x2 matrix, so keep it!]
\emph{R is a language. Use it every day, and you will learn it quickly.}

\S, the precursor to \R, is a quantitative programming environment developed at AT\&T Bell Labs in the 1970s. \tens{S-Plus}~is a commercial, ``value-added'' version and was begun in the 1980s, and \R~was begun in the 1990s by Robert Gentleman and Ross Ihaka of the Statistics Department of the University of Auckland. Nearly 20 senior statisticians provide the core development group of the \R~language, including the primary developer of the original \S~language, John Chambers, of Bell Labs. 

\R~is an official part of the Free Software
Foundation's GNU project\footnote{Pronounced ``g-noo'' --- it is a recursive
  acronym standing for ``GNU's Not Unix.''} 
(http://www.fsf.org/). It is free to all, and licensed to stay that way. 

\R\ is a language and environment for dynamical and statistical
computing and graphics. \R~is similar to the \S~language, different implementation of \S. Technically speaking, \R~is a ``dialect'' of \S.
\R~provides a very wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, \ldots) and graphical techniques, and is highly extensible. \R~has become the lingua franca of academic statistics, and is very useful for a wide variety of computational fields, such as theoretical ecology.

\section{Strengths of \textsf{R/S}}
\begin{itemize}
\item Simple and compact syntax of the language. You can learn \R~quickly, and can accomplish a lot with very little code.
\item Extensibility. Anyone can extend the functionality of \R~by writing code. This may be a simple function for personal use, or a whole new family of statistical procedures in a new package.
\item A huge variety of statistical and computing procedures. This derives from the ease with which \R/\S~can be extended and shared by users around the world.
\item Rapid updates.
\item Replicability and validation. All data analyses should be well documented, and this only happens reliably when the analyses are performed with scripts or programs, as in \R~or SAS. Point-and-click procedures cannot be validated by supervisors, reviewers, or auditors.
\item Getting help from others is easy. Any scripted language can be quickly and easily shared with someone who can help you. I cannot help someone who says ``first I clicked on this, and then I clicked on that \ldots.''
\item Repetitive tasks simplified. Writing code allows you to do anything you want a huge number of times. It also allows very simple updates with new data.
\item High quality graphics. Well-designed publication-quality plots
  can be produced with ease, including mathematical symbols and formulae where needed. Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control. 
\item \R~is available as Free Software under the terms of the Free Software Foundation's GNU General Public License in source code form. It compiles and runs out of the box on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux). It also compiles and runs on Windows 9x/NT/2000 and Mac OS. 
\item Accessibility. Go now to www.r-project.org. Type ``\R'' into Google. The \R~Project page is typically the first hit.
\end{itemize}

There is a tremendous amount of free documentation for \R. \R\ comes
with a selection of manuals under the ``Help'' menu --- explore these
first. At the main \R~project web site, see the ``Documentation'' on
the left sidebar. The FAQ's are very helpful. The ``Other'' category
includes a huge variety of items; search in particular for
``Contributed documentation.''\footnote{I find this when I google ``r
  `contributed documentation'.''}  There you will find long (100+ pages) and short tutorials. You will also find two different ``\R Reference Cards,'' which are useful lists of commonly used functions.\footnote{Try googling `R Reference Card' in quotes.}

\section{The \R~Graphical User Interface (GUI)}
\R~has a very simple but useful graphical user interface (GUI; Fig. \ref{fig:gui}). A few points regarding the GUI:
\begin{itemize}
\item ``You call this a graphical user interface?'' Just kidding --- the GUI is \emph{not} designed for point-and-click modeling.
\item The \R~GUI is designed for package management and updates. 
\item The \R~GUI is designed for use with scripts.
\end{itemize}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Screenshot1.png}
  \caption{The Mac OS X \R~GUI. Color coded syntax not visible in this figure.}
  \label{fig:gui}
\end{figure}

The \R~GUI does not provide a ``statistics package.''
R is a language and programming environment. You can download an \R~package called \texttt{Rcmdr} that provides a point-and-click interface for introductory statistics, if you really, really want to. In my experience, students who plan to use statistics in their research find it more frustrating to learn this interface than to learn to take advantage of the language.

The \R~GUI \emph{is} designed for maintenance. With the \R~GUI you can
check for updates, and download any of the hundreds of small packages
that extend \R~in hundreds of ways. (A package is not unlike a
``PROC,'' for SAS users --- first you tell call it, then you use it). 

The \R~GUI \emph{is} designed for using scripts. Scripts are text files that contain your analysis; that is, they contain both code to do stuff, and comments about what you are doing and why. These are opened within \R~and allow you to do work and save the code.
\begin{itemize}
\item Scripts are NOT Microsoft Word documents that require Microsoft
  Word to open them, but rather, simple text files, such as one could open in Notepad, or SimpleText. 
\item Scripts are a written record of everything you do along the way to achieving your results. 
\item Scripts are the core of data analysis, and provide many of the benefits of using a command-driven system, whether \R, \textsf{Matlab}, or some other program or environment.
\item Scripts are interactive. I find that because scripts allow me to
  do anything and record what I do, they are very interactive. They let me try a great variety of different things very quickly. This will be true for you too, once you begin to master the language.
\end{itemize}

The \R~GUI can be used for simple command line use. At the command line, you can add $2 + 2$ to get 4. You could also 
do \texttt{ar(lake.phosphorus)} to perform autoregressive time series 
analysis on a variable called \texttt{lake.phosphorus}, but you would 
probably want to do that in a script that you can save and 
edit, to keep track of the analysis that you are doing. 

\section{Where is \R?}
As an Open Source project, \R~is distributed across the web. People
all around the world continue to develop it, and much of it is stored
on large computer servers (``mirrors'') across the world. Therefore,
when you download \R, you download only a portion of it --- the language and a few base packages that help everything run. Hundreds of ``value-added'' packages are available to make particular tasks and groups of tasks easier. We will download one or more of these. 

It is useful to have a clear conception of where different parts of
\R~reside (Fig. \ref{fig:Rcomman}). Computer servers around the world
store identical copies of everything (hence the terms archive and
``mirrors''). When you open \R, you load into your computer's virtual,
temporary RAM more
than just the \R~language --- you automatically load several useful packages including ``base,'' ``stat,'' and others. Many more packages exist (about a dozen come with the normal download) and hundreds are available at each mirror. These are easily downloaded through the \R~GUI.
\label{sec:where-r}

\begin{figure}[ht]
  \centering
\includegraphics[width=8cm]{RComManage.pdf}
  \caption{A conceptual representation of where \R~exists. "CRAN" stands for "Comprehensive \R~Archive Network." "RAM" (random access memory) is your computer's active brain; \R~keeps some stuff floating in this active memory and this "stuff" is the \emph{workspace}.  \label{fig:Rcomman}}
\end{figure}

\section{Starting at the Very Beginning}
To begin with, we will go through a series of steps that will get you up and running using a script file with which to interact with \R, and using the proper working directory.
Start here.
\begin{enumerate}
  \item Create a new directory (i.e., a folder) in ``Documents'' (Mac) or ``My Documents'' (Windows).  \emph{and call it} ``Rwork.'' For now, calling it the same thing as everyone else will just simplify your life.  If you put ``Rwork'' elsewhere, adjust the relevant code as you go. For now, keep all of your script files and output files into that directory. 
      \item Open the \R~GUI in a manner that is appropriate for your operating system and setup (e.g., double-click the desktop icon).
        \item Set the \emph{working directory.} You can do this via \textsf{Misc} directory in Mac OS X or in the \textsf{File} menu in Windows using ``Change dir....'' Set the working directory to ``Rwork.'' (If you have not already made an \texttt{Rwork} directory, do so now --- put it in ``Documents'' or ``My Documents.'')
      \item Open a new \R~script (``New Document'') by using the \textsf{File} menu in the \R~GUI. On the first line, type \texttt{\# My first script} with the pound sign. On the next line, type \texttt{setwd(`$\sim$/Documents/Rwork')} if you are on a Mac, or  \texttt{setwd(`C:/Documents and Settings/Users/Jane/My Documents/Rwork')} on Windows, assuming you are named ``Jane;'' if not, use the appropriate pathname. Save this file in ``Rwork;'' save it as ``RIntro.R.'' Windows may hide the fact that it is saving it as a ``.txt'' file. I will assume you can prevent this.
\end{enumerate}

You should submit code to \R~directly from the script. \emph{Use the script to store your ideas as comments (beginning with \#) and your code, and submit code directly from the script file within \R~(see below for how to do that)}. You do \emph{not} need to cut-and-paste. There are slight differences between operating systems in how to submit code from the script file to the command line.
\begin{itemize}
\item In Microsoft Windows, place the cursor on a line in the script file \emph{or} highlight a section of code, and then hit \texttt{Ctrl-R} to submit the code.
\item In Apple Mac OS X, highlight a section of code and then hit \texttt{Command-return} to submit the code (see the \textsf{Edit} menu).
\end{itemize}

\emph{From this point on, enter all of the code (indicated in typewriter font, and beginning with ``>'') in your script file, save the file with Command-S (Mac) or Ctrl-S (Windows), and then submit the code as described above.} Where a line begins with ``+,'' ignore the plus sign, because this is just used by \R~ to indicate continued lines of code. You may enter a single line of code on more than one line. \R~will simply continue as if you wrote it all on one line.

You can start the help interface with this command.
<<eval=false>>=
help.start() 
@ 
This starts the HTML interface to on-line help (using a web browser available at your machine). You can use help within the \R~GUI, or with this HTML help system.

Find out where you are using \texttt{getwd()} (\emph{get} the
\emph{w}orking \emph{d}irectory). Use a comment (beginning with \#) to remind yourself
what this does.
<< keep.source=true, results=hide >>=
# Here I Get my Working Directory; that is,
# I find out which folder R is currently operating from.
getwd()
@ 
The precise output will depend on your computer.

You can also \emph{set} the \emph{w}orking \emph{d}irectory using \texttt{setwd()}; if you created a directory called \texttt{Rwork} as specified above, one of the following these should work, depending on your operating system. If these both fail, keep trying, or use the menu in the \R~GUI to set the working directory. 
<<eval=false>>=
setwd("~/Documents/Rwork")
@ 
or
<<eval=false>>=
setwd("C:/Documents and Settings/Users/Jane/My Documents/Rwork")

@ 
On the Mac, a UNIX environment, the tilde-slash ($\sim/$) represents your home directory.


\emph{I urge you to use \texttt{setwd} at the beginning of each script file you write so that this script always sets you up to work in a particular, specified directory.} As you write your script file, remember,
\begin{itemize}
\item Text that begins with ``\#'' will be ignored by \R. 
\item Text that does not make sense to \R~will cause \R~to return an error message, but will not otherwise mess things up.
\end{itemize}

Remember that a strength of \R~is that you can provide, to yourself and others, comments that explain every step and every object. To add a comment, simply type one or more ``\#,'' followed by your comment.

Finally, have fun, be amused. Now you are ready for programming in \R.


\emph{R is a language. Use it every day, and you will learn it quickly.}


\chapter{Programming in \R}
This material assumes you have completed Appendix A, the overview of
\R. Do the rest of this Appendix in a script. Make comments of your own throughout your script. 

Open and save a new file (or \emph{script}) in \R. Think of your
script as a pad of paper, on which you program and \emph{also on which
  you write notes to yourself}. See Appendix A for instructions.

You will want to take copious notes about what you do. Make these notes in the script file, using the pound sign \#. Here is an example:

<<keep.source=true>>=
# This will calculate the mean of 10 random standard normal variables. 
mean( rnorm( 10 ) )
@ 
You submit this (as described above) from the script directly to the Console with (select) Command-r on a Mac, or Ctrl-r on Windows.

\section{Help}
You cannot possibly use \R~without using its help facilities. \R~comes with a lot of documentation (see the relevant menu items), and people are writing more all the time (this document is an example!). 

After working through this tutorial, you could go through the document ``An Introduction to \R'' that comes with \R. You can also browse ``Keywords by Topic'' which is found under ``Search Engine \& Keywords'' in the Help menu.

To access help for a specific function, try
<<>>=
?mean
@ 
or 
<<help>>=
help(mean)
@ 

The help pages provide a very regular structure. There is a \emph{name}, a brief \emph{description}, its \emph{usage}, its \emph{arguments}, \emph{details} about particular aspects of its use, the \emph{value} (what you get when you use the function), \emph{references}, \emph{links} to other functions, and last, \emph{examples}.

If you don't know the exact name of the \R~function you want help with, you can try
<<eval=false>>=
help.search("mean")
apropos("mean")
@ 
These will provide lists of places you can look for functions related to this keyword.

Last, a great deal of \R~resides in \emph{packages} online (on
duplicate servers around the world). If you are on line, help for functions that you have not yet downloaded can be retrieved with
<<eval=false>>=
RSiteSearch("violin")
RSiteSearch("violin",restrict = c("functions"))
@ 
To learn more about help functions, combine them!
<<eval=false>>=
help(RSiteSearch)
@ 
\section{Assignment}
In general in \R, we perform an action, and take the results of that action and \emph{assign} the results to a new object, thereby creating a new object. Here I add two numbers and assign the result to an new object I call \texttt{a}.
<<ass>>=
a <- 2+3
a
@ 
Note that I use an arrow to make the assignment --- I make the arrow with a less-than sign, $<$, and a dash.  Note also that to reveal the contents of the object, I can type the name of the object. 

I can then use the new object to perform another action, and assign 
<<a2>>=
b <- a+a
@ 

I can perform two actions on one line by separating them with a semicolon. 
<<semi>>=
a+a; a+b
@ 
Sometimes the semicolon is referred to as an ``end of line'' operator.
\section{Data Structures}
We refer to a single number as a scalar; a scalar is a single real number. Most objects in \R~are more complex. Here we describe some of these other objects: vectors, matrices, data frames, lists, and functions.
\subsection{Vectors}
Perhaps the fundamental unit of \R~is the \emph{vector}, and most operations in \R~are performed on vectors. A vector may be just a column of scalars, for instance; this would be a column vector.

Here we create a vector called \texttt{Y}.

To enter data directly into \R, we will use \texttt{c()} and create an
\R~object, in particular a vector. A vector is, in this case, simply a group of numbers arranged in a row or column. Type into your script
<<>>=
Y <- c(8.3,8.6,10.7,10.8,11,11,11.1,11.2,11.3,11.4)
@ 
where the arrow is a less-than sign, \texttt{<}, and a dash,
\texttt{-}. Similarly, you could use
<<>>=
Y = c(8.3,8.6,10.7,10.8,11,11,11.1,11.2,11.3,11.4)
@ 
These are equivalent.

\R~operates (does stuff) to \emph{objects}. Those objects may be
\emph{vectors}, \emph{matrices}, \emph{lists}, or some other class of object. 

\subsubsection{Sequences}
I frequently want to create ordered sequences of numbers. \R~has a shortcut for sequences of integers, and a slightly longer method that is completely flexible. First, integers:
<<>>=
1:4
4:1
-1:3
-(1:3)
@ 
Now more complex stuff, specifying either the units of the sequence, or the total length of the sequence.
<<>>=
seq(from=1, to=3, by = .2)
seq(1, 3, by=.2)
seq(1, 3, length=7)
@ 
I can also fill in with repetitive sequences. Compare carefully these examples.

<<>>=
rep(1, 3)
rep(1:3, 2)
rep(1:3, each=2)
@ 
\subsection{Getting information about vectors}
Here we can ask \R~to tell us about \texttt{Y}, getting the length (the number of elements), the mean, the maximum, and a six number summary.
<<>>=
sum(Y)
mean(Y)
max(Y)
length(Y) 
summary(Y)
@ 
A vector could be character, or logical as well, for instance
<<>>=
Names <- c("Sarah", "Yunluan")
Names
b <- c(TRUE, FALSE)
b

@ 
Vectors can also be dates, complex numbers, real numbers, integers, or factors. For factors, such as experimental treatments, see section \ref{sec:factors}. We can also ask \R~what classes of data these belong to.
<<>>=
class(Y)
class(b)
@ 

Here we test whether each element of a vector is greater than a particular value or greater than its mean. \emph{When we test an object, we get a logical vector back that tells us, for each element, whether the condition was true or false}.
<<>>=
Y > 10
Y > mean(Y)
@ 

 We test using $>, <, >=, <=, ==, !=$ and other conditions. Here we test whether a vector is equal to a number.
<<>>=
Y == 11
@ 
A test of ``not equal to''
<<>>=
Y != 11
@ 
This result turns out to be quite useful, including when we want to \emph{extract} subsets of data.

\subsubsection{Algebra with vectors}
In \R, we can add, subtract, multiply and divide vectors. When we do this, we are really \emph{operating on the elements in the vectors}. Here we add vectors \texttt{a} and \texttt{b}.
<<>>=
a <- 1:3
b <- 4:6
a + b
@ 
Similarly, when we multiply or divide, we also operate on each pair of elements in the pair of vectors.
<<>>=
a*b
a/b
@ 
We can also use scalars to operate on vectors.
<<>>=
a + 1
a * 2
1/a
@ 
What \R~is doing is \emph{recycling} the scalar (the 1 or 2) as many times as it needs to in order to match the length of the vector. Note that if we try to multiply vectors of unequal length, \R~performs the operation but may or may not give a warning. Above, we got no warningmessage. However, if we multiply a vector of length 3 by a vector of length 2, \R~returns a warning.
<<>>=
a*1:2
@ 
\R~\emph{recycles the shorter vector} just enough to match the length of the longer vector. The above is the same as 
<<>>=
a*c(1,2,1)
@ 
On the other hand, if we multiply vectors of length 4 and 2, we get no error, because four is a multple of 2.
<<>>=
1:4 * 1:2
@ 
Recycling makes the above the same as the following.
<<>>=
1:4 * c(1,2,1,2)
@ 

\subsection{Extraction and missing values}
We can extract or subset elements of the vector. 

I extract subsets of data in two basic ways, by
\begin{itemize}
\item identifying which rows (or columns) I want (i.e. the first row), or
\item providing a \emph{logical} vector (of TRUE's and FALSE's) of the same length as the vector I am subsetting.
\end{itemize}

Here I use the first method, using a single integer, and a sequence of integers.
<<>>=
Y[1]
Y[1:3]

@ 
Now I want to extract all girths greater than the average girth. Although I don't have to, I remind myself what the logical vector looks like, and then I use it.
<<>>=
Y > mean(Y)
Y[ Y > mean(Y) ]
@ 
Note that I get back all the values of the vector where the condition was TRUE.

In \R, \emph{missing data }are of the type ``NA.'' This means ``not available,'' and \R~takes this appellation seriously. thus is you try to calculate the mean of a vector with missing data, \R~resists doing it, because if there are numbers missing from the set, how could it possibly calculate a mean? If you ask it to do something with missing data, the answer will be missing too.

Given that \R~treats missing data as missing data (and not something to be casually tossed aside), there are special methods to deal with such data. For instance, we can test which elements are missing with a special function, \texttt{is.na}. 
<<>>=
a <- c(5,3,6, NA)
a
is.na(a)
!is.na(a)
a[ !is.na(a) ]
na.exclude(a)
@ 
Some functions allow you to remove missing elements on the fly. Here we let a function fail with missing data, and then provide three different ways to get the same thing. 
<<>>=
mean(a)
mean(a, na.rm=TRUE)
d <- na.exclude(a)
mean(d)
@ 
Note that \R~takes missing data seriously. If the fourth element of the set really is missing, I cannot calculate a mean because I don't know what the vector is. 

@ 
\subsection{Matrices}
A matrix is a two dimensional set of elements, for which \emph{all elements are of the same type}. Here is a character matrix.
<<>>=
matrix(letters[1:4], ncol=2)
@ 
Here we make a numeric matrix.
<<>>=
M <- matrix( 1:4, nrow=2 )
M
@ 
Note that the matrix is filled in by columns, or \emph{column major order}. We could also do it by rows.
<<>>=
M2 <- matrix( 1:4, nrow=2, byrow=TRUE )
M2
@ 
Here is a matrix with 1s on the diagonal.
<<>>=
I <- diag(1, nrow=2)
I
@ 
The identity matrix plays a special role in matrix algebra; in many ways it is equivalent to the scalar 1. For instance, the inverse of a matrix, \textbf{M}, is \textbf{M}$^{-1}$, which is the matrix which satisfies the equality $\mathbf{MM^{-1}}=\mathbf{I}$, where \textbf{I} is the identity matrix. We solve for the inverse using a few different methods, including
<<solve>>=
Minv <- solve(M)
M %*% Minv
@ 
QR decomposition is available (e.g., \texttt{qr.solve()}).

Note that \R~recycles the ``1'' until the specified number of rows and columns are filled. If we do not specify the number of rows and columns, \R~fills in the matrix with what you give it (as it did above).

\subsubsection{Extraction in matrices}
I extract elements of matrices in the same fashion as vectors, but specify both rows and columns.
<<>>=
M[1,2]
M[1, 1:2]
@ 
If I leave either rows or columns blank, \R~returns all rows (or columns).
<<>>=
M[,2]
M[,]
@ 
\subsubsection{Simple matrix algebra}
Basic matrix algebra is similar to algebra with scalars, but with a few very important differences. Let us define another matrix.
<<>>=
N <- matrix(0:3, nrow=2)
N
@ 
To perform scalar, or element-wsie operations, we have
\begin{align}
  \label{exM}
  \vec{A} & =
\left(
\begin{array}{cc}
a & b \\
c& d
\end{array} \right); \;
\vec{B} =
\left(\begin{array}{cc}
m & o\\
n & p\end{array}\right)\\
\vec{AB} &=
\left( \begin{array}{cc}
am & bo\\
cn & dp
\end{array}\right) 
\end{align}

The element-wise operation on these two is the default in \R,
<<>>=
M * N
@ 
where the element in row 1, column 1 in \texttt{M} is multiplied by the element in the same position in \texttt{N}. 

To perform \emph{matrix mulitplication}, recall from Chap. 2 that,
\begin{align}
  \label{exM}
  \vec{A} & =
\left(
\begin{array}{cc}
a & b \\
c& d
\end{array} \right); \;
\vec{B} =
\left(\begin{array}{cc}
m & o\\
n & p\end{array}\right)\\
\vec{AB} &=
\left( \begin{array}{cc}
\left( am + bn \right) & \left(ao+bp \right)\\
\left(cm + dn \right) & \left(co + dp \right)
\end{array}\right) 
\end{align}

To perform \emph{matrix mulitplication} in \R, we use \texttt{\%*\%},
<<>>=
M%*%N
@
Refer to Chapter 2 (or ``matrix algebra'' at Wikipedia) for why this is so. 

Note that matrix multiplication is not commutative, that is, $\mathbf{NM} \ne \mathbf{MN}$. Compare the previous result to
<<commutative>>=
N%*%M
@ 

Note that a \emph{vector} in \R~is not defined \emph{a priori} as a column matrix or a row matrix. Rather, it is used as either depending on the circumstances. Thus, we can either left multiply or right multiply a vector of length 2 and \texttt{M}.
<<>>=
1:2%*%M
M%*%1:2
@ 
If you want to be very, very clear that your vector is really a matrix with one column (a column vector), you can make it thus.
<<>>=
V <- matrix(1:2, ncol=1)
@ 
Now when you multiply \texttt{M} by \texttt{V}, you will get the expected sucesses and failure, according to the rules of matrix algebra.
<<>>=
M %*% V
try(V %*% M)
@ 
 \R~has formal rules about how it converts vectors to matrices on-the-fly, but it is good to be clear on your own.
 
Other matrix  operations are available. Whenever we add or subtract matrices together, or add a matrix and a scalar, it is always element-wise.
<<>>=
M + N
M + 2
@ 
The transpose of a matrix is the matrix we get when we substitute rows for columns, and columns for rows. To transpose matrices, we use \texttt{t()}.
<<>>=
t(M)
@ 

More advanced matrix operations are available as well, for singular value decomposition (\texttt{svd}), eigenanalysis (\texttt{eigen}), finding determinants (\texttt{det}), QR decomposition (\texttt{qr}), Choleski factorization (\texttt{chol}), and related functions. The \texttt{Matrix} package was designed to handle with aplomb large sparse matrices.

\subsection{Data frames}
Data frames are two dimensional, a little like spreadsheets and matrices. All columns having exactly the same number of rows. Unlike matrices, each column can be a different data type (e.g., numeric, integer, charactor, complex, imaginary). For instance, the columns of a data frame could contain the names of species, the experimental treatment used, and the dimensions of species traits, as character, factor, and numeric variables, respectively.

<<>>=
dat <- data.frame(species=c("S.altissima", "S.rugosa", 
                    "E.graminifolia", "A. pilosus"),
                  treatment=factor( c("Control", "Water", "Control", "Water") ),
                  height = c(1.1, 0.8, 0.9, 1.0),
                  width =c(1.0, 1.7, 0.6, 0.2)
                  )
dat
@ 
We can extract data from data frames just the way we can with matrices.
<<>>=
dat[2,]
dat[3,4]
@ 
We can test elements in data frames, as here where I test whether each element column 2 is ``Water.'' I then use that to extract rows of data that are associated with this criterion.
<<>>=
dat[,2]=="Water"
dat[ dat[,2]=="Water", ]
@ 
I could also use the subset function
<<>>=
subset(dat, treatment == "Water")
@ 
There are advantages to using data frames which will become apparent. 

\subsubsection{Factors}
\label{sec:factors}
Factors are a class of data; as such they could belong above with our discussion of character and logical and numeric vectors. I tend, however, to use them in data frames almost exclusively, because I have a data set that includes a bunch of response variables, and \emph{the factors imposed by my experiment}. 

When defining a factor, \R~by default orders the factor levels in alphabetic order --- we can reorder them as we like. Here I demonstrate each piece of code and then use the pieces to make a factor in one line of code.
<<>>=
c("Control", "Medium", "High")
rep( c("Control", "Medium", "High"), each=3 )
 
Treatment <- factor( rep( c("Control", "Medium", "High"), each=3 ) )
Treatment
@ 
Note that \R~orders the factor alphabetically. This may be relevant if we do something with the factor, such as when we plot it (Fig. \ref{fig:strip1}).
<<results=hide, echo=false>>=
quartz(,4,4)
<<>>=
levels(Treatment)
stripchart(1:9 ~ Treatment)
<<results=hide, echo=false>>=
dev.print(pdf, "strip.pdf")
@ 
\begin{figure}[ht]
  \centering
  \subfloat[Before]{\includegraphics[width=4cm]{strip.pdf} \label{fig:strip1}}
  \subfloat[After]{\includegraphics[width=4cm]{strip2.pdf} \label{fig:strip2}} 
  \caption{Graphics before and after the factor was releveled to place the factor levels in a logical order.}
\end{figure}

Now we can re-specify the factor, telling \R~the order of the levels we want, taking care to remember that \R~can tell the difference between upper and lower case (Fig. \ref{fig:strip2}). See also the function {\tt relevel}.

<<results=hide, echo=false>>=
quartz(,4,4)
<<>>=
Treatment <- factor( rep( c("Control", "Medium", "High"), each=3 ), 
                    levels=c("Control", "Medium", "High") )
levels(Treatment)

stripchart(1:9 ~ Treatment)
<<results=hide, echo=false>>=
dev.print(pdf, "strip2.pdf")

@ 

\subsection{Lists}
An amazing data structure that \R~boasts is the \emph{list}. A \emph{list} is simply a collection of other objects kept together in a hierarchical structure. Each component of the list can be a complete different class of object. Let's build one.
<<>>=
my.list <- list(My.Y = Y, b = b, Names, Weed.data=dat,
                My.matrix = M2, my.no = 4 )
my.list

@ 
We see that this list is a set of objects: a numeric vector, a logical vector, a character vector, a data frame, a matrix, and a scalar (a number). Lists can be nested within other lists.

Note that if we do not specify a name for a component, we can still extract it using the number of the component.

I extract list components in several ways, including by name, and by
number (see \texttt{?'['} for more information).
<<>>=
my.list[["b"]]
my.list[[2]] 
@ 
If I use a name, there are a few ways, including
<<>>=
my.list[["b"]]
my.list$b
@ 
If by number, that are two ways, with one or two brackets. In addition
to two brackets, as above, we can use one bracket. This allows for
extraction of more than one component of the list.
<<>>=
my.list[1:2]
@ 
Note that I can extract a subset of one component.
<<>>=
my.list[["b"]][1]
@ 
If one way of extraction is working for you, experiment with others.

\subsection{Data frames are also lists}
You can also think of a data frame as a list of columns of identical length. I like to extract columns the same way --- by name.
<<>>=
mean( dat$height )
@
\section{Functions}
A function is a command that does something. You have already been using functions, throughout this document. Let's examine functions more closely.

Among other things, a function has a name, arguments, and values. For instance, 
<<eval=false>>=
help(mean)
@ 
This will open the help page (again), showing us the
\emph{arguments}. The first argument \texttt{x} is the object for
which a mean will be calculated. The second argument is
\texttt{trim=0}. If we read about this argument, we find that it will
``trim'' a specified fraction of the most extreme observations of \texttt{x}. \emph{The fact that the argument \texttt{trim} is already set equal to zero means that is the default}. If you do not use \texttt{trim}, then the function will use \texttt{trim=0}. Thus,
these two are equivalent.
<<>>=
mean(1:4)
mean(1:4, trim=0)
@ 
\R~is an ``object-oriented'' language. A consequence of this is that the same function name will perform different actions, depending on the \emph{class} of the object.\footnote{\R~has hundreds of built-in data sets for demonstrating things. We use one here called 'warpbreaks.' You can find some others by typing \texttt{data()}.}
<<>>=
class(1:10)
class(warpbreaks)
summary( 1:10)
summary(warpbreaks)
@ 
In the \texttt{warpbreaks} data frame, summary provides the six number summary for each numeric or integer column, but provides ``tables'' of the factors, that is, it counts the occurrences of each level of a factor and sorts the levels. When we use summary on a linear model, we get output of the regression,
<<>>=
summary( lm(breaks ~ wool, data=warpbreaks) )


@ 
\subsection{Writing your own functions}
\label{sec:writing-your-own}
One very cool thing in \R~is that you can write your own functions. Indeed it is the extensibility of \R~that makes it the home of cutting edge working, because edge cutters (i.e., leading scientists) can write code that we all can use. People actually write entire \emph{packages}, which are integrated collections of functions, and \R~has been extended with hundreds of such packages available for download at all the \R~mirrors.

Let's make our own function to calculate a mean. Let's further pretend you work for an unethical boss who wants you to show that average sales are higher than they really are. Therefore your function should provide a mean plus 5\%.
<<>>=
MyBogusMean <- function(x, cheat=0.05) { SumOfX <- sum(x)
                             n <- length(x)
                             trueMean <- SumOfX/n 
                           (1 + cheat) * trueMean}
RealSales <- c(100, 200, 300)
MyBogusMean(RealSales)
@ 
Thus a function can take any input, do stuff, including produce graphics, or interact with the operating system, or manipulated numbers. You decide on the arguments of the function, in this case, \texttt{x} and \texttt{cheat}. Note that we supplied a number for \texttt{cheat}; this results in the \texttt{cheat} argument having a \emph{default} value, and we do not have to supply it. If an argument does not have a default, we have to supply it. If there is a default value, we can change it. Now try these.
<<>>=
MyBogusMean(RealSales, cheat=0.1)
MyBogusMean(RealSales, cheat=0)
@ 
\section{Sorting}
We often like to sort our numbers and our data sets; a single vector is easy. To do something else is only a little more difficult.
<<>>=

e <- c(5, 4, 2, 1, 3)
e
sort(e)
sort(e, decreasing=TRUE) 
@ 
If we want to sort all the rows of a data frame, keeping records
(rows) intact, we can use \texttt{order}. This function is a little
tricky, so we explore its use in a vector.
<<>>=
e
order(e)
e[ order(e) ]
@ 
Here \texttt{order} generates an \emph{index} to properly \emph{order}
something. Above, this index is used to tell \R~to select the 4th
element of \texttt{e} first --- \texttt{order} puts the number '4'
into the first spot, indicating that \R~should put the 4th element of \texttt{e} first. Next, it places '3' in the second spot because the 3rd element of \texttt{e} belongs in the 2nd spot of an ordered vector, and so on. 

We can use \texttt{order} to sort the rows of a data frame. Here I order the rows of the data frame according to increasing order of plant heights.
<<>>=
dat
order.nos <- order(dat$height)
order.nos
@ 
This tells us that to order the rows, we have to use the 2nd row of the original data frame as the first row in the ordered data frame, the 3rd row as the new second row, etc. Now we use this index to select the rows of the original data frame in the correct order to sort the whole data frame.

<<>>=
dat[order.nos, ]
@ 
We can reverse this too, of course.
<<>>=
dat[ rev(order.nos), ] 

@

\section{Iterated Actions: the \texttt{apply} Family and Loops}
\label{sec:iter-acti-apply}
We often want to perform an action again and again and again\ldots, perhaps thousands or millions of times. In some cases, each action is independent --- we just want to do it a lot. In these cases, we have a choice of methods. Other times, each action depends on the previous action. In this case, I always use for-loops.\footnote{There are other methods we could use. These are discussed by others, under various topics, including ``flow control.'' We use ODE solvers for continuous ordinary differential equations.} Here I discuss first methods that work only for independent actions.

\subsection{Iterations of independent actions}
Imagine that we have a matrix or data frame and we want to do the same thing to each column (or row).  For this we use \texttt{apply}, to ``apply'' a function to each column (or row). We tell \texttt{apply} what data we want to use, we tell it the ``margin'' we want to focus on, and then we tell it the function. The \emph{margin} is the \emph{side} of the matrix. We describe matrices by their number of rows, then columns, as in ``a 2 by 5 matrix,'' so rows constitute the first margin, and columns constitute the second margin. Here we create a $2 \times 5$ matrix, and take the mean of rows, for the first margin. Then we sum the columns for the second margin.
<<>>=
m <- matrix( 1:10, nrow=2)
m
apply(m, MARGIN=1, mean)
apply(m, MARGIN=2, sum)
@ 
See \texttt{?rowMeans} for simple, and even faster, operations. 

Similarly, \texttt{lapply} will ``apply'' a function to each element of a list, or each column of a data frame, and always returns a list. \texttt{sapply} does something similar, but will simplify the result, to a less complex data structure if possible.

Here we do an independent operation 10 times using {\tt sapply}, defining a function on-the-fly to calculate the mean of a random draw of five observations from the standard normal distribution.
<<>>=
sapply(1:10, function(i) mean( rnorm(5) ) )
@ 
\subsection{Dependent iterations}
Often the repeated actions depend on previous outcomes, as with
population growth. Here we provide a couple of examples where we accomplish this with \emph{for loops}.

One thing to keep in mind for \emph{for loops} in \R: the computation of this is fastest if we first make a holder for the output. Here I simulate a random walk, where, for instance, we start with 25 individuals at time = 0, and increase or decrease by some amount that is drawn randomly from a normal distribution, with a mean of zero and a standard deviation 2. We will round the ``amount'' to the nearest integer (the zero-th decimal place). Your output will differ because it is a random process.

<<>>=
gens <- 10
output <- numeric(gens + 1)
output[1] <- 25
for( t in 1:gens ) output[t + 1] <- output[t] + round( rnorm(n=1, mean=0, sd=2), 0)
output

@ 
\section{Rearranging and Aggregating Data Frames}
\subsection{Rearranging or reshaping data}
We often need to rearrange our data. A common example in ecology is to collect repeated measurements of an experimental unit and enter the data into multiple columns of a spreadsheet, creating a \emph{wide} format. \R~prefers to analyze data in a single column, in a \emph{long} format. Here we use {\tt reshape} to rearrange this.

These data are carbon dioxide uptake in 12 individual plants. They are currently structured as longitudinal data; here we rearrange them in the wide format, as if we record uptake seven sequential observations on each plant in different columns. See {\tt ?reshape} for details. Here {\tt v.names} refers to the column name of the response variable, {\tt idvar} refers to the column name for the variable that identifies an individual on which we have repeated measurements, and {\tt timevar} refers to the column name which identifies different observations of {\em the same individual plant}.
<<>>=
summary(CO2)
CO2.wide <- reshape(CO2, v.names="uptake", idvar="Plant", timevar="conc",
                    direction="wide")
names(CO2.wide)
@ 
This is often how we might record data, with an experimental unit (individual, or plot) occupying a single row. If we import the data in this format, we would typically like to reorganize it in the long format, because most analyses we want to do may require this. Here, {\tt v.names} and {\tt timevar} are the names we want to use for some new columns, for the response variable and the identifier of the repeated measurement (typically the latter may be a time interval, but here it is a CO$_2$ concentration). {\tt times} supplies the identifier for each repeated observation.
<<>>=
CO2.long <- reshape(CO2.wide, 
                    v.names="Uptake", varying=list(4:10), 
                    timevar="Concentration", 
                    times=c(95, 175, 250, 350, 500, 675,1000) )
head(CO2.long)
@ 
If we wanted to, we could use {\tt order()} to re-sort the data frame, for instance to match the original.
<<>>=
CO2.long2 <- with( CO2.long, CO2.long[order(Plant, Concentration),])
head(CO2.long2)
@ 
See also the very simple functions {\tt stack} and {\tt unstack}.

\subsection{Summarizing by groups}
We often want to summarize a column of data \emph{by groups} identified in another column. Here I summarize CO$_2$ uptake by the means of each experimental treatment, chilling. The code below provides the column to be summarized ({\tt uptake}), a vector (or list of vectors) containing the group id's, and the function to use to summarize each subset (means). We calculate the mean CO$_2$ uptake for each group.
<<>>=
tapply(CO2[["uptake"]], list(CO2[["Treatment"]]), mean)
@ 
We can get fancier, as well, with combinations of groups, for each combination of Type and Treatment. 
<<>>=
tapply(CO2[["uptake"]], list(CO2[["Treatment"]], CO2[["Type"]]), sd )
@ 
We can also define a function on-the-fly to calculate both mean and standard deviation of Type and Treatment combination. We will need, however, to define groups differently, by creating the interaction of the two factors.
<<>>=
tapply(CO2[["uptake"]], list(CO2[["Treatment"]], CO2[["Type"]]), function(x) c(mean(x), sd(x)) )

@ 
See also \texttt{by} that actually uses {\tt tapply} to operate on data frames.


When we summarize data, as in \texttt{tapply}, we often want the result in a nice neat data frame. The function \texttt{aggregate} does this. Its use is a bit like {\tt tapply} --- you provide (i) the numeric columns of a data frame, or a matrix, (ii) a list of named factors by which to organize the responses, and then (iii) the function to summarize (or aggregate) the data. Here we summarize both concentration and uptake.
<<>>=
aggregate(CO2[,4:5], list(Plant = CO2[["Plant"]]), mean)

@
 A separate package entitled {\tt reshape} supplies some very elegant and
 intuitive approaches to the sorting, reshaping and aggregating of
 data frames. I typically use the \texttt{reshape} \emph{package}
 (with functions \texttt{melt} and \texttt{cast}), rather
 than the \texttt{reshape} function supplied in the \texttt{stat}. I
 do so merely because I find it a little more intuitive. \R~also has
 strong connections to relational database systems such as MySQL.
 
\section{Getting Data out of and into the Workspace}
We often want to get data into \R, and we sometimes want to get it out, as well. Here we start with the latter (referred to as \emph{writing} data), and finish with the former (referred to as \emph{reading} data).

Here I create a data frame of numbers, and write it to a text file in two different formats. The first is a file where the observations in each row are separated by tabs, and the second separates them by commas.
<<>>=
dat <- data.frame(Name=rep(c("Control","Treatment"), each=5), First=runif(10), Second=rnorm(1))
write.table(dat, file="dat.txt")
write.csv(dat, file="dat.csv")
@ 
Open these in a spreadsheet such as Calc (in OpenOffice and
NeoOffice). We can then read these into \R~using the \texttt{read.*}
family of functions. 
<<>>=
dat.new <- read.csv("dat.csv")
dat.new2 <- read.table("dat.txt", header=TRUE)
@ 
These objects will both be data frames.

Now let's get a statistical summary and export that.
<<>>=
mod.out <- summary( aov(First ~ Name, data=dat))
mod.out[[1]]
write.csv(mod.out[[1]], "ModelANOVA.csv")
@ 
Open this in a spreadsheet, such as Calc, in OpenOffice, or in any other application.

See also the \texttt{xtable} package for making tables in \LaTeX{} or
HTML formats.

\section{Probability Distributions and Randomization}
\R~has a variety of probability distributions built-in. For the normal distribution, for instance, there are four functions:
\begin{description}
\item[\texttt{dnorm}] The probability density function, that creates the widely observed bell-shaped curve.
  \item[\texttt{pnorm}] The cumulative probability function that we usually use to describe the probability that a test statistic is greater than or equal to a critical value.
    \item[\texttt{qnorm}] The quantile function that takes probabilities as input.
      \item[\texttt{rnorm}] A random number generator which draws values (quantiles) from a distribution with a specified mean and standard deviation.
\end{description}
For each of these, default parameter values return the standard normal distribution ($\mu=0$, $\sigma=1$), but these parameters can be changed. 

Here we have the 95\% confidence intervals.
<<>>=
qnorm(p=c(0.025,0.975))
@ 
Next we create a histogram using 20 random draws from a normal
distribution with a mean of 11 and a standard deviation of 6; we overlay this with the probability density function (Fig. \ref{fig:myhist}).
<<myhist, fig=true>>=
myplot <- hist(rnorm(20, m=11, sd=6), probability=TRUE)
myplot
lines(myplot$mids, dnorm(myplot$mids, m=11, sd=6) )
@ 
\begin{figure}[ht]
  \centering
  \includegraphics[width=5cm]{myhist.pdf}
  \caption{Histogram of random numbers drawn from a normal distribution with $\mu=11$ and $\sigma=6$. The normal probability density function is drawn as well.}
  \label{fig:myhist}
\end{figure}


\section{Numerical integration of ordinary differential equations}
\label{sec:numer-integr-ordin}
In order to study continuous population dynamics, we often would like to integrate complex nonlinear functions of population dynamics. To do this, we need to use numerical techniques that turn the infinitely small steps of calculus, $\D x$, into very small, but finite steps, in order to approximate the change in $y$, given the change in $x$, or $\D y /\D x$. Mathematicians and computer scientists have devised very clever ways of doing this very accurately and precisely. In \R, the best package for this is \texttt{deSolve}, which contains several \emph{solvers} for differential equations that perform numerical integration. We will access these solvers (i.e. numerical integraters) using the \texttt{ode} function in the \texttt{deSolve} package. This function, \texttt{ode}, is a ``wrapper'' for the underlying suite of functions that do the work. That is, it provides a simple way to use any one of the small suite of functions.

When we have an ordinary differential equation (ODE) such as logistic growth,\footnote{$e.g. dN/dt = rN(1-\alpha N)$} we say that we ``solve'' the equation for a particular time interval given a set of parameters and initial conditions or initial population size. For instance, we say that we solve the logistic growth model for time at $t=0,\, 1 \ldots \, 20$, with parameters $r=1$, $\alpha=0.001$, and $N_0=10$. 

Let's do an example with \texttt{ode}, using logistic growth. We first have to define a function in a particular way. The arguments for the function must be time, a vector of populations, and a vector or list of model parameters.
<<>>=
logGrowth <- function(t, y, p){
  N <- y[1]
  with(as.list(p), {
    dN.dt <- r * N * (1 - a * N)
    return( list( dN.dt ) )
  } )
}
@  
Note that I like to convert $y$ into a readable or transparent state variable ($N$ in this case). I also like to use \texttt{with} which allows me to use the names of my parameters \cite{Petzoldt:2003dp}; this works only is \texttt{p} is a vector with named paramters (see below). Finally, we return the derivative as a list of one component. 

The following is equivalent, but slightly less readable or transparent.
<<>>=
  logGrowth <- function(t, y, p){
    dN.dt <- p[1] * y[1] * (1 - p[2] * y[1])
    return( list( dN.dt ) )
  }
@ 
To solve the ODE, we will need to specify parameters, and initial conditions. Because we are using a vector of named parameters, we need to make sure we name them! We also need to supply the time steps we want.
<<>>=
p <- c(r=1, a = 0.001)
y0 <- c(N=10)
t <- 1:20
@ 
Now you put it all into \texttt{ode}, with the correct arguments. The output is a matrix, with the first column being the time steps, and the remaining being your state variables. First we load the \texttt{deSolve} package.
<<>>=
library(deSolve)
out <- ode(y=y0, times=t, func=logGrowth, parms=p)
out[1:5,]
@ 
If you are going to model more than two species, y becomes a vector of length 2. Here we create a function for Lotka-Volterra competition, where 
\begin{align}
  \label{eq:1}
  \frac{\D N_1}{\D t} &= r_1N_1\left(1 - \alpha_{11}N_1 - \alpha_{12}N_2\right)\\
  \frac{\D N_2}{\D t} &= r_2N_2\left(1 - \alpha_{22}N_2 - \alpha_{21}N_1\right)\\
\end{align}
<<>>=
LVComp <- function(t, y, p){
  N <- y
  with(as.list(p), {
    dN1.dt <- r[1] * N[1] * (1 - a[1,1]*N[1] - a[1,2]*N[2])
    dN2.dt <- r[2] * N[2] * (1 - a[2,1]*N[1] - a[2,2]*N[2])
    return( list( c(dN1.dt, dN2.dt) ) )
  } )
}
@ 
Note that \texttt{LVComp} assumes that $N$ and $r$ are vectors, and the competition coefficients are in a matrix. For instance, the function extracts the the first element of \texttt{r} for the first species (\texttt{r[1]}); for the intraspecific  competition coefficient for species 1, it uses the element of \texttt{a} that is in the first column and first row (\texttt{a[1,1]}).  The vector of population sizes, $N$, contains one value for each population \emph{at one time point}. Thus here, the vector contains only two elements (one for each of the two species); it holds only these values, but will do so repeatedly, at each time point. Only the output will contain all of the population sizxes through time. 

To integrate these populations, we need to specify new initial conditions, and new parameters for the two-species model.
<<>>=
a <- matrix(c(0.02, 0.01, 0.01, 0.03), nrow=2)
r <- c(1,1)
p2 <- list(r, a)
N0 <- c(10,10)
t2 <- c(1,5,10,20)
out <- ode(y=N0, times=t2, func=LVComp, parms=p2)
out[1:4,]
@ 

The \texttt{ode} function uses a superb ODE solver, \texttt{lsoda}, which is a very powerful, well tested tool, superior to many other such solvers. In addition, it has several bells and whistles that we will not need to take advantage of here, although I will mention one, \texttt{hmax}. This tells \texttt{lsoda} the largest step it can take. Once in a great while, with a very \emph{stiff} ODE (a very wiggly complex dynamic), ODE assumes it can take a bigger step than it should. Setting \texttt{hmax} to a smallish number will limit the size of the step to ensure that the integration proceeds as it should. 

One of the other solvers in the \texttt{deSolve}, \texttt{lsodar}, will also return roots (or equilibria), for a system of ODEs, if they exist. Here we find the roots (i.e. the solutions, or equilibria) for a two species enemy-victim model.
<<>>=
EV <- function(t, y, p){
  with(as.list(p), {
    dv.dt <- b * y[1]*(1-.005*y[1]) - a*y[1]*y[2]
    de.dt <-a*e*y[1]*y[2] - s*y[2]
    return( list( c(dv.dt, de.dt) ) )
  } )
}
@ 
To use  \texttt{lsodar} to find equilibria, we need to specify a root finding function whose inputs are are the sme of the ODE function, and which returns a scalar (a single number) that determines whether the rate of change ($\D y/\D x$) is sufficiently close to zero that we can say that the system has stopped changed, that is, has reached a steady state or equilibrium. Here we sum the absolute rates of change of each species, and then subtract 10$^{-10}$; if that difference is zero, we decide that, for all pratcial purposes, the system has stopped changing.
<<>>=
rootfun <- function (t,y,p) 
{ 
dstate <- unlist(EV(t,y,p)) #rate of change vector 
return(sum(abs(dstate))-1e-10) 
} 
@ 
Note that \texttt{unlist} changes the \texttt{list} returned by \texttt{EV} into a simple vector, which can then be summed.

Next we specify parameters, and time. Here all we want is the root, so we specify that we want the value of $y$ after a really long time ($t=10^{10}$). The \texttt{lsodar} function will stop sooner than that, and return the equilibrium it finds, and the time step at which it occurred.
<<>>=
p <- c(b = 0.5, a = 0.02, e=0.1, s=0.2)
t <- c(0,1e10)
@ 
Now we run the function.
<<>>=
out <- ode(y=c(45,200), t, EV, parms=p, rootfun=rootfun, method="lsodar")
out[,]
@ 
Here we see that the steady state population sizes are $V=100$ and $E=12.5$, and that given our starting point, this steady state was achieved at $t=500.8$. Other information is available; see \texttt{?lsodar} after loading the \texttt{deSolve package}.

\section{Numerical Optimization}
\label{sec:numer-optim}
We frequently have a function or a model that we think can describe a pattern or process, but we need to ``play around with'' the numerical values of the constants in order to make the right shape with our function/model. That is, we need to find the value of the constant (or constants) that create the ``best'' representation of our data. This problem is known as \emph{optimization}. 

Optimization is an entire scientific discipline (or two). It boils down to quickly and efficiently finding parameters (i.e. constants) that meet our criteria. This is what we are doing when we ``do'' statistics. We fit models to data by telling the computer the structure of the model, and asking it to find values of the constants that minimize the residual error.

Once you have a model of the reality you want to describe, the basic steps toward optimization we consider are (i) create an \emph{objective function}, (ii) use a routine to \emph{minimize} (or \emph{maximize}) the objective function through optimal choice of parameter values, and (iii) see if the ``optimal'' parameters values make sense, and perhaps refine and interpret them.

An \emph{objective function} compares the data to the predicted values from the model, and returns a quantitative measure of their difference. One widely used objective function the \emph{least-squares criterion}, that is, the objective function is the average or the sum of the squared deviations between the model values and the data --- just like a simple ANOVA might. An optimization routine then tries to find model parameters that minimize this criterion. 

Another widely used objective function is the likelihood function, or \emph{maximum likelihood}. The likelihood function uses a probability distribution of our choice (often the normal distribution). The objective function then calculates the collective probability of observing those data, given the parameters and fit of the model. In other words, we pretend that the model and the predicted values are true, measure how far off each datum is from the predicted value, and then use a probability distribution to calculate the probability of seeing each datum. It then multiplies all those probabilities to get the \emph{likelihood} of observing those data, given the selected parameters. An optimization routine then tries to find model parameters that maximize this likelihood. In practice, it is more computationally stable to calculate the negative of the sum of the logarithms of the probabilities, and try to minimize that quantity, rather than maximize the likelihood --- but in principle they are they same thing.

Once we have an objective function, an optimization routine makes educated guesses regarding good values for the parameters, until it finds the best values it can, those which minimize the objective function. There are a great variety of optimization routines, and they all have their strengths. One important tradeoff they exhibit is that the fastest and most accurate methods are sometimes the least able to handle difficult data \cite{Bolker:2008rr}. Below, we rely on a combination to take advantage of the strengths of each type.


Here we introduce two of \R's general purpose functions in the \texttt{base} package, \texttt{optimize} and \texttt{optim}, and another, in the \texttt{bbmle} package, \texttt{mle2} \cite{Bolker:2008rr}. The function \texttt{optimize} should be used where we are in search of one parameter; use others when more than one parameter is being optimized. There are many other optimization routines in \R, but we start here\footnote{Indeed, all statistical models are fancy optimization routines.}. 

Here we start with one of \R's general optimization functions, the one designed for finding a single parameter. Let us find the mean ($\bar{x}$) of some data through optimization. We will start with data, \texttt{y}, and let our conceptual model of that data be $\mu$, the mean. We then create a \emph{objective function} whose output will get smaller as the parameter of our model approaches the value we want. Poughly speaking, the mean is the value that minimizes the total difference between all the data and the itself. We will use the least-squares criterion, where the sum of all the squared deviations reaches a minimum when $\mu$ approaches the mean.
<<>>=
y <- c(1, 0:10)
f <- function(y, mu) { sum( (y-mu)^2 ) }

@ 
Our function, \texttt{f}, subtracts $\mu$ from each value of $y$, squares each of these differences, and then sums these squared differences, to get the sum of squares. Our goal is to minimize this. If we guessed at it by hand, we would get this (Fig. \ref{fig:LS}).
<<LS, fig=true>>=
guesses <- seq(4, 6, by=.05)
LS.criterion <- sapply(guesses, function(mu) f(mu=mu, y=y))
plot(guesses, LS.criterion, type='l')
@ 
\begin{figure}[ht]
  \centering
  \includegraphics[width=8cm]{LS}
  \caption{Illustration of the least squares criterion. Our objective function returns (i.e. generates) the squared deviations between the fitted model and the data. Optimization minimizes the criterion (``LS.criterion'') and thereby finds the right guess (x axis).}
  \label{fig:LS}
\end{figure}
Fig. \ref{fig:LS} shows us that the minimum of the objective function occurs when \texttt{mu} is a little over 4.5. Now let's let \R~minimize our least squared deviations. With \texttt{optimize}, we provide the function first, we then provide a range of possible values for the parameter of interest, and then give it the values of parameters or data used by the function, other than the parameter we want to fit.
<<optimize>>=
(results <- optimize(f, c(0,10), y=y))
@ 
We see that \texttt{optimize} returns two components in a list. The first is called \texttt{minimum}, which is the parameter value that causes our function \texttt{f} to be at a minimum. The second component, \texttt{objective} is the value of {\tt f} when \Sexpr{paste('mu =', round(results[[1]],3))}.

Next we demonstrate \texttt{mle2}, a function for maximum likelihood estimation. Maximum likelihood relies on probability distributions to find the probability of observing a particular data set, assuming the model is correct. This class of optimization routines finds the parameters that maximize that probability.

Let us solve the same problem as above. For the same data, \texttt{y}, we create a maximum likelihood function to calculate the mean. In maximum likelihood, we actually minimize the negative logarithm of the likelihood because it is more computationally stable --- the same parameters that minimize the negative log-likelihood also maximize the likelihood. We assume that the data are normally distributed, so it makes sense to assume that the probabilities derive from the normal probability density function.
<<>>=
LL <- function(mu, SD){
  -sum( dnorm(y, mean=mu, sd=SD, log=TRUE) )
  }
@ 
This objective function calculates the negative logarithm of the probability density of each datum, given a particular mean and standard deviation, \texttt{mu, SD}. The optimization routine, \texttt{mle2}, then finds \texttt{mu} and \texttt{SD} that minimize the negative log-likelihood of those data.
<<mle2>>=
library(bbmle)
(fit <- mle2(LL, start=list(mu=5, SD=1), control=list(maxit=10^5) ) )
@
Another way to put this objective function into \texttt{mle2} is with a formula interface.
<<>>=
mle2(y ~ dnorm(mu, sd=SD), start=list(mu=1, SD=2))
@ 
We can examine this more closely, examing the probablities associated with the profile confidence intervals.
<<>>=
summary(fit)
pr <- profile(fit)
<<profile, fig=TRUE, width=8>>=
par(mar=c(5,4,3,2))
plot(pr)
@ 
\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{profile}
\caption{Profile confidence intervals for various limits, based on \texttt{mle2}.}
\label{fig:pci}
\end{figure}

Often we have reason to limit parameters to particular bounds. Most often, we may need to ensure that a parameter is greater than zero, or less than zero, or less often between zero and one. Sometimes we have a rationale based on physical or biological constraints that will limit a parameter within particular values.

To constrain parameters, we could use a routine that applies constraints directly (see particular optimization methods under \texttt{mle2},\texttt{nlminb}, and \texttt{optim}). We could also transform the parameters, so that the optimizer uses the transformed version, while the ODE model uses the parameters in their original units. For instance, we could let the optimizer find the best value of a logarithm of our parameter that allows the original parameter to make model predictions that fit the data. Another consequence of using logarithms, rather than the original scale is that it facilitates computational procedures in estimating vary large and very small numbers. An example helps make this clear --- see an extended example in Chap. 6, on disease models.

@
\section{Derivatives}
We can use \texttt{deriv} and \texttt{D} to have \R~provides derivatives. First we supply an expression, and then we get gradients.
<<>>=
host1 <- expression(R*H*(1+a*P)^-k)
D(host1, "H") 

@ 
\section{Graphics}
\R~is well known for its graphics capabilities, and entire books have been written of the subject(s). For beginners, however, \R~can be frustrating when compared to the point-and-click systems of most graphics ``packages.'' This frustration derives from two issues. First, \R's graphics have of a learning curve, and second, \R~requires us to type in, or code, our specifications. The upsides of these are that \R~has infinite flexibility, and total replicability, so that we get exactly the right figure, and the same figure, every time we run the same code. 

\subsection{\texttt{plot}}
The most used graphics function is \texttt{plot}. Here I demonstrate several uses.

First let's just create the simplest scatterplot (Fig. \ref{fig:eg1}).
<<Myplot, fig=true>>=
data(trees); attach(trees)
plot(Girth, Height)
@ 

\begin{figure}[ht]
  \centering
  \subfloat[Simple]{\includegraphics[width=.48\linewidth]{Myplot.pdf}\label{fig:eg1}}
  \subfloat[With Colors]{\includegraphics[width=.48\linewidth]{MyTrees1.pdf}\label{fig:eg2}}\\
  \subfloat[Two Y vars.]{\includegraphics[width=.48\linewidth]{Mymatplot.pdf}\label{fig:eg3}}
  \subfloat[Two Y axes]{\includegraphics[width=.48\linewidth]{MyTrees2Y.pdf}\label{fig:eg4}}
  \caption[Examples of graphics]{See code for graphics parameters used to generate these plots. Fig. \subref{fig:eg2} uses an alternate color scheme that provides human perception-adjusted hsv (hue, saturation, and value) specification.}
  \label{fig:eg}
\end{figure}

To this we can add a huge variety of variation, using arguments to \texttt{plot}.
\begin{table}[ht]
  \caption{Commonly used arguments to \texttt{plot}. See help pages at \texttt{?plot} and \texttt{?plot.default} for more information.}
  \centering
  \begin{tabular}{lp{9cm}}
    \hline
    Argument & Meaning \\
    \hline
    \texttt{type} & Determines the type of X-Y plot, for example \textbf{p}, {\bf l}, {\bf s}, for points, lines, stair-step, and none, respectively. ``None'' is useful for setting up a plotting region upon which to elaborate (see example below). Defaults to {\bf p}; see ?plot.default for other types. \\
    \texttt{axes} & Indicates whether to plot the axes; defaults to TRUE. Useful if you want more control over each axis by using the \texttt{axis} function separately (see below).\\
        \texttt{pch} & Point character (numeric value, 1--21). This can be a single value for an entire plot, or take on a unique value for each point, or anything in between. Defaults to 1. \emph{To add text more than one character in length, for instance a  species name, we can easily add text to a plot at each point (see the next section)}.\\
            \texttt{lty} & Line type, such as solid (1), dashed (2), etc.  Defaults to 1.\\
                \texttt{lwd} & Line width (numeric value usually 0.5--3; default is 1). \\
                    \texttt{col} & Color; can be specified by number (e.g., 2), or character (e.g. ``red''). Defaults to 1 (``black''). \R~has tremendous options for color; see \texttt{?hcl}.\\
                   \texttt{main, ylab, xlab~} & Text for main title, or axis labels. \\
                   \texttt{xlim, ylim} & Limits for x and y axes, e.g. \texttt{ylim=c(0, 1.5)} sets the limits for the y-axis at zero and 1.5. Defaults are calcualted from the data. \\
                  \texttt{log} & Indicates which axes should use a (natural) logarithm scale, e.g. \texttt{log = `xy'} causes both axes to use logarithmic scales.\\
                  \hline
  \end{tabular}
\end{table}

\subsection{Adding points, lines and text to a plot}
After we have started a plot, we may want to add more data or information. Here set up a new graph without plotting points, add text at each point, then more points, a line and some text.
<<eval=false>>=
par(mar=c(5,4,3,2))
plot(Girth, Volume, type='n', main = "My Trees")
points(Girth, Volume, type='h' , col="lightgrey", pch=19)
@ 
Now we want to add points for these data, using the tree heights as the plotting symbol. We are going to use an alternate coloring system, designed with human perception in mind (\texttt{hcl}). We scale the colors so that the hue varies between 30 and 300, depending on the height of the tree; I allow the symbols to be transparent (90\% opaque) overlapping. I also allow the size of the numbers to vary with height ({\tt cex = 0.5 + hts}) Last, we add a legend (Fig. \ref{fig:eg2}). 
<<eval=false>>=
hts <- (Height-min(Height))/max(Height-min(Height))
my.colors <- hcl(h=30 + 270 * hts, alpha=0.90)
text(Girth,  Volume, Height,  col=my.colors, cex = 0.5 + hts )
<<MyTrees1, fig=true, echo=false, results=hide>>=
par(mar=c(5,4,3,2))
plot(Girth, Volume, type='n', main = "My Trees")
points(Girth, Volume, type='h' , col="lightgrey", pch=19)
hts <- (Height-min(Height))/max(Height-min(Height))
my.colors <- hcl(h=30 + 270 * hts, l=50, alpha=0.90)
text(Girth,  Volume, Height,  col=my.colors, cex = 0.5 + hts )
@ 
\subsection{More than one response variable}
We often plot more than one response variable on a single axis. We could use {\tt lines} or {\tt points} to add each additional variable. We could also use {\tt matplot} to plot a matrix of variables {\em vs.} one predictor (Fig. \ref{fig:eg3}).
<<Mymatplot, fig=true>>=
trees.sort <- trees[order(trees$Girth, trees$Height),]
matplot(trees.sort$Girth, trees.sort[,2:3], type='b')
text(18, 40, "Volume", col="darkred")
text(10, 58, "Height")
@ 
We frequently want to add a second y-axis to a graph that has a different scale  (Fig. \ref{fig:eg4}).  The trick we use here is that we plot a graph, but then tell \R~we want to do the next command ``\ldots \emph{as if it was on a new device}''\footnote{From the \texttt{par} help page.} while it really is not. We overlay what we just did with new stuff, without clearing the previous stuff. 

For our example, let's start with just X and our first Y. Note we also specify extra margin space room on the right hand side, preparing for the second Y axis.
<<>>=
quartz(,4,4)
par(mar=c(5,4,2,4))
plot(Girth, Volume, main = "My Trees")

@ 
Now we try our trick. We draw a new plot ``as if'' it were a new graph. We use the same X values, and the new Y data, and we also specify no labels. We also use a different line type, for clarity.
<<>>=
par(new=TRUE)
plot(Girth, Height, axes=FALSE, bty="n", xlab="", ylab="", pch=3)
@ 
Now we put the new Y values on the fourth side, the right hand Y axis. We add a Y axis label using a function for \emph{marginal text} (Fig. \ref{fig:eg4}).
<<>>=
axis(4)
mtext("Height", side=4, line=3) 

<<MyTrees2Y, fig=true>>=
par(mar=c(5,4,2,4))
plot(Girth, Volume, main = "My Trees")
par(new=TRUE)
plot(Girth, Height, axes=FALSE, bty="n", xlab="", ylab="", pch=3)
axis(4)
mtext("Height", side=4, line=3) 

@ 
\subsection{Controlling Graphics Devices}
When we make a graph with the \texttt{plot} function, or other function, it will typically open a graphics window on the computer screen automatically; if we desire more control, we can use several functions to be more deliberate.
We create new graphics ``devices'' or graphs in several ways, including the functions \texttt{windows()} (Microsoft Windows OS), \texttt{quartz()} (Mac OS), \texttt{x11()} (X11 Window system). For instance, to open a ``graphics device'' on a Mac computer that is 5 inches wide and 3 inches tall, we write
<<>>=
quartz(width=5, height=3)
@ 
To do the same thing on a computer running Windows, we type
<<eval=false, results=hide>>=
windows(width=5, height=3)
@ 
To control the \emph{par}ameters of the graph, that is, what it looks like, aside from data, we use arguments to the \texttt{par} function. Many of these arguments refer to \emph{sides} of the graph. These a numbered 1--4 for the bottom X axis, the left side Y axis, the top, and the right side Y axis. Arguments to \texttt{par} are many (see \texttt{?par}), and include the following.
\begin{description}
\item[\texttt{mar}] controls the width of margins on each side; units are number of lines of text; defaults to c(5, 4, 4, 2) + 0.1, so the bottom has the most room, and the right hand side has the least room.
  \item[\texttt{mgp}] controls the spacing of the axis title, labels and the actual line itself; units of number of lines of text, and default to c(3, 1, 0), so the axis title sits three lines away from the edge of the plotting region, the axis labels, one line away and the axis line sits at the edge of the plotting region.
  \item[\texttt{tcl}] tick length, as a fraction of the height of a line of text; negative values put the tick marks outside, positive values put the tick marks inside. Defaults to -0.5.
\end{description}

We can build each side of the graph separately by initiating a graph but not plotting axes \texttt{plot(..., axes = FALSE)}, and then adding the axes separately. For instance, \texttt{axis(1)} adds the bottom axis.

Last, we can use \texttt{layout} to make graph with several smaller subgraphs (see also (\texttt{mfrow} and \texttt{mfcol} arguments to \texttt{par} and the function \texttt{split.screen}). The function \texttt{layout} takes a matrix as its argument, the matrix contains a sequence of numbers that tells \R~how to fill the regions Graphs can fit in more than one of these regions if indicated by the same number.

Here we create a compound graphic organized on top of a $4 \times 4$ grid; it will have two rows, will be be filled in by rows. The first graph will be the upper left, the second the upper right, and the third will fill the third and fourth spots in the second. We will fill each with a slightly different plot of the same data (Fig. \ref{fig:par}).
<<>>=
quartz(,5,5)
layout( matrix( c(1,2,3,3), nrow=2, byrow=TRUE) )

plot(Girth, Height)
@ 
Now we add the second and third ones but with different settings.
<<>>=
par(mar=c(3,3,1,1), mgp=c(1.6, .2, 0), tcl=.2)
plot(Girth, Height)
par(mar=c(3,3,2,1), mgp=c(1.6, .2, 0), tcl=.2)
plot(Girth, Height, axes=FALSE, xlim=c(8,22) )
axis(1, tcl=-.3)
axis(2, tick=F)
rug(Height, side=2, col=2)
title("A Third, Very Wide, Plot")
<<results=hide, echo=false>>=
dev.print(pdf, 'MyTrees4.pdf')
@ 
\begin{figure}[ht]
  \centering
  \includegraphics[width=12cm]{MyTrees4.pdf}
  \caption{A variety of examples with different graphics \emph{par}ameters.}
  \label{fig:par}
\end{figure}
\subsection{Creating a Graphics File}
Now that you have made this beautiful thing, I suppose you would like to stick it into a manuscript. One way to get graphics out of \R~and into something else (presentation software, a manuscript), is to create a graphics device, and then save it with \texttt{dev.print} in\ a format that you like, such as PDF, postscript, PNG, or JPEG.

For instance, we might do this to save a graphics file in our working directory.
<<results=hide>>=
getwd()
quartz(,4,4)
plot(Height, Volume, main="Tree Data")
dev.print(pdf, "MyTree.pdf")

@ 
This should have saved a small PDF figure in your current working directory, returned by {\tt getwd}.

\emph{You will have to find your own way to make graphics files that suits your operating system, your preferred applications, and your personality.}

\section{Graphical displays that show distributions}
Here we take a quick look at ways to reveal distributions of data.
First, two views to see in the Console, a six number summary of quantiles and the mean, and the good ol' stem and leaf plot, a favorite of computational botanists everywhere.
<<>>=
summary(Girth)
stem(Girth)
@ 
Here we will create 4 various plots revealing different ways to look at your data, each with a couple bells and whistles. For kicks, we put them into a single compound figure, in a ``layout'' composed of a matrix of graphs.
<<results=hide, echo=false>>=
quartz(,6,7)
<<>>=
layout( matrix( c(1,2,2,3,4,4), nrow=2, byrow=TRUE ) ) 
plot(1:length(Girth), Girth, xlab="Order of Sample Collection?")
hist(Girth, prob=TRUE)
rug(Girth)
lines(density(Girth))
boxplot(Girth, main="Boxplot of Girth")
points(jitter( rep(1,length(Girth)) ), Girth)
qqnorm(log(Girth))
qqline(log(Girth))
title(sub="Log transformed data")
<<results=hide, echo=false>>=
dev.print(pdf, "Distributions.pdf")

@ 
\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Distributions}
  \caption{Examples of ways to look at the distribution of your data. See \texttt{?hist}, for example, for more information.}
  \label{fig:dists}
\end{figure}

\section{Eigenanalysis}
Performing eigenanalysis in \R~is easy.  We use the \texttt{eigen} function which returns a list with two components. The first named component is a vector of eigenvalues and the second named component is a matrix of corresponding eigenvectors. These will be numeric if possible, or complex, if any of the elements are complex numbers.

Here we have a typical demographic stage matrix.
<<>>=
A <- matrix(c(0, .1, 10, .5), nrow=2)
eig.A <- eigen(A)
str(eig.A)
@ 
Singular value decomposition (SVD) is a generalization of eigenanalysis and is used in \R for some applications where eigenanalysis was used historically, but where SVD is more numerically accurate (\texttt{prcomp} for principle components analysis).

\section{Eigenanalysis of demographic versus Jacobian matrices}

Eigenanalyses of demographic and Jacobian matrices are worth comparing. In one sense, they have similar meanings --- they both describe the asymptotic (long-term) properties of a system, either population size (demographic matrix) or a perturbation at an equilibrium. The quantitative interpretation of the eigenvalues will therefore differ.

In the case of the stage (or age) structured demographic model, the elements of the demographic matrix are discrete per capita increments of change over a specified time interval. This is directly analogous to the finite rate of increase, $\lambda$, in discrete unstructured models. (Indeed, an unstructured discrete growth model is a stage-structured model with one stage). Therefore, the eigenvalues of a demographic matrix will have the same units --- a per capita increment of change. That is why the dominant eigenvalue has to be greater than 1.0 for the population to increase, and less than 1 (not merely less than zero) for the population to decline.

In the case of the Jacobian matrix, comprised of continuous partial differential equations, the elements are  per capita \emph{instantaneous} rates of change. As differential equations, they describe the \emph{instantanteous} rates of change, analogous to $r$. Therefore, values greater than zero indicate increases, and values less than zero indicate decreases. Because these rates are evaluated at an equilibrium, the equilibrium acts like a new zero --- positive values indicate growth away from the equilibrium, and negative values indicate shrinkage back toward the equilibrium.  When we evaluate these elements at the equilibrium, the numbers we get are in the same units as $r$, where values greater than zero indicate increase, and values less than zero indicate decrease. The change they describe is the instantaneous per capita rate of change of each population with respect to the others. The eigenvalues summarizing all of the elements Jacobian matrix thus must be less than zero for the disturbance to decline.

So, in summary, the elements of a demographic matrix are discrete increments over a real time interval. Therefore its eigenvalues  represent relative per capita growth rates a discrete time interval, and we interpret the eigenvalues with respect to 1.0. On the other hand, the elements of the Jacobian matrix are instantaneous per captia rates of change evaluated at an equilibrium. Therefore its eigenvalues represent the per capita \emph{instantaneous} rates of change of a tiny perturbation at the equilibrium. We interpret the eigenvalues with respect to 0 indicating whether the perturbation grows or shrinks.

## Derivation of the right eigenvalues - check!!

\begin{align*}
Aw &=\lambda w\\
\lambda w - Aw &=0\\
\lambda I w - Aw &=0 \\ %% really?
\lambda I - A =0
\end{align*}

This implies that the determinant of the absolute value is also zero
\begin{align*}

$$ \mathrm{det} |\lambda I - A| =0$$
$$ \mathrm{det} \begin{vmatrix} 
\lambda-a & b \\
c & \lambda - d 
\end{vmatrix} = (\lambda-a)(\lambda-d) - bc = 0$$
$$\lambda^2 -(a+d)\lambda + (ad-bc) = 0$$
$$\lambda = \frac{-(a+d) \pm \sqrt{(a+d)^2 - 4(ad-bc)}}{2}$$

And that is a way to find the two eigenvalues of a $2 \times 2$ matrix, whether it is a demographic projection matrix, predator-prey interaction matrix, or something else.






\section{Symbols used in this book}
\index{$\Alpha$@greek symbols}
I am convinced that one of the biggest hurdles to learning theoretical ecology --- and the one that is easiest to overcome --- is to be able to ``read'' and hear them in your head. This requires being able to pronounce Greek symbols. Few of us learned how to pronounce ``$\alpha$'' in primary school. Therefore, I provide here an incomplete simplistic American English pronunciation guide  for (some of) the rest of us, for symbols in this book. Only a few are tricky, and different people will pronounce them differently.
\begin{table}[hb]
  \centering
  \caption{Symbols and their pronunciation; occasional usage applies to lowercase, unless otherwise specified. A few symbols have common variants. Any symbol might be part of any equation; ecologists frequently ascribe other meanings which have to be defined each time they are used. See also http://en.wikipedia.org/wiki/Greek\_letters}
\begin{tabular}[c]{llp{10cm}}
  \hline \hline Symbol~ & Spelling & Pronunciation; occasional or conventional usage \\
\hline
A, $\alpha$ & alpha & al'-fa; point or local diversity (or a parameter in the logseries abundance distribution)\\
B, $\beta$ & beta & bay'-ta; turnover diversity \\
$\mathrm{\Gamma}$, $\gamma$ & gamma & gam'-ma; regional diversity \\
$\mathrm{\Delta}$, $\delta$, $\partial$ & delta & del'-ta; change or difference \\
E, $\epsilon$, $\varepsilon$ & epsilon & ep'-si-lon; error \\
$\mathrm{\Theta}$, $\theta$ & theta & thay'-ta (``th'' as in ``thanks''); in neutral theory, biodiversity. \\
$\mathrm{\Lambda}$, $\lambda$ & lambda & lam'-da; eigenvalues, and finite rate of increase \\
M, $\mu$ & mu & meeoo, myou; mean  \\
N, $\nu$ & nu  & noo, nou \\
$\mathrm{\Pi}$, $\pi$ & pi  & pie; uppercase for product (of the elements of a vector) \\
P, $\rho$ & rho  & row (as in ``a boat''); correlation \\
$\mathrm{\Sigma}$, $\sigma$, $\varsigma$ & sigma  & sig'-ma; standard deviation (uppercase is used for summation)\\
T, $\tau$ & tau & (sounds like what you say when you stub your toe - ``Ow!'' but with a ``t'').   \\
$\Phi$, $\phi$ & phi  & fie, figh \\
X, $\chi$ & chi  & kie, kigh \\
$\mathrm{\Psi}$, $\psi$ & psi  & sie, sigh \\
$\mathrm{\Omega}$, $\omega$ & omega  & oh-may'-ga; degree of omnivory \\
\hline
\end{tabular}

\end{table}

<!--chapter:end:appendix.Rmd-->

---
title: "Untitled"
output: html_document
---
### Proportional change
If all we are interested in is the proportional change with each time step, that is simply 
$$\frac{N_{t+1}}{N_t}$$
where $t$ is a particular time step, suych as two weeks. For our flies, that is simply
```{r}
N[2:4] / N[1:3]
```
showing that the population doubles every two weeks. When we treat this proportional cahnge as a constant, we typically symbolize it as $\frac{N_{i+1}}{N_i} = \lambda$, and refer to it as the *finite rate of increase* of a population.

If we want to project the population size into the future, we can use $\lambda$, where 
$$N_{t+1} = N_t \lambda$$
$$N_{t+2} = N_{t+1} \lambda = \left(N_{t}\lambda\right) \lambda $$
$$N_{t+3} = N_{t+2} \lambda = \left(N_{t}\lambda\right) \lambda \lambda$$
From this we can infer this rule:
$$N_{t} = N_0 \lambda^t$$
where $N_0$ is the initial population size, and $t$ is however many time steps we want to project into the future. We refer to this as the *projection* form of geometric growth.

When $\lambda = 1$ the population is neither growing nor shrinking. When $\lambda > 1$, the population is growing, and hen $\lambda < 1$, the population is shrinking. Finally, $\lambda$ can never be less than zero.




### Growth rates
The *population growth rate* the *change in population size per unit time*. In this case we can say
$$\frac{\Delta N}{\Delta t} = \frac{N_{t+\Delta t}-N_t}{\Delta t}$$
where $t$ is any point in time, and $\Delta t$ is interval between successive time points. In our case, if $t=0$, then $N_0 = 2$, and $N_{0+2} = 4$. Therefore, we have 
$$\frac{\Delta N}{\Delta t} = \frac{4-2}{2} = 1$$ flies per week, at least in the first week. Let's do that calculation for each time period.

The *per capita growth rate* is the population growth rate divided by $N$, or
$$\frac{\Delta N}{\Delta t N_t} = \frac{N_{t+\Delta t}-N_t}{\Delta t} \frac{1}{N_t}$$
```{r}
# change in population sizes
DeltaN <- N[2:4] - N[1:3]
Deltat <- t[2:4] - t[1:3]
DN.Dt <- DeltaN / Deltat
DN.DtN <- DN.Dt / N[1:3]
cbind(Deltat, DeltaN, DN.Dt, DN.DtN)
```
From these columns of calculations, we see that (i) the time intervals are constant (two weeks apart), (ii) the change in population size is getting progressively bigger, (iii) the population growth rate is increasing, and (iv) the per capita growth rate is constant. 

If we wish to project the size of the population into the future, we can multiply the size of the population times the per capita growth rate and add that to the population. Let the per capita growth rate be 
$$r_d=\frac{\Delta N}{ \Delta t N_t}$$.

To project the population, we simply let

$$N_{t + \Delta t} = N_t + N_t r_ = N_t(1+r_d)$$
$$N_{t + 2\Delta t} =  N_{t+\Delta t}(1+r_d) = N_t(1+r_d)^2$$

Now we see the relation between the per capita growth rate, the population growth, and the finite rate of increase.

### Fruit flies with continuous overlapping generations

In the reality that is my aseasonal kitchen, individual fruit flies are having sex and reproducing on their own schedules - they breed continuously and the cohorts are not synchronous. For populations like that, we need to describe instantaneous growth rates, where $\Delta t$ i no longer a fixed period of time, but is the rate at a given instant. For this we need differential colculus, where the differential rate equation is
$$\frac{dN}{dt} = rN$$
Note the similarity to and difference from $\lambda$. When $ r = 0$ the population is neither growing nor shrinking. When $r > 0$, the population is growing, and when $r < 0$, the population is shrinking. 

To project a population, we integrate $rN$ with respect to time.
$$dN/N = r dt$$
$$\int_{N_0}^{N_t} \frac{1}{N}dN = \int_{0}^{t}rdt$$


```{r diagram, eval=FALSE, echo=FALSE}
library(diagram)
par(mar=c(1,1,1,1))
names <- c("A", "F", "B")
f <- matrix(0, nc=3, nr=3)

f[2,1] <- "I"
f[3,2] <- "E~D"
f[2,2] <- "B"
arr.hd <- matrix(c(0,1,0, 0,1,1, 0,0,0), nr=3)
pp <- plotmat(f,pos=c(3),name=names,lwd=1,box.lwd=2,cex.txt=0.8,
             box.type="circle", box.size=0.05, box.lcol=c(0,1,0),
             shadow.size=0, arr.pos=arr.hd,
             self.shiftx=0, self.shifty=-.05, self.arrpos=.5)
```

<!--chapter:end:ch1KindOfNew.Rmd-->

---
title: "models and observations"
author: "Hank Stevens"
date: "9/19/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(deSolve)
```

Most of this book describes and explains models. Models are explicit manifestations of theory with which we can organize our thoughts and and compare with our data. Here we bring our attnetion to two layers of uncertainty when we compare models and data: *process error* and *observation error*. These types of error are not mistakes, but differences from "actual" values. These terms can be confusing if we think that "error" means "mistake". They aren't; they are just differences between reality and our guesses of reality.

Observation error is the uncertainty associated with drawing inferences about a population based on only samples of the population. We can never know the actual abundance of tuna in the sea because we can only estimate it with uncertainty. We refer to the uncertainty due to our observation methods as *observation error*. It is not error in the colloquial sense of making a mistake, but simply a difference between an actual population, and our sample of it.

Process error is not error at all, in the colloquial sense. It is uncertainty in the very process we are studying. We can use the logistic growth model to predict the dynamics of a population, but the underlying processes of reproduction and density-dependence will vary for mechanistic reasons that we cannot fathom, or which we choose judiciously to ignore. Process error is the some underlying mechanistic difference between our model and the actual ecological process.

Any time we examine a time series of data, we need to envision both observation and porocess error underlying the dynamics. Often, we don't do that, and assume either only observation error or process error. 

Here we explore the consequences of process and observation error. This is inspired by @Bolker2008 (Ch. 11). Following this, we then move on to wax poetic about elephants in the rain and heat [@Chamaille-Jammes2008].

## Geometric growth with observation and process error

If we believe that a population of brown rats grows geometrically with $\lambda =1$, that means we predict that it doesn't change size. If we observe and sample this population we will sample it with error. We could describe the population, $N$, and our observations of it thus,
$$N_0 = a$$
$$N_{t+1} = \lambda N_t$$
$$N_{obs,t} \sim \mathrm{Normal}(N_t, \sigma^2_{obs})$$
where we say the observed $N$ at time $t$ is a Normally distributed random variable with a mean of the actual $N$ and a variance of $\sigma^2_{obs}$. Thus we estimate $N_t$ with uncertainty or observation error.

Now we model it.

```{r obs1}
# set lamba, number of time steps (nt), a vector to hold N, and N0
lambda <- 1
nt <- 100
N <- numeric(nt)
Nobs.o <- numeric(nt)
N0 = 100
N[1] <- N0

# decide on an amount of uncertainty in our estimate
# standard deviation

sd.obs <- 20

# now we project the population and model observation error
for (t in 1:(nt - 1)) {
  ## observation error
     Nobs.o[t] = rnorm(1, mean = N[t], sd = sd.obs)
     ## mechanistic projection
     N[t+1]=lambda*N[t]
}
# finish up adding error to the last observation
Nobs.o[nt] = rnorm(1, mean = N[nt])

## plot it
 qplot(1:nt, Nobs.o, geom=c("line", "point"))
```

Tons of density dependence, right? Nope, just observation error and an unchanging population size. Sometimes our estimate is high, and sometimes low.

### Process error
If we believe that a population of brown rats grows geometrically with $\lambda =1$, that means we predict that it doesn't change size. However, in a real population of brown rats (*Rattus norvegicus*), we know that $\lambda$ will vary, even if we predict a mean of $\lambda=1$. We also know that there are other mechanisms influencing $N$ in any given year such as migration, density-independent predation, and many others. We do not know what these are, but they are real.If we model this process and somehow sample this population perfectly with no uncertainty, we could describe the population, $N$, and our observations of it thus,
$$N_0 = a$$
$$N_{obs,t} = N_t$$
$$N_{t+1} \sim \mathrm{Normal}(\lambda N_t, \sigma^2_{proc})$$

where we say the actual $N$ at time $t+1$ is a Normally distributed random variable with a mean of $\lambda N_t$ and a variance of $\sigma^2_{proc}$. Thus we estimate the *process* with uncertainty or observation error, even while observing it perfectly.

Now we model it.
```{r}
# Pure process error (process variation)
# use the same standard deviation, and start anew
sd.proc=sd.obs
N = numeric(nt)
Nobs.p = numeric(nt)
N[1] = N0
for (t in 1:(nt - 1)) {
     N[t + 1] = rnorm(1, mean = lambda*N[t], sd = sd.proc)
 Nobs.p[t] = N[t] 
 }
Nobs.p[nt] = N[nt]

qplot(1:nt, Nobs.p, geom=c("line", "point"))
```

Now let's plot them all together.
```{r}
df <- data.frame(time = c(1:nt, 1:nt), Error=rep(c("observation", "process"), each=nt), 
                 N=c(Nobs.o, Nobs.p) )
ggplot(df, aes(time, N, colour=Error) ) + geom_line() + geom_point()
```
Decide why these plots exhibit such different dynamics. Ideally, you should take time to explain it in writing. You could also discuss it with someone to practice saying it out loud.

All real data sets have both observation and process error imply. If we can assume one of those processes is minor, we can catiously ignore it. 

## Thirsty, thirsty elephants

@Chamaille-Jammes2008 evaluated the role of temporal and spatial heterogeneity of resources on elephant populations in HwangeNational Park, Zimbabwe.
```{r hwange, out.width="80%", fig.cap="Hwange National Park is home to a very large African elephant population. Image courtesy of Google Maps."}
knitr::include_graphics("figs/hwange.png")
```

@Chamaille-Jammes2008 fitted a variety of deterministic and stochastic models to elephant abundance (Fig. \@ref(fig:Nobs)). 
```{r Nobs, fig.cap="Approximate elephant population dynamics from 1996 to 2001. (Data were estimated visually from the original publication).", out.wdith="75%"}
N.est <- c(14000, 19000, 23000, 24000, 29000, 31000, 35500, 22000, 31000, 
        22000, 27000, 32000, 29000, 34000, NA, 45000)

qplot(x=1986:2001, y=N.est, geom=c("line", "point"))
```
When we gaze upon Fig. \@ref(fig:Nobs), we now realize that these numbers are the result of (i) mechanistic processes that we hypothesis and make explicit, (ii) mechanistic processes that we presumably don't understand and certainly don't make explicit, and (iii) inaccuracies of estimating the treu number that was present at the time of the survey. Item (ii) is process error, and item (iii) is observation error.

In this section, we will model the elephant population using the model that @Chamaille-Jammes2008 found provided the best explanation of their population dynamics.

The model that that appear to be the best was the $\theta$-logistic model,
$$\frac{dN}{dt} = rN\left(1-\left(\frac{N}{K}\right)^\theta \right)$$
where $K = \alpha R$. Upon fitting the data, they also found that $\theta$ really didn't differ substantially from 1, and used a model where $\theta =1$. When $\theta=1$, what do we call this model? 

The authors found that carrying capacity, $K$, was a function of annual rainfall, $R$. They incorportated this insight into carry capacity by allowing $K = \alpha R$. This model is reflected in Fig. 2b. If we interpolate per capita growth rate back to $x=0$,  we can approximate $r$ as,
```{r}
rmax=.45
```

In their, like, totally weird, un-American notation, they tell us (p. 139) that $\alpha = 61\cdot280 \pm 5\cdot776$, or, about 61.
```{r}
alpha <- 61.280
```

Rainfall...what do we make of that? They don't provide, and I'm shocked ("shocked, I tell, you shocked!") that they don't provide these data in a data repository linked from an appendix. So, we are left simulating it. An examination of Fig. 4a suggests that annual rainfall, which we are symbolizing with $R$, might be approximated as a uniformly distributed random variable with a minimum of 300 and a maximum of 850\,mm\,y$^{-1}$. For their 16 years of data, we can simulate 16 years of rainfall,
```{r}
nt <- 16
rand.seed <- 1
{set.seed(rand.seed)
AR <- runif(nt, min=300, max=850)
}
```

In the proceeding sections, we simulate the dynamics of this elephant population under conditions of 

* no observation or process error
* obervation error only
* process error only, and
* both observation and process error.

You will need to run this in order, as we reuse parameters and variables.


**No process or observation error**
```{r}
# create a vector to hold "actual" population size
N = numeric(nt)
# create a vector to hold our observation of actual population size
Nobs = numeric(nt)
# start with lots of elephants (circa 1986)
N[1] = 14000
# simulate a population
for (t in 1:(nt - 1)) {
  N[t + 1] = N[t] + rmax*N[t]*(1 - N[t]/(alpha*AR[t])) 
  Nobs[t] = N[t] 
}
# make sure our last observation is there
Nobs[nt] = N[nt]
```
Plot what we created.
```{r}
# the dynamics (cf. Figs. 1, 3)
qplot(1:nt, Nobs, geom=c("line", "point"))

# Something akin to Fig. 2b
pcPGR <- log(Nobs[-1]/Nobs[-nt])
N.R <- Nobs[-nt]/AR[-nt]
qplot(x=N.R, y=pcPGR, geom="point", xlim=c(0, 120)) +
  geom_smooth(method="lm")

```
Let's fit a statistical model to that, and get the intercept and slope.
```{r}
(cfs0 <- coefficients( summary( fit <- lm(pcPGR ~ N.R) )))
```
We learn that the estimate of $r$ is a little different than than the true value. The carrying capacity is the $x$-intercept, which is $x=-b/m$, is easily estimated.
```{r}
# r (0.45)
(r1 <- as.numeric(coef(fit)[1]))
# x-int (61.28)
(K1 <- as.numeric( - coef(fit)[1] / coef(fit)[2]) )
```


**Observation error, but no process error**
```{r}
# standard.deviation will be a constant
SD <- 5000
{set.seed(rand.seed)
for (t in 1:(nt - 1)) {
  Nobs[t] = rnorm(1, mean=N[t] , sd=SD)
  N[t + 1] = N[t] + rmax*N[t]*(1 - N[t]/(alpha*AR[t])) 
}
Nobs[nt] = rnorm(1, mean=N[nt] , sd=SD)
}
```
Plot stuff
```{r}
# the dynamics (cf. Figs. 1, 3)
qplot(1:nt, Nobs, geom=c("line", "point"))

# Something akin to Fig. 2b
pcPGR <- log(Nobs[-1]/Nobs[-nt])
N.R <- Nobs[-nt]/AR[-nt]
qplot(x=N.R, y=pcPGR, geom="point", xlim=c(0, 120))  +
  geom_smooth(method="lm")
```
Let's fit a statistical model to that, and get the intercept ($r$) and the slope.
```{r}
(cfs.obs <- coefficients( summary( fit <- lm(pcPGR ~ N.R) )))
```

```{r}
(r2 <- as.numeric(coef(fit)[1]) )
# x-int, or K or alpha(R) (61.28)
(K2 <- as.numeric( - coef(fit)[1] / coef(fit)[2] ))
```


**Process error, but no observation error**
```{r}
# standard.deviation will be a proportion of N
{set.seed(rand.seed)
prop <- 0.1
for (t in 1:(nt - 1)) {
  nextN <- N[t] + rmax*N[t]*(1 - N[t]/(alpha*AR[t])) 
  N[t + 1] = rnorm(1, mean=nextN, sd=prop*N[t])
  Nobs[t] = N[t] 
}
Nobs[nt] = N[nt]
}
```
Plot stuff.
```{r}
qplot(x=1:nt, y=Nobs, geom=c("line", "point"))

# Something akin to Fig. 2b
pcPGR <- log(Nobs[-1]/Nobs[-nt])
N.R <- Nobs[-nt]/AR[-nt]
qplot(x=N.R, y=pcPGR, geom="point", xlim=c(0, 120)) +
  geom_smooth(method="lm")
```
Let's fit a statistical model to that, and get the intercept, $r$, and the slope.
```{r}
(cfs.proc <- coefficients( summary( fit <- lm(pcPGR ~ N.R) ) ))
```

```{r}
(r3 <- as.numeric(coef(fit)[1]))
# x-int, alpha R (61.28)
(K3 <- as.numeric( - coef(fit)[1] / coef(fit)[2]))
```

**Both observation and process error**
```{r}
{set.seed(rand.seed)
for (t in 1:(nt - 1)) {
  Nobs[t] = rnorm(1, mean=N[t] , sd=SD)
  nextN <- N[t] + rmax*N[t]*(1 - N[t]/(alpha*AR[t])) 
  N[t + 1] = rnorm(1, mean=nextN, sd=prop*N[t])
}
Nobs[nt] = rnorm(1, mean=N[nt] , sd=SD)
}
```
Plot stuff.
```{r}
qplot(x=1:nt, y=Nobs, geom=c("line", "point"))

# Something akin to Fig. 2b
pcPGR <- log(Nobs[-1]/Nobs[-nt])
N.R <- Nobs[-nt]/AR[-nt]
qplot(x=N.R, y=pcPGR, geom="point", xlim=c(0, 120)) + 
  geom_smooth(method="lm")
```
Let's fit a statistical model to that, and get the intercept and slope.
```{r}
(cfs.both <- coefficients( summary( fit <- lm(pcPGR ~ N.R) ) ))
```

```{r}
(r4 <- as.numeric(coef(fit)[1]))
# x-int (61.28)
(K4 <- as.numeric(- coef(fit)[1] / coef(fit)[2]))
```

## Take home
```{r}
list(No.error=cfs0, Obs.error=cfs.obs, Proc.error=cfs.proc, Both.error=cfs.both)

c(r1,r2,r3,r4)
c(K1,K2,K3,K4)
```
What do we learn from this rigmarole? How do we now think about

* time series data?
* observation and process error?

It is very difficult to understand the underlying processes driving population dynamics. We need quantitative hypotheses about what is happening, and methods that can estimate the pieces of our models. These methods have to take into account obervation error and process error for the processes we are not testing.

```{r}

errorEval <- function(obs.e = 5000, proc.e = 0.1, nt=16, rmax=0.45, alpha=61.28, N1=14000, seed=2) {
{set.seed(seed)
AR <- runif(nt, min=300, max=850)
ann.rain <- seq(300, 850, length.out=nt)
AR <- spec_mimic(ann.rain, gamma=0)
}
# create a vector to hold "actual" population size
N = numeric(nt)
# create a vector to hold our observation of actual population size
Nobs = numeric(nt)
# start with lots of elephants (circa 1986)
N[1] = N1
t=1
for (t in 1:(nt - 1)) {
  Nobs[t] = rnorm(1, mean=N[t] , sd=obs.e)
  nextN <- N[t] + rmax*N[t]*(1 - N[t]/(alpha*AR[t])) 
  N[t + 1] = max(0, rnorm(1, mean=nextN, sd=proc.e*N[t]) )
}
Nobs[nt] = rnorm(1, mean=N[nt] , sd=obs.e)

pcPGR <- log(Nobs[-1]/Nobs[-nt])
N.R <- Nobs[-nt]/AR[-nt]
cfs <- coefficients( summary( fit <- lm(pcPGR ~ N.R) ) )
obs.r <-  as.numeric( cfs[1] )
obs.alpha <- as.numeric( - cfs[1] / cfs[2] )
return( c(obs.r=obs.r, obs.alpha=obs.alpha) )
}
errorEval()
```

```{r}
Obs.e <- 10^seq(0, 4, by=.1) 
Proc.e <- seq(0,.3, by=0.05)
df <- expand.grid(Obs.e=Obs.e, Proc.e=Proc.e)
df$r.est <- numeric(nrow(df))
df$K.est <- numeric(nrow(df))
for(i in 1:nrow(df) ){
  df[i,3:4] <- errorEval(obs.e=df$Obs.e[i], proc.e=df$Proc.e[i])
}
ggplot(df, aes(Obs.e, K.est, colour=as.factor(Proc.e))) + geom_line()
```



<!--chapter:end:dd-exercise1.Rmd-->





\subsection{Modeling data from \index{Bombay|see{Mumbai}}Bombay}
Here we try our hand at fitting the SIR model to some data. Kermack and McCormick \cite{Kermack:1927fk} provided data on the number of \index{plague}plague deaths per week in Bombay\footnote{Bombay is the coastal city now known as \index{Mumbai}Mumbai, and is the capital of Maharashtra; it is one of the largest cities in the world.} in 1905--06. We first enter them and look at them\footnote{Data provided kindly by S.P. Ellner}.
<<Ross, fig=TRUE>>=
data(ross)
plot(CumulativeDeaths ~ Week, data=ross)

@
\begin{figure}[ht]
  \centering
  \includegraphics[width=.6\textwidth]{FittedBombay}
  \caption{Cumulative deaths for plague, in Bombay, India, 1905--1906 (raw data and fitted model, as described in this section).}
  \label{fig:sir2}
\end{figure}

As with most such enterprises, we wish we knew far more than we do about the which depends on fleas, rodents, humans, and \emph{Yersinia pestis} on which the dynamics depend. To squeeze this real-life scenario  into a model with a small number of parameters requires a few assumptions.

% We begin by assuming, and Kermack and Mckendrick did, that mortality from bubonic plague, in Bombay, in 1905, was quite high, close to 90\%. Next, we realize that the only data we have is deaths. Given, however, that the mortality is very high, we might approximate the number of infections by saying that $Deaths = 0.9 \times Infections$. 

A good starting place is a simple SIR model for a population of constant size (eq. \ref{eq:SIR}) \cite{Ellner:2006qe, Kermack:1927fk}.

@ 
\subsubsection{Optimization}
We next want to let \R~find the most accurate estimates of our model parameters $\beta,\,\gamma$. The best and most accessible reference for this is Bolker \cite{Bolker:2008rr}. Please read the Appendix \ref{sec:numer-optim} for more explanation regarding optimization and objective functions.

Now we create the objective function. An objective function compares a model to data, and calculates a measure of fit (e.g., residual sum of squares, likelihood). Our objective function will calculate the likelihood\footnote{\emph{Likelihood} is the probability of data, given a particular model and its parameters.} of particular values for SIR model parameters. These parameters include  $\gamma$ and $\beta$ of the SIR model. The parameters will also include two other unknowns, (i) $N$, total relevant population size of Bombay at the time (1905--1906), and (ii) $I_0$, initial size of the infected population at our $t=0$. A survey of easily accessed census data suggests the population at the time was in the neighborhood of $\sim 10^6$ individuals. We also might assume that $I_0$ would be a very small number, perhaps $\sim 1$ in principle.

Some details about our objective function:
\begin{compactenum}
  \item Arguments are transformed parameters (this allows the optimizer to work on the logit\footnote{A logit is the transformation of a proportion which will linearize the logistic curve, logit (p) = $\log ( p/(1-p) )$.} and log scales).
    \item Transformed parameters are backtransformed to the scale of the model.
      \item Parameters are used to integrate the ODEs using \texttt{ode}, retaining only the resistant (i.e. dead) group (the fourth column of the output); this provides the \emph{predicted values} given the parameters and the time from onset, and the standard deviation of the residuals around the predicted values.
          \item  It returns the negative log-likelihood of the data, given the parameter values.
\end{compactenum}

Here is our objective function. Note that its last value is the negative sum of the log-probabilities of the data (given a particular realization of the model).
<<>>=
sirLL=function(logit.B, logit.g, log.N, log.I0) {
  parms <- c(B=plogis(logit.B), g=plogis(logit.g));
  x0 <- c(S=exp(log.N), I=exp(log.I0), R=0)
  Rs <- ode(y=x0, ross$Week, SIR, parms, hmax=.01)[,4]
  SD <- sqrt(sum( (ross$CumulativeDeaths -Rs)^2)/length(ross$Week) )
   -sum(dnorm(ross$CumulativeDeaths, mean=Rs, sd=SD, log=TRUE))
}
@ 
We then use this function, \texttt{sirLL}, to find the likelihood of the best parameters. The \texttt{mle2} function in the \texttt{bbmle} library\footnote{You will need to load the \texttt{bbmle} package from a convenient mirror, unless someone has already done this for the computer you are using. See the Appendix for details about packages (\ref{sec:where-r}) and optimization in \R~(\ref{sec:numer-optim}).} will minimize the negative log-likelihood generated by \texttt{sirLL}, and return values for the parameters of interest.

We will use a robust, but relatively slow method called Nelder-Mead (it is the default). We supply \texttt{mle2} with the objective function and a list of initial parameter values. This can take a few minutes.
<<echo=false, results=hide>>=
load("rossfits.Rdata")
@ 
<<eval=false>>=
require(bbmle)
fit <- mle2(sirLL, start=list(logit.B=qlogis(1e-5), logit.g=qlogis(.2), 
                     log.N=log(1e6), log.I0=log(1) ), method="Nelder-Mead")
<<sumfit>>=
summary(fit)
 
@ 
This gets us some parameter estimates, but subsequent attempts to actually get confidence intervals failed. This occurs frequently when we ask the computer to estimate too many, often correlated, parameters for a given data set. Therefore, we have to make assumptions regarding selected parameters. Let us assume for the time being that the two variable estimates are correct, that the population size of the vulnerable population was approximately exp(\Sexpr{round(coef(fit)[3],1)}) and the number of infections at the onset of the outbreak was \Sexpr{round(coef(fit)[4],1)}. We will hold these constant and ask \R~to refit the model, using the default method.
<<eval=false>>=
fit2 <- mle2(sirLL, start=as.list(coef(fit)),
            fixed=list(log.N=coef(fit)[3], log.I0=coef(fit)[4]), method="Nelder-Mead")
<<sumfit2>>=
summary(fit2)
@ 
Next we want to find confidence intervals for $\beta$ and $\gamma$. This can take \emph{several} minutes, but results in a likelihood profile for these parameters, which show the confidence regions for these parameters (Fig. \ref{fig:profile}). 
<<eval=false>>=
pr2 <- profile(fit2)
<<profileSIR, fig=TRUE, width=9, height=6>>=
par(mar=c(5,4,4,1))
plot(pr2)

<<eval=false,echo=false, results=hide>>=
save(fit,fit2,pr2, file="rossfits.Rdata")
@ 
We see that the confidence intervals for the transformed variables provide estimates of our confidence in these parameters.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{profileSIR}
  \caption{Likelihood profile plots, indicating confidence intervals on transformed SIR model parameters.}
  \label{fig:profile}
\end{figure}

Last we get to plot our curve with the data. We first backtransform the coefficients of the objective function.
<<>>=
p <- as.numeric(c(plogis(coef(fit2)[1:2]),
       exp(coef(fit2)[3:4])) ); p
@ 
We then get ready to integrate the disease dynamics over this time period.
<<>>=
inits <- c(S = p[3], I=p[4], R=0)
params <- c(B=p[1], g=p[2])
SIR.Bombay <- data.frame( ode(inits, ross$Week, SIR, params) )
@ 
Last, we plot the model and the data (Fig. \ref{fig:sir2}).
<<FittedBombay, fig=TRUE>>=
matplot(SIR.Bombay[,1], SIR.Bombay[,3:4], type='l', col=1)
points(ross$Week, ross$CumulativeDeaths)
legend('topleft', c("I", "R"), lty=1:2, bty='n')
@
So, what does this mean (Fig. \ref{fig:sir2})? We might check what these values mean, against what we know about the reality. Our model predicts that logit of $\gamma$ was a confidence interval,
<<>>=
(CIs <- confint(pr2))
@ 
This corresponds to a confidence interval for $\gamma$ of 
<<>>=
(gs <- as.numeric(plogis(CIs[2,]) ))
@ 
Recall that the duration of the disease in the host is $1/\gamma$. Therefore, our model predicts a confidence interval for the duration (in days) of 
<<>>=
7*1/gs
@ 
Thus, based on this analysis, the duration of the disease is right around 9.5 days.  This seems to agree with what we know about the biology of the Bubonic plague. Its duration, in a human host, is typically thought to last 4--10 days. 



% Problems or Exercises should be sorted chapterwise
\section*{Problems}
\addcontentsline{toc}{section}{Problems}
%
% Use the following environment.
% Don't forget to label each problem;
% the label is needed for the solutions' environment
% \begin{prob}
% \label{prob1}
% The problem\footnote{Footnote} is described here. The
% problem is described here. The problem is described here.
% \end{prob}

% \begin{prob}
% \label{prob1}
% \textbf{Problem Heading}\\
% (a) The first part of the problem is described here.\\
% (b) The second part of the problem is described here.
% \end{prob}
\begin{prob}
\textbf{Lotka--Volterra Predator--prey Model}\\
(a) Write down the two species Lotka--Volterra predator--prey model.\\
(b) Describe how Fig. \ref{fig:LVDyn} illustrates neutral oscillatory dynamics.\\
(c) What are the units of the predator--prey model coefficients $b$, $a$, $e$, and $s$? How do we interpret them?\\
\end{prob}

\begin{prob}
\textbf{Rosenzweig-MacArthur Predator--prey Model}\\
(a) Write down the two species Rosenzweig-MacArthur predator--prey model.\\
(b) How do we interpret $b$, $K$, $w$, $D$, $e$ and $s$? What are their units?\\
(c) What is the value of the functional response when $H = D$? Explain how this result provides the meaning behind the name we use for $D$, the \emph{half saturation} constant.\\
(d) For each point A--D in Fig. \ref{fig:RMiso}, determine whether the growth rate for the predator and the herbivore are zero, positive, or negative.\\
(e) In what way is the Rosenzweig-MacArthur
predator isocline  (Fig. \ref{fig:RMiso}) similar to the
Lotka--Volterra model? It also differs from the Lotka--Volterra
isocline--explain the ecological interpretation of $D$ in the type II
functional response and its consequence for this isocline.\\
(f) Explain the interpretation of real and imaginary
parts of the eigenvalues for this paramterization of the
Rosenzweig-MacArthur predator--prey model.\\
(g)  In what ways does Fig. \ref{PPP}
match the interpretation of the eigenanalysis of this model?\\
(h) Examine the prey isoclines in Fig. \ref{PPP}. How you can tell
what the carrying capacities of the prey are?\\
(i) What do the above eigenanalyses tell us about how the stability of the predator--prey interaction varies with the carrying capacity of the prey?\\
(j) Consult Fig. \ref{PPP}. What is the
relation between the carrying capacity of the prey and the magnitude
of the oscillations? What is the relation between the carrying
capacity of the prey and the minimum population size? What does this interpretation imply about natural ecosystems?
\end{prob}

\begin{prob}
  \textbf{Effects of dispersion on host--parasitoid dynamics}\\
  (a) Demonstrate the effects of aggregation on host--parasitoid dynamics. Specifically, vary the magnitude of $k$ to find the effects on stability.\\
(b) Demonstrate the effects of $a$ on stability.\\
(c) Demonstrate the effects of $R$ on stability.
\end{prob}

\begin{prob}
  \textbf{Effects of age at onset and disease duration on outbreak periodicity}\\
  (a) Create three simulations showing how diseases of different durations influence the periodicity of the outbreaks.\\
(b) Create three simulations showing how the age at onset for different diseases influence the periodicity of the outbreaks.\\
(c) Consider which factor is more important in influencing outbreak interval. How do you measure the interval? What criteria would you use to determine ``importance''? How do the realities of age and duration influence your selection of criteria? Write a short essay that asserts a thesis, and then provides support based on this exercise.
\end{prob}


<!--chapter:end:enemyVictimstufftoadd.Rmd-->

---
title: "igraph"
output: html_document
---

```{r}
library(igraph)
```

### Background
In the early and mid-1970's, Robert May and others
 demonstrated that important predictions could be made with relatively simple Lotka--Volterra models [@May1973ab], and this work still comprises an important compendium of lessons for ecologists today [@May:2006lr]. May used simple Lotka--Volterra models to show that increasing the number of species in a food web tended to make the food web less stable [@May1973ab; @May1972]. In species-rich webs, species were more likely to become extinct. Further, he showed that the more connections there were among species in the web (higher connectance), and the stronger those connections (higher interaction strength), the *less* stable the web. At the time, this ran counter to a prevailing sentiment that more diverse ecosystems were more stable, and led to heated discussion.
```{r PLABEviz, echo=FALSE, fig.cap="Three of the six webs investigated by Pimm and Lawton (1977). Left to right, these correspond to Pimm and Lawton (1977) Figs. 1A, E, and B. All basal species exhibit negative density dependence.", out.width=c("15%","55%","25%"), fig.show='hold', fig.ncol=3, fig.height=10, fig.width=5}
resc <- FALSE
gA <- graph(c(1,1, 1,2,2,1, 2,3,3,2, 3,4,4,3))
signs <- c("-", '+',"-", '+',"-", '+',"-")
plot(gA, layout=matrix(c(.5,.5,.5,.5, 0,.33,.67,1), nc=2), edge.curved=.5,      edge.label=signs, edge.label.font=1,  edge.label.cex=1, 
     vertex.size=10, edge.loop.angle=pi/4,
     margin=c(0,0,0,0), rescale=resc)

gE <- graph(c(1,1, 2,2, 3,3, 1,4,4,1, 2,4,4,2, 3,4,4,3))
signs <- c("-","-","-", '+',"-", '+',"-", '+',"-")
plot(gE, layout=matrix(c(0,.5, 1,.5, 0,0,0,.5), nc=2), edge.curved=.5,      edge.label=signs, edge.label.font=1,  edge.label.cex=1, 
     vertex.size=10, edge.loop.angle=pi/4,
     margin=c(0,0,0,0), rescale=resc)

gB <- graph(c(1,1, 1,2,2,1, 
              2,3,3,2, 
              3,4,4,3, 
              2,4,4,2))

signs <- c("-", '+',"-", 
           '+',"-", 
           '+',"-", 
           '+',"-")
plot(gB, layout=matrix(c(.5,.5,.5,.4,  0,.33,.67,1), nc=2), 
     edge.curved=.5,edge.label=signs, edge.label.font=1,  edge.label.cex=1, 
     vertex.size=10, edge.loop.angle=pi/4,
     margin=c(0,0,0,0), rescale=resc)
```

<!--chapter:end:igraph.Rmd-->

# Life History Theory {#lht}

Life history theory explains variation among individuals, populations, and species in traits related to growth, survival, and reproduction. Such traits include, the number of seed per flower, the average size of an individual at birth, generation time, life span and others. It assumes that natural selection finds shapes these traits in myriad ways resulting in wide variation in these traits, and tradeoffs among these traits.

Natural selection can be thought of as an optimizing process: phenotypes diversify, winners replicate and losers don't, and the phenotypes of winners tend to get passed on to the replicants. We therefore often assume, as did Dr. Pangloss, that the species that exist now are the best of all possible species, that is, they are of optimal design. And like Dr. Pangloss, we would be woefully mistaken if we stopped there. 

<!--chapter:end:life-history.Rmd-->

# Mutualisms

## Background

Mutualisms occur in a wide variety of different species. Mutualisms form the
foundations of ecosystems including coral reefs (coral and
zooxanthellae), grasslands, and forests (plants and mycorrhizal
fungi, pollinators, and dispersers). 

The natural history of mutualisms are wildly diverse and complex, and 
and @Boucherecologymutualism1982 lists types of mutualisms:

* Energetic
* Nutritional
* Defense
* Transport
* Obligate \emph{vs.} facultative
* Direct \emph{vs.} indirect
* Symbiotic \emph{vs.} independent

 Perhaps the most common of these is
*indirect mutualism* (Fig. \ref{f:mut3}). In this case, negative direct interactions
sum to yield net positive indirect interactions, as in the classic
case where ``the enemy of my enemy is my friend.'' For instance,
consider a three plant species that all compete for the same limiting
resource. When species A suppresses species B, it is indirectly
helping species C [@Miller1994]. 

## Lotka-Volterra mutualism
Lotka-Volterra mutualism is very straightforward, following the samed template as competition. All that we have to do is change the sign associated with the other species. As usual, $\alpha_{ij}$ is the per capita effect of species $j$ on species $i$.

\begin{align}
\frac{dN_1}{dt} &= r_1N_1\left( 1 - \alpha_{11}N_1 + \alpha_{12}N_2\right)\\
\frac{dN_2}{dt} &= r_2N_2 \left(1 + \alpha_{21}N_1 - \alpha_{22}N_2\right)
\end{align}

Here we will focus on *direct mutualisms*, and in this chapter, we'll focus on two approaches to models, Lotka-Volterra models and consumer-resource models.

In the above equations we see the sign of the intraspecific terms $\alpha_{ii}N_i$ is negative, while the sign of the interspecific terms $\alpha_{ij}N_j$.

The zero net growth isoclines are found the same way we found those for interspecific competition, by solving $\frac{dN_1}{dt}$ (for zero). We will graph these, so we solve each in terms of species 1.

The ZNGI for species 2 is
\begin{align*}
0 &= 1 + \alpha_{21}N_1 - \alpha_{22}N_2\\
N_2 &= 1/\alpha_{22} + \alpha_{21}N_1
\begin{align*}
Thus when $N_1=0$, $N_2 = 1/\alpha_22} = K_2$.

The ZNGI for species 1, in terms of species 1, is
\begin{align*}
0 &= 1 - \alpha_{11}N_1 + \alpha_{12}N_2\\
N_2 &= \alpha_{11}/\alpha_{12}N_1 - 1/\alpha_{12}
\begin{align*}

```{r}
alpha11 <- alpha22 <- 0.2
alpha12 <- alpha21 <- 0.1

plot(0:5, 0:5, type="n")
abline(1/alpha22, alpha21)
abline(1/alpha12, alpha11/alpha12, lty=2)

```

## Consumer-resource mutualism
Sometimes mutualists share resources, as with mycorrhizal
associations, where species share a resource or a product that they
can acquire or manufacture in surplus and trade it for a resource or product that is
limiting. This is a bi-drectional mutualism (Fig. \ref{f:mut3})
because the flow of \emph{resources} are bidirectional: both species provide a resource to the other [@Holland2010]. 
```{r figs, echo=FALSE}
# a digraph is a directed graph
# the model is written in the "dot" language
bimut <-
"digraph{
  rankdir = LR;
node[shape=box];
// oval is the default node shape
  M1 -> M2[label='R'];
M2 -> M1[label='R'];
}"

#grViz(bimut)

umut <-
"digraph{
  rankdir = LR;
node[shape=box];
// oval is the default node shape
  M1 -> M2[label='R'];
M2 -> M1[label='S', style=dashed]
}"

imut <-
"digraph{
  rankdir = TB;
node[shape=box];
{rank=max; R1; R2;}
{rank=min; M1; M2;}
// oval is the default node shape
  M1 -> M2[label='R'];
M2 -> M1[label='S', style=dashed]
R1 -> M1; M1 -> R1[arrowhead='odot'];
R2 -> M2; M2 -> R2[arrowhead='odot'];
R1 -> R2[arrowhead='odot']; R2 -> R1[arrowhead='odot'];
}"

grViz(imut)

grViz(bimut) %>%
  export_svg %>% charToRaw %>% rsvg_pdf("bimut.pdf")

grViz(umut) %>%
  export_svg %>% charToRaw %>% rsvg_pdf("umut.pdf")

grViz(imut) %>%
  export_svg %>% charToRaw %>% rsvg_pdf("imut.pdf")
```

\begin{figure}
\begin{center}
\hfill
\includegraphics[width=.4\linewidth]{bimut} 
\hfill
\includegraphics[width=.4\linewidth]{umut}
\hfill 
\\
\includegraphics[width=.4\linewidth]{imut}
\caption{Bidirection, unidirectional, and indirect mutualisms. Normal
  arrowhead is a positive effect, whereas an open dot arrowhead is a
  negative effect. A dashed line indicates a service, and a solid line
  indicates a consumeable resource. \label{fig:mut3}}
\end{center}
\end{figure}

In other cases, mutualists may share services, as with moray
eel-grouper foraging. In this particular case, grouper and the giant
moray eel have different foraging strategies, with the eel going into
reef crevices and the grouper waiting outside to pick off prey that
escape the eel [@Bshary2006]. Sometimes these mutualisms are
called *by-product* mutualisms, in cases where there is no
evidence of coordinated behavior, but nonetheless a mutualism arises via the independent activities of each species.

In the following, we explore the type of mutualism in which one
species provides a resource and the other species provides a
service (Fig. \ref{fig:mut3}). This is a uni-directional mutualism because
the flow of \emph{resources} is only unidrectional, from one species to the other [@Holland2010].

A good example of a uni-directional of mutualism is seed dispersal via
a frugivore that disperses seeds. A plant species provides a 
fruit that is nutritious for an animal. A primary benefit for the
plant is that seeds are carried away from the parent plant. This may
help seeds escape enemies that accumulate near parent plants
(Connell-Janzen hyptothesis). In addition, it provides a bet hedging
opportunity wherein seeds are dispersed to a variety of habitats any
one of which may be better for survival than the current location. It
is even possible that dispersal is *directed* to preferred
habitat that is consistently better than under the parent plant due to
better resource levels or environmental conditions
[@Wenny1998]. The animal benefits by consuming the fleshy part of
the fruit surrounding the seed. An interesting case arises when seed
predators collect seeds but do not kill them before having buried,
cached, or moved seeds. 

<!--chapter:end:mutualisms.Rmd-->

# Mutualisms

## Background

Mutualisms occur in a wide variety of different species. Mutualisms form the
foundations of ecosystems including coral reefs (coral and zooxanthellae), grasslands, and forests (plants and mycorrhizal fungi, pollinators, and dispersers). The natural history of mutualisms are wildly diverse and complex, and 
and @Boucherecologymutualism1982 list types of mutualisms:
  
* Energetic
* Nutritional
* Defense
* Transport
* Obligate \emph{vs.} facultative
* Direct \emph{vs.} indirect
* Symbiotic \emph{vs.} independent

One of the most common of these is
*indirect mutualism* (Fig. \@ref(fig:mut3)). In this case, a series of negative direct interactions can yield net positive indirect interactions, as in the classic
case where ``the enemy of my enemy is my friend.'' For instance,
consider a three plant species that all compete for the same limiting
resource. When species A suppresses species B, it is indirectly
helping species C [@Miller1994]. 

Here we will focus on *direct mutualisms*, and in this chapter, we'll focus on two approaches to models, Lotka-Volterra models and consumer-resource models.

## Lotka-Volterra mutualism
Lotka-Volterra mutualism is very straightforward, following the same template as competition. All that we have to do is change the sign associated with the other species. As usual, $\alpha_{ij}$ is the per capita effect of species $j$ on species $i$.

\begin{align*}
\frac{dN_1}{dt} &= r_1N_1\left( 1 - \alpha_{11}N_1 + \alpha_{12}N_2\right)\\
\frac{dN_2}{dt} &= r_2N_2 \left(1 + \alpha_{21}N_1 - \alpha_{22}N_2\right)
\end{align*}

In the above equations we see the sign of the intraspecific terms $\alpha_{ii}N_i$ is negative, while the sign of the interspecific terms $\alpha_{ij}N_j$.

The zero net growth isoclines are found the same way we found those for interspecific competition, by solving $dN_1/dt$ (for zero). We will graph these, so we solve each in terms of species 1.
\begin{align*}
0 &= 1 + \alpha_{21}N_1 - \alpha_{22}N_2\\
N_2 &= 1/\alpha_{22} + \alpha_{21}\alpha_{11}N_1
\end{align*}
\begin{align*}
0 &= 1 - \alpha_{11}N_1 + \alpha_{12}N_2\\
N_2 &=\alpha_{11}\alpha_{12}N_1 - 1/\alpha_{12}
\end{align*}

Let's use this to describe a facultative mutualism, that is, one in which neither species needs the other to persist, but in which they each do better. What we mean by that is that let $ r>0$. For now, we will also stipulate that the mutualism provides only a modest benefit, and so $\alpha_{ii} > \alpha_{ij}$. The dynamics are stable (Fig. \@ref(fig:LVmut))


```{r fig=TRUE, fig.width=2.5, fig.height=2.5, echo=FALSE, eval=FALSE}
alpha11 <- alpha22 <- 0.2
alpha12 <- alpha21 <- 0.1

zngi.m <- ggplot(data.frame(x=c(0,15)), aes(x=x)) +
  stat_function(fun=function(x) {1/alpha22 + alpha21*x}) +
  stat_function(fun=function(x) {-1/alpha12 + alpha11/alpha12*x}, linetype=2) +
  lims(y=c(0,15)) + labs(y="N2", x="N1") 

df2 <- data.frame(x=c(3,13,3,13), y=c(3,3,9,10), 
                  xend=c(3,13,3,13), yend=c(4,5,6,7))
df2 <- data.frame(x=c(3,9), y=c(3,9), xend=c(3, 9), yend=c(4.5, 7.5) )

zngi.m + geom_segment(data=df2, aes(x=x, y=y, xend=xend, yend=yend),  
              arrow = arrow(length = unit(.5, units="cm") ) )

```

To describe this popuation, we will paramterize an ODE model. Our parameters meet our stipulations, above.
```{r}
parameters <- c(alpha11 = .2, alpha22 = .2, alpha12 = .1, alpha21=.1,
                r1=.01, r2=.01)
```

Here we write an ODE model to help anaylze dynamics.
```{r echo=TRUE}
mutualism.LV <- function(t, y, params){
  n1 <- y[1]; n2 <- y[2]
  with(as.list(params), {
    dn1.dt = r1*n1*(1 - alpha11*n1 + alpha12*n2)
    dn2.dt = r2*n2*(1 + alpha21*n1 - alpha22*n2)
    return(list( c(dn1.dt, dn2.dt) ) )
  })
}
```
```{r results='hide', include=FALSE}
t <- 0:100
n2star <- with(as.list(parameters), {(alpha22+alpha21)/(alpha22*alpha11-alpha12*alpha21)})
out <- ode(y=c(11,11), time=t, mutualism.LV, parameters)
#plot(out)
```

The Lotka-Volterra approach fails if we try to describe two realistic scenarios. First,
we cannot describe an obligate mutualism. An obligate mutualism is one in which growth rate is negative in the absence of the mutualism. The way to describe that in Lotka-Volterra models is to set $r < 0$. Second, if the benefit, is greater than the negative intraspecific effect, we get an uncontrolled positive feedback loop.

```{r LVmut, echo=FALSE, fig=TRUE, fig.show = 'hold', fig.cap="Only weak facultative mutualisms are stable.", fig.width=3, fig.height=3.5, out.width='33%'}
#source("nullclines2.R")
umut.flowfield <-  flowField(mutualism.LV, xlim = c(0, 15), ylim = c(0,15),
                                 parameters = parameters, points = 11, add = FALSE, 
                             ylab="N2", xlab="N1", 
main=(-alpha[ii] >  alpha[ij]))

umut.nullclines <- nullclines(mutualism.LV, xlim = c(0, 15), ylim = c(0,15), parameters = parameters, add.legend = FALSE)

p.o <- c(alpha11 = .2, alpha22 = .2, alpha12 = .1, alpha21=.1,
                r1 = -0.01, r2 = -0.01)
umut.flowfield <-  flowField(mutualism.LV, xlim = c(0, 15), ylim = c(0,15),
                                 parameters = p.o, points = 11, add = FALSE, 
                             ylab="N2", xlab="N1", main= (italic(r) < 0))

umut.nullclines <- nullclines(mutualism.LV, xlim = c(0, 15), ylim = c(0,15), 
                              parameters = p.o, add.legend = FALSE)

p.p <- c(alpha11 = .2, alpha22 = .2, alpha12 = .3, alpha21=.3,
                r1 = 0.1, r2 = 0.1)
umut.flowfield <-  flowField(mutualism.LV, xlim = c(0, 15), ylim = c(0,15),
                                 parameters = p.p, points = 11, add = FALSE, 
                             ylab="N2", xlab="N1", 
                             main=(abs(alpha[ii])<alpha[ij]))
umut.nullclines <- nullclines(mutualism.LV, xlim = c(0, 15), ylim = c(0,15), 
                              parameters = p.p,  add.legend = FALSE)

```

## Consumer-resource mutualism

In this section, we will model mutualisms using a framework that, unlike our Lotka-Volterra approach, was inteneded specifically describe resource consumption [@macarthur1972Geographical]. 

Sometimes mutualists share resources, as with mycorrhizal
associations, where species share a resource or a product that they
can acquire or manufacture in surplus and trade it for a resource or product that is
limiting. This is a bi-drectional mutualism (Fig. \ref{fig:mut3})
because the flow of \emph{resources} are bidirectional: both species provide a resource to the other [@Holland2010]. 
```{r figs, echo=FALSE}
# a digraph is a directed graph
# the model is written in the "dot" language
bimut <-
"digraph{
  rankdir = LR;
node[shape=box];
// oval is the default node shape
  M1 -> M2[label='R'];
M2 -> M1[label='R'];
}"

#grViz(bimut)

umut <-
"digraph{
  rankdir = LR;
node[shape=box];
// oval is the default node shape
  M1 -> M2[label='R'];
M2 -> M1[label='S', style=dashed]
}"

imut <-
"digraph{
  rankdir = TB;
node[shape=box];
{rank=max; R1; R2;}
{rank=min; M1; M2;}
// oval is the default node shape
  M1 -> M2[label='R'];
M2 -> M1[label='S', style=dashed]
R1 -> M1; M1 -> R1[arrowhead='odot'];
R2 -> M2; M2 -> R2[arrowhead='odot'];
R1 -> R2[arrowhead='odot']; R2 -> R1[arrowhead='odot'];
}"

# grViz(imut)

grViz(bimut) %>%
  export_svg %>% charToRaw %>% rsvg_pdf("bimut.pdf")

grViz(umut) %>%
  export_svg %>% charToRaw %>% rsvg_pdf("umut.pdf")

grViz(imut) %>%
  export_svg %>% charToRaw %>% rsvg_pdf("imut.pdf")
```

\begin{figure}
\begin{center}
\hfill
\includegraphics[width=.4\linewidth]{bimut} 
\hfill
\includegraphics[width=.4\linewidth]{umut}
\hfill 
\\
\includegraphics[width=.4\linewidth]{imut}
\caption{Bidirection, unidirectional, and indirect mutualisms. Normal
  arrowhead is a positive effect, whereas an open dot arrowhead is a
  negative effect. A dashed line indicates a service, and a solid line
  indicates a consumeable resource. \label{fig:mut3}}
\end{center}
\end{figure}

In other cases, mutualists may share services, as with moray
eel-grouper foraging. In this particular case, grouper and the giant
moray eel have different foraging strategies, with the eel going into
reef crevices and the grouper waiting outside to pick off prey that
escape the eel [@Bshary2006]. Sometimes these mutualisms are
called *by-product* mutualisms, in cases where there is no
evidence of coordinated behavior, but nonetheless a mutualism arises via the independent activities of each species.

In the following, we explore the type of mutualism in which one
species provides a resource and the other species provides a
service (Fig. \ref{fig:mut3}). This is a uni-directional mutualism because
the flow of \emph{resources} is only unidrectional, from one species to the other [@Holland2010].

A good example of a uni-directional of mutualism is seed dispersal via
a frugivore that disperses seeds. A plant species provides a 
fruit that is nutritious for an animal. A primary benefit for the
plant is that seeds are carried away from the parent plant. This may
help seeds escape enemies that accumulate near parent plants
(Connell-Janzen hyptothesis). In addition, it provides a bet hedging
opportunity wherein seeds are dispersed to a variety of habitats any
one of which may be better for survival than the current location. It
is even possible that dispersal is *directed* to preferred
habitat that is consistently better than under the parent plant due to
better resource levels or environmental conditions
[@Wenny1998]. The animal benefits by consuming the fleshy part of
the fruit surrounding the seed. An interesting case arises when seed
predators collect seeds but do not kill them before having buried,
cached, or moved seeds. 

Another important example example of this type of mutualism is pollination. Pollinators (bats, insects, birds) receive resources (nectar, pollen) and provide services (pollination, gene flow). 

  
### A model of uni-directional mutualism

Here we describe a model of a facultative unidirectional mutualism that uses type II foraging model for both consumption of the resource and provision of the service. 

The growth equation for the plant species, $M_1$, includes logistic growth, and has terms for the benefit it receives from its mutualist frugivore and the cost paid to that frugivore.

$$\frac{dM_1}{dt} = r_1 M_1\left(1-d_1M_1\right)  - aM_2\left(\frac{M_1}{h_1 + M_1}\right) + sM_1\left(\frac{M_2}{h_2 + M_2}\right)$$

The plant species ($M_1$) grows logistically in the absence of fruit consumption or seed dispersal $r_1 M_1\left(1-d_1M_1\right)$. Consumption of fruits are governed by a type II functional response by the frugivore $aM_2\left(\frac{M_1}{h_1 + M_1}\right)$, so that the consumption rate of a single frugivore has an upper limit (Fig. \@ref(fig:funcResp)). Successful seed dispersal and establishment is also governed by a type II functional response. The success of a single seed reaches an upper limit as the number of frugivores continues to increase, $c_1M_1\left(\frac{M_2}{h_2 + M_2}\right)$ (Fig. \@ref(fig:funcResp)). 

```{r funcResp, echo=TRUE, fig=TRUE, fig.show = 'hold', fig.width=3, fig.height=3.5, out.width='50%', fig.cap="Fruit consumption by a single frugivore is governed by a type II functional response as fruit abundance increases. Successful seed dispersal and establishment is governed by a type II functional response as frugivores increase."}

ggplot(data.frame(x=c(0, 1)), aes(x=x) ) + 
  stat_function(fun=function(x) { 10*x/(1+10*x) }) +
  labs( x="Fruit abundance", y="Consumption Rate of a frugivore")


ggplot(data.frame(x=c(0, 1)), aes(x=x) ) + 
  stat_function(fun=function(x) { 10*x/(1+10*x) }) +
  labs( x="Frugivore abundance", y="Dispersal rate of a seed")

```

The growth equation for the frugivore species, $M_2$, includes logistic growth, and one term for the benefit gained from the plant.

$$\frac{dM_2}{dt} =  r_2M_2\left(1-d_2M_2\right) + eaM_2\left(\frac{M_1}{h_1+M_1}\right) $$
The animal seed disperser grows logistically in in the absence of consuming this particular plant species $r_2M_2\left(1-d_2M_2\right)$. The animal benefits from consuming the fruit which it attacks at the same rate as for $M_1$, $aM_2\left(\frac{M_1}{h_1+M_1}\right)$, but converts the fruit into new individuals with efficiency $e$.

* $c > s$, consumption > service = net predation
* $s > c$, service > consumption = net mutualism


The dynamics can result in *multiple basins of attraction* or *alternative stable states* (Fig. \@ref(fig:mba)).

```{r mba, echo=FALSE, fig=TRUE,  fig.width=5, fig.height=5.5, fig.cap="A resource-service mutualism may result in alternative stable states. If both species achieve moderate abundance, then both species increase toward a stable mutualism. However, if the frugivore is too abundant relative to the plant, then its consumption dominates the interaction and drives the equilibrium mutualism"} 
# The derivative 
## A two-species consumer-resource mutualism
## Holland and DeAngelis 2009, Ecology Letters Ecology Letters, 12: 1357–1366.
## The parameterization follows that in this article. It differs from those 
## in the above equations.

bimut <- function(t,y,parameters) {
  M1 <- y[1] # Plant
  M2 <- y[2] # Frugivore
  with( as.list( parameters ), {
    dM1 <- M1 * (r1 - d1*M1 + a12*M2/(h2 + M2) - B1*M2/(e1+M1) )
    dM2 <- M2 * (r2 - d2*M2 + a21*M1/(h1 + M1) - B2*M1/(e2+M2) )
    list( c( dM1, dM2 ))
  }
  )
}

##### 
### These parameters create a unidirectional, resource-service mutualism (B2 = 0)
## Translating, a12 = a above, and a21 = ea above, and B1 = s above
## therefore e = a21/a12

p.u <- list(r1 = 1, r2 = 1, d1=0.01, d2=0.01, 
            a12 = 0.4, a21 = 0.25, B1 = 0.3, B2 = 0, 
            h1=0.3, h2=0.3, e1=0.3, e2=0.3)

#####
# times series
t <- 0:100
y <- c(M1=50, M2=50)

#####
## Phase plane diagram
umut.flowfield <-  flowField(bimut, xlim = c(0, 170), ylim = c(0,220),
                             parameters = p.u, points = 21, add = FALSE, 
                             ylab="M2 - Frugivore", xlab="M1 - Garden tree species")

umut.nullclines <- nullclines(bimut, c(0, 170), ylim = c(0,220), parameters = p.u,
                                  add.legend = FALSE)

#####
# manifold for the interior saddle
isad <- findEquilibrium(bimut, y0 = c(40,150), parameters = p.u, system = "two.dim",
                              tol = 1e-16, max.iter = 50, h = 1e-06, plot.it = TRUE,
                              summary = TRUE)

# umut.manifolds <- drawManifolds(bimut, parameters = p.u, y0=as.numeric(isad$ystar),
 #                                  add.legend = FALSE)
#dev.print(pdf, "saddle.pdf")

## Include all equilibria and manifolds
y0 <- matrix(c(0, 25,125, 0, 100,
                  100,100,150, 0,0), nc=2)
eqs <- list(NULL)
for(i in 1:nrow(y0) ){
  eqs[[i]] <- findEquilibrium(bimut, y0 = y0[i,], parameters = p.u, system = "two.dim",
                              tol = 1e-16, max.iter = 50, h = 1e-06, plot.it = TRUE,
                              summary = TRUE)
}
```

Holland and DeAngelis (2009) vary $a_{21}$ between $(0.25,\ldots,\,0.4)$ to vary the relation from mutualism to predation. Try that now. Save a picture of your results.


\newpage
## Plant-soil feedbacks
Vascular plants form the basis of terrestrial 
ecosystems, and they live in intimate contact with the most diverse
group of organisms on Earth--soil microbes. Ecologists have
begun to focus on this intimate relationship, and Jim Bever [@BeverSoilcommunityfeedback2003] has been
one of ecologists helping lead the way (Fig. \@ref(fig:beverfig)). 


```{r beverfig, echo=FALSE, fig.cap="Plants and soil microbes can interact in negative or positive ways. Plants may grow more poorly with their own soil flora than with that of a competitor's."}
library(diagram)
bm <- matrix(c("a11", "ca","1", 0, "cb", "a22", "0", "nu", 
               "alphaA", "alphaB", 0,0,
               "betaA","betaB", 0,0), nc=4)

bm <- matrix(c( 
  "-1/K[A]", "-c[A]/K[A]",1, 0,
               "-c[B]/K[B]", "-1/K[B]",0, "nu",
               "alpha[A]", "alpha[B]", -1, 0,
               "beta[A]","beta[B]", 0, -1 
  ), nc=4)
name <- c(expression(N[A]), expression(N[B]), expression(S[A]), expression(S[B]))
par(mar=c(0,0,0,0))
plotmat(bm, pos=c(2,2), name=name)
```

Fig. \@ref(fig:theory) reflects the following Lotka-Volterra style model of plant species $N_A$ and $N_B$, and soil microbial floras $S_A$ and $S_B$:
\begin{equation}
  \frac{dN_A}{dt} = r_A N_A \left( 1 + \alpha_A S_A + \beta_A S_B -
    \frac{N_A + c_B N_B}{K_A}\right) 
\end{equation}
where the effects of microbes ($\alpha$, $\beta$) could be positive or negative (e.g. pathogenic). The plants each have their own carrying capacities ($K$), and negative effects on each other ($c$). 

The corresponding equation for the other plant species is 
begin{equation}
  \frac{dN_B}{dt} = r_B N_B \left( 1 + \alpha_B S_A + \beta_B S_B -
    \frac{c_A N_A + N_B}{K_B}\right).
\end{equation}

We often refer to the soil flora associated with a single plant species as its *home* flora, and the other as the *away* flora. Each microbial flora comprises many, many species and we aggregate the net effects. Bever (2003) further simplified his model by placing  constraints on soil microbes, so that $S_A + S_B=1$. This allows him to focus on the relations among microbes and plants. With these assumptions, he showed that the net effect of each flora as a single state variable for the home flora of each species:
$$\frac{S_A}{dt} = S_A(1-S_A) \left( \frac{N_A}{N_A + N_B} - \nu\frac{N_B}{N_A + N_B}\right)$$
and that $$S_B=1-S_A$$.

**Coexistence criteria**
Bever states that (2003, p. 467, bottom of the second column) out that the interactions are more stable if 
$$\alpha_A + \beta_B < \alpha_B + \beta_A $$
which means that the effects of the home floras own their home plants are less positive (or more negative) then on the other species. 

Bever also showed that for either plant species to increase when rare (i.e. coexist), that the interspecific competition has to be less than the benefits of the interactions of the soil floras, or
$$c_A c_B < \frac{(1+\alpha_B)(1+\beta_A)}{(1+\alpha_A)(1+\beta_B)}(\#eq:bevercrit)$$
Plant interspecific competition appears on the left. In the numerator on the right, we have the effect of each soil flora on their away plant species. In the denominator, we have the effects of each flora on the home plant species. Thus, the fraction on the right is the relative benefit of the soil flora. If there is no effect at all of the soil flora, then the coexistence criterion is what we would expect from Lotka-Volterra competition, where interspecific competition has to be less than one. For comparison, $c_A = \alpha_{21}^\prime$ from Chapter \@ref(Competition).

Now we will create an ODE function that will let us see the dynamics play out. We need only model one soil flora because $S_B = 1-S_A$.
```{r}
bever3 <- function(t,y,p){
  with(as.list(c(y,p)), {
    dNa <- ra * Na * (1+alphaA*SA + betaA*(1-SA) - (Na + cB*Nb)/Ka)
    dNb <- rb * Nb * (1+alphaB*SA + betaB*(1-SA) - (cA*Na + Nb)/Kb)
    dSA <- SA*(1-SA) * ( Na/(Na+Nb) - nu*Nb/(Na+Nb) )
    return(list(c(dNa, dNb, dSA), SB=1-SA))
  })
}
```

We will parameterize the model in a way that reveals the importance of the soil floras to coexistence, according to criteria stated above \@ref(eq:bevercrit). We will create a vector of parameters, and then test whether the inequality in \@ref(eq:bevercrit) is true. 
```{r}
p <- list(ra=0.7, rb=0.5, Ka = 5, Kb=6, cA=.99, cB=.99, 
       alphaA=-0.03, alphaB=0.1, betaA=0.1, betaB=-0.2, nu=0.8)

p <- list(ra=0.7, rb=0.5, Ka = 100, Kb=120, cA=.885, cB=.98, 
       alphaA=0, alphaB=0, betaA=0, betaB=0, nu=0.8)
with(p, {
  cA*cB < (1+alphaB)*(1+betaA) / ( (1+alphaA)*(1+betaB))
})
```

On this basis, we should see that the two 



We can also make a  phase plane plot, that give results analogous to those in Bever (2003, Fig. 4).

```{r pretty, echo=FALSE, fig=TRUE, width=10, height=10, include=TRUE, fig.cap="Dynamics of competing plant species, subject to self-limiting negative feedbacks with soil pathogens.", fig.show='hold'}
t <- seq(0,200, .1)
y0 <- c(Na=2, Nb=4, SA=0.5)
{outdf <- as.data.frame( ode(y=y0, times=t, fun=bever3, parms=p) )
outL <- pivot_longer(outdf, -time, 
                     names_to="State_var", values_to="N")
ggplot(outL, aes(time, N, colour=State_var)) + geom_line()
}
head(outdf)
### Phase plane plot
plot(outdf$Na, outdf$Nb, type='l', ylab="N_B", xlab="N_A")

### Make the start clear
points(y0[1], y0[2], cex=2, col=2, pch=19)
text(y[1], y[2], "Start here (t=0)", adj=c(-.2,.5), srt=-45, col="green")

```

```{r echo=FALSE, eval=FALSE}
bever <- function(t,y,p){
  with(as.list(c(y,p)), {
    dNa <- ra * Na * (1+alphaA*SA + betaA*SB - (Na + cB*Nb)/Ka)
    dNb <- rb * Nb * (1+alphaB*SA + betaB*SB - (cA*Na + Nb)/Kb)
    dSA <- SA*(1-SA)*(Na/(Na+Nb) - nu*Nb/(Na+Nb))
    dSB <- -dSA
    return(list(c(dNa, dNb, dSA, dSB)))
  })
}
t <- seq(0,500, .1)
y0 <- c(Na=1, Nb=.5, SA=0.5, SB=0.5)
p <- c(ra=.7, rb=.5, Ka = 10, Kb=12, cA=0.885, cB=0.98, 
       alphaA=-0.03, alphaB=0.1, betaA=0.1, betaB=-0.2, nu=0.8)
# out <- ode(y=y0, times=t, fun=bever, parms=p)
# outL <- pivot_longer(as.data.frame(out), -time, 
#                      names_to="State_var", values_to="N" )
# 
# ggplot(outL, aes(time, N, colour=State_var)) + geom_line()
```

## Simulations for learning

Often, we learn a lot by describing interactions in simple enough terms that we can analyze the interaction analytically, such as the way we analyzed Lotka-Volterra competition and mutualism. In more complex cases, we often cannot find an analytical, or *closed form*, solution, or if we do, it is so complex that it is difficult to extract meaning. In those cases, we may be able to learn a lot about the consequences of our model's assumptions through simulation. Using simulation, we examine the observed dynamics for a wide range of parameter values and then infer general rules about how parameters or parameter combinations influence dynamics.

In this study, we will use simulations to attempt to determine rules that govern the long-term dynamics of the mutualism model with linear density dependence in both intra- and interspecific interactions. We will also use simulation to confirm our earlier analytical solution and *vice versa*.

### ```lvcompg()```
The core of our simulations will use the ``` lvcompg``` function in the ```primer``` package. Take a look at its arguments using the help page.
```{r, message = FALSE,echo = FALSE, results='hide'}
help(lvcompg)
```

You will notice that the list of parameters is quite specific. It is a *list* (a type of R data object) with two components. The first is a vector of *r* and the second is a matrix of interaction coefficients.

The matrix of interaction coefficients was designed for a competition-only model. Users typically provide positive values for what are actually negative  interactions (i.e. competition). We will do something a little different: below we start with the biology by providing negative values for intraspecific competition coefficients, and positive values for mutualistic interactions. We then reverse the signs of the entire matrix in order to make them work with ```lvcompg```.
```{r echo = TRUE, results='markup' }
a <- matrix(c( -.1, .02,
                      .02, -.09), nrow = 2, byrow=TRUE)
a
a.reversed <- -1 * a
a.reversed
```

Now let's numerically integrate, or simulate, the model and plot the outcome. Comment your code to help remind you what is going on.

```{r fig.width=3.5, fig.height=2.5,echo = TRUE, results='markup' }
r <- c(1,1)
parms <- list(r, a.reversed)
parms

t=seq(0,30, by=.1)
N0 <- c(N1=2, N2=1)
lvout <- ode(N0, t, lvcompg, parms)
lvm <- melt( as.data.frame(lvout), id="time", 
             variable.name="pop", value.name="N")
head(lvm)

ggplot(lvm, aes(time, N, linetype=pop)) + geom_line() + 
  geom_hline(yintercept=c(-1/a[1,1], -1/a[2,2]), linetype=c(1,3)) +
  annotate(geom="text", 
           x=c(15, 20),y=c(-1/a[1,1], -1/a[2,2]), label=c("K1", "K2"), vjust=-0.2 )
```

### Simulations
When we model a phenomenon, we want to use a range of plausible parameter values. In a case like this one where we are exploring a model more than the natural phenomenon it represents, we should use a wide range of parameter values that include cases where it is likely to fail. In our case, we will hold intraspecific density dependence constant and vary interspecific interaction coefficients from zero to twice the value of intraspecific interactions. 

We start with building the bits and pieces that the simulation will use, and then use a for-loop to cycle through all the parameter combinations we are interested in.
```{r, message=FALSE, warning=FALSE, echo = TRUE, results='markup' }
# Use expand.grid to create systematically combinations of parameters
df <- expand.grid(a11 = 0.1, a12=seq(0, .2, by =.02),
                  a21 = seq(0, .2, by=.02),  a22=0.1 )
dim(df) # The number of rows and columns
head(df) # the top of the data frame

# set intrinsic rates of increase...
r <- c(1,1)

# and starting values...
N0 <- c(N1 = 10, N2= 10.001)

# and times for which to return output.
t <- 0:200

# Set values that will be useful later on in our for-loop.
n <- length( t + 1 )
npar <- nrow( df ) 

# create an empty data frame to hold our simulation output.
output <- data.frame( t = numeric(npar), N1 = numeric( npar ), 
                      N2 = numeric( npar ) )

### USE A  FOR-LOOP TO INTEGRATE THE MODEL FOR EACH COMBINATION OF PARAMETERS
for( i in 1:npar ) {
  coefs.true <- matrix( as.numeric( df[i,] ), 
                        nrow = 2, byrow = TRUE)
  coefs.rev <- matrix(c(1,-1,-1,1), nrow=2) * coefs.true
  capture.output( 
    # capture.output is a function that allows us to avoid seeing warnings...
    # not usually a good idea to ignore warnings!!!
    # but it is OK here....
    lvout <- ode(N0, t, lvcompg, parms = list(r, coefs.rev) )
  )
  
  # hang on to only the last row of output, and  
  # put it in the i-th row of our output data frame.
  last <- nrow(lvout)
   output[i,] <- lvout[last,]
}


# Combine columns of parameters and output.
simdat <- cbind(df, output)
```

### Checking simulation results
Let's look for anomalous results, such as negative population sizes, or infinite or "too" large sizes. 
```{r echo = TRUE, results='markup' }
# Do simplistic numerical summaries.
summary(simdat[,5:7])
```
If we run this, we see some strange population sizes and time was not always `r max(t)`, showing that the numerical integration failed before completing. Also, $N$ included unreasonably large values.

Let's look for all rows in which *either* $N_1$ *or* $N_2$ are huge ($> 10\,000$).
```{r echo = TRUE, results='markup' }
# the vertical bar | means "or"
sim.N <- subset(simdat, N1 > 10000 | N2 > 10000)
head(sim.N)
```
Notice also that values for $t$ indicate that the integration couldn't find values beyond a certain time point. Hmm. For now, let's assume our simulation is doing what we think it is and that these results indicate a true lack of equilibrium.


What could we do to learn the emerging rules? What if we just plot the values of $\alpha_{ij},\,\alpha_{ji}$ where $N$ is really, really big?

```{r echo = TRUE, results='markup' , fig.width=3, fig.height=2.5}
ggplot(sim.N, aes(a12, a21)) + geom_point()
```

What do we learn from just this figure? Write down a possible set of conditions necessary to get these anomalous results.



### Comparing simulation results to analytical solutions

Any time we do something novel in our modeling, we need to find a way to evaluate the novel approach to make sure it is doing what we think it is. When we have an analytical solution for our model, it is great to compare the analytical solution to the simulations.

One thing we can do is to compare the long-term steady state values we get in simulation to the analytical equilibria of this mutualism model.
$$N_i^* = \frac{\alpha_{jj} + \alpha_{ij}}{\alpha_{ii}\alpha_{jj} - \alpha_{ij}\alpha_{ji}}$$


First, we find analytical solutions for all of the equilibria predicted by the coefficients. We then compare these to numerical solutions.

Here we calculate the predicted equilibria for each row of our coefficients, using the above solution.
```{r echo = TRUE, results='markup' }
denom <- simdat[,1] * simdat[,4] - simdat[,2] * simdat[,3]
N1.eq <- (simdat[,4] + simdat[,2]) / denom
N2.eq <- (simdat[,1] + simdat[,3]) / denom

# combine with simulation data set
sim.check1 <- cbind(simdat, N1.eq, N2.eq)
```
Let's find the subset with potential problems, such as for $N_1$ or $N_2$ less than than zero or really, really big ($>10000$).

```{r echo = TRUE, results='markup' ,fig.width=3, fig.height=2.5}
check1 <- subset(sim.check1, (N1 < 0 | N2 < 0 | N1 > 1e4 | N2 > 1e4 ) )
# look at the subset
ggplot(check1, aes(N1, N1.eq)) + geom_point()
```

Fascinating! Both the analytical and the simulated results give us wacky, but very different results. Let's examine the coefficients that generated these results.
```{r echo = TRUE, results='markup' }
check1[1:5,]
```
What do you notice about these coefficients? 

* What would you get for each term in the  for the analytical equilibrium using the first row of data?
* How do you reconcile these results?

Once we understand the anomalous results above, let's compare the simulated and the analytical solutions for $N_1$ and $N_2$ both greater than zero and both less than infinity.
```{r echo = TRUE, results='markup' }
check2 <- subset(sim.check1, (N1.eq > 0 & N2.eq > 0) & 
                   (N1.eq != Inf & N2.eq != Inf ) )
```

Next we calculate the differences between the analytical solutions and the simulated values.
```{r echo = TRUE, results='markup' }
N1.diff <- check2$N1 - check2$N1.eq
N2.diff <- check2$N2 - check2$N2.eq
summary(N1.diff); summary(N2.diff)
```

These summaries show us that the differences between the  analytical solutions and the numerical approximations are very small. That is a good thing. It gives us confidence that our simulations are doing what we think they are doing.

So, we can now tentatively assume that our simulations are doing what we think they are, and we could have confidence with results that we generate.

In practice, sometimes we simulate a very complex model and compare the results with a simpler, tractable analytical solution. Here, we get the hang of the approach by comparing simulations against known solutions.



### One last thing: using the Jacobian matrix

Use eigenanalysis of the Jacobian matrix to assess the stability of these equilibria. Consult eq. 5.21 and Table 5.2 of Stevens (2009). 

\medskip

A sensible equilibrium.
```{r echo = TRUE, results='markup' }
subset(sim.check1, a12 == .02 & a21 == .02)
# form remember that we set r = 1
J1 <- matrix(c(-.1*12.5, -.02*12.5,
               -.02*12.5, -.1*12.5), nrow=2, byrow=TRUE)
eigen(J1)
```
What do these results mean?

\bigskip

A nonsensical equilibrium
```{r echo = TRUE, results='markup'}
subset(sim.check1, a12 == .2 & a21 == .2)
J2 <- matrix( c( -.1*(-10), -.2*(-10), 
                 -.2*(-10), -.1*(-10)), nrow=2, byrow=TRUE) 
eigen(J2)
```
What do these results mean?

## *In Fine*
So, how can you digest and communicate what you have found? You shjould be able to answer these questions:

1. Did the simulation perform as you thought and how do you know?
2. Did the simulation *confirm* something we first assessed analytically? If so, how?
3. How do you explain the differences between the simulations and analytical equilibrium (extremely large vs. negative population sizes)?


<!--chapter:end:MututualismBeverIssues.Rmd-->

---
title: "Beiler et al. 2015"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(igraph)
```

## properties assessed
node = tree
link = a connection between two trees, 1 or more genets = 1 link 


Network properties

* link density
* degree distribution
* global or network clustering coefficient, global transivity; (no. of closed triplets) / (no. of open + no. closed triplets)
* mean clustering coefficient among nodes in each network
* Betweenness centrality: quantifies the number of times a node acts as a bridge along the shortest path between two other nodes. How important is this node in making connections for other nodes?

Evaluated by testing whether the ovbserved differ from random ER network models based on the same number of nodes and average node degree as empirical MNs


Node properties

* degree
* normalized degree: (d / all possible d)
* node clustering coefficient (local transivity): how close its neighbors are to being a clique themselves. "proportion of links between the vertices within its neighbourhood divided by the number of links that could possibly exist between them" (Wikipedia, clustering coefficient) 


"Network classifications were verified by contrasting empirical networks against undirected random Erdos-Renyi configurations ... having the same number of nodes and mean node degree as the empirical networks, when pairs of nodes are linked at random." 


## Results 

* Vesiculosus more structured than vinicolor
* Vinicolor nested with vesiculosus

No significant relations found between site (soil moisture) and netowrk properties

* network density
* mean node degree, or
* network clustering coefficient 

However, node clustering coefficient and node degree increased with tree dbh (Fig. 4). Seems strange then, that there is a negative relation between node clustering coef and node degree. 


## Data from Beiler et al.

Plot 1
* 26 trees, 258 links (Table 1), node clustering coefficient: 0.84

```{r }
# for Plot 1
V <- 26
E <- 258

gr <- erdos.renyi.game(V, E, type = "gnm")
# graph "order" and "size"
gorder(gr); gsize(gr)

plot(gr, vertex.label= NA, edge.arrow.size=0.02,vertex.size = 0.5, xlab = "Random Network: G(V,E) model")
```

```{r}
# betweenness centrality
bc <- betweenness(gr, v = V(gr), directed = TRUE, weights = NULL,
  nobigint = TRUE, normalized = FALSE)
bcbar <- mean(bc)

# node clustering coefficients (per node)
ncc <- transitivity(gr, type = "local")
# mean node clustering coefficient
mean(ncc) # cf 0.84

# degrees of each node
d <- degree(gr)
# mean degree
mean(d) # cf 18

qplot(d, ncc, geom="jitter")
```

### for all plots

```{r} 
V <- c(26, 13, 31, 27, 27, 41)
E <- c(258, 66, 283, 254, 177, 434)
pV <- c(.846, .5, .875, .846, .778, .889)
dbar <- c(18, 695, 17.15, 17.52, 12.21, 20.19)
nccbar <- c(.84, .63, .84, .85, .83, .8)
plot <- 1:6
df <- data.frame(plot=plot, V=V, E=E, pV=pV, dbar=dbar, nccbar=nccbar)

myStats <- function(ve){
  # ve is a vector of V, E
  V <- ve[1];  E <- ve[2]
  gr <- erdos.renyi.game(V, E, type = "gnm")
  
  bc <- betweenness(gr, directed = FALSE, weights = NULL,
  nobigint = TRUE, normalized = FALSE)
  bcbar <- mean(bc)
  
  #Network clustering coefficient
  gcc <- transitivity(gr)
  
  ncc <- transitivity(gr, type = "local")
# mean node clustering coefficient
 nccbar.o <- mean(ncc) # cf 0.84
# degrees of each node
d <- degree(gr)
# mean degree
dbar.o <- mean(d) # cf 18
return( c(bcbar.op=bcbar, gcc.o=gcc, nccbar.o=nccbar.o, dbar.o=dbar.o))
}
df2 <- cbind(df, t(apply(df[,2:3], 1, myStats)) )
pairs(df2[,5:10])
```
## Fig. 2 but with random graphs
```{r}


ncc1 <- unlist( apply(df[1:3,2:3], 1, function(ve){
   gr <- erdos.renyi.game(ve[1], ve[2], type = "gnm")
   transitivity(gr, type = "local")
 }))
ncc2 <- unlist( apply(df[4:6,2:3], 1, function(ve){
   gr <- erdos.renyi.game(ve[1], ve[2], type = "gnm")
   transitivity(gr, type = "local")
 }))
 
d1 <- unlist( apply(df[1:3,2:3], 1, function(ve){
   gr <- erdos.renyi.game(ve[1], ve[2], type = "gnm")
   degree(gr)
 }))
d2 <- unlist( apply(df[4:6,2:3], 1, function(ve){
   gr <- erdos.renyi.game(ve[1], ve[2], type = "gnm")
   degree(gr)
 }))
dff2 <- data.frame(site = factor(c(rep(1, length(ncc1)), rep(2, length(ncc2)))),
                            ncc =c(ncc1, ncc2), d=c(d1,d2) )
ggplot(dff2, aes(d, ncc, colour=site) ) + geom_jitter()
```

## Fig. 2 but with scale-free graphs
```{r}

exp.out <- 2.1
ncc1 <- unlist( apply(df[1:3,2:3], 1, function(ve){
  gsf <- sample_fitness_pl(ve[1], ve[2], exponent.out= exp.out, exponent.in = -1, loops = FALSE, multiple = FALSE, finite.size.correction = TRUE)
   transitivity(gsf, type = "local")
 }))
ncc2 <- unlist( apply(df[4:6,2:3], 1, function(ve){
  gsf <- sample_fitness_pl(ve[1], ve[2], exponent.out= exp.out, exponent.in = -1, loops = FALSE, multiple = FALSE, finite.size.correction = TRUE)
   transitivity(gsf, type = "local")
 }))
 
d1 <- unlist( apply(df[1:3,2:3], 1, function(ve){
  gsf <- sample_fitness_pl(ve[1], ve[2], exponent.out= exp.out, exponent.in = -1, loops = FALSE, multiple = FALSE, finite.size.correction = TRUE)
   degree(gsf)
 }))
qplot(d1, ncc1)
d2 <- unlist( apply(df[4:6,2:3], 1, function(ve){
   gsf <- sample_fitness_pl(ve[1], ve[2], exponent.out= exp.out, exponent.in = -1, loops = FALSE, multiple = FALSE, finite.size.correction = TRUE)
   degree(gsf)
 }))
dff2 <- data.frame(site = factor(c(rep(1, length(ncc1)), rep(2, length(ncc2)))),
                            ncc =c(ncc1, ncc2), d=c(d1,d2) )
ggplot(dff2, aes(d, ncc, colour=site) ) + geom_jitter()
```

## Fig. 2 but with small-world
```{r}
dm=2
nei=3
p=.2
{
ncc1 <- unlist( apply(df[1:3,2:3], 1, function(ve){
  size <- ceiling((ve[1])^(1/dm))
  gsw <- sample_smallworld(dim=2, size, nei , p )
   transitivity(gsw, type = "local")
 }))
ncc2 <- unlist( apply(df[4:6,2:3], 1, function(ve){
    size <- ceiling((ve[1])^(1/dm))
  gsw <- sample_smallworld(dim=2, size, nei, p)
   transitivity(gsw, type = "local")
 }))
 
d1 <- unlist( apply(df[1:3,2:3], 1, function(ve){
   size <- ceiling((ve[1])^(1/dm))
  gsw <- sample_smallworld(dim=2, size, nei, p)
   degree(gsw)
 }))
qplot(d1, ncc1)
d2 <- unlist( apply(df[4:6,2:3], 1, function(ve){
      size <- ceiling((ve[1])^(1/dm))
  gsw <- sample_smallworld(dim=2, size, nei, p)
   degree(gsw)
 }))
dff2 <- data.frame(site = factor(c(rep(1, length(ncc1)), rep(2, length(ncc2)))),
                            ncc =c(ncc1, ncc2), d=c(d1,d2) )
ggplot(dff2, aes(d, ncc, colour=site) ) + geom_jitter()
}
```

<!--chapter:end:networks.Rmd-->

# Optimal Foraging {#oft}
```{r cowie, fig.cap="Optimal foraging theory (OFT) generates testable quantitative predictions that allow a less ambiguous description and explanation for observed patterns and processes. Here, a simplistic model of Great Tit (Parus major) foraging that includes only gross energy intake underestimates the time spent in patches (dashed). In contrast, a model that includes energetic costs of traveling and searching matches predictions far better (solid).  From Cowie (1977).", out.width="75%"}
knitr::include_graphics("figs/CowieF3.png")
```
Natural selection can be thought of as an optimizing process: phenotypes diversify, winners replicate and losers don't, and the phenotypes of winners tend to get passed on to the replicants. We therefore often assume, as did Dr. Pangloss, that the species that exist now are the best of all possible species, that is, they are of optimal design. And like Dr. Pangloss, we would be woefully mistaken if we stopped there. 

Optimal Foraging Theory (OFT) allows us to consider what organisms would do *if they foraged optimally.* All organisms--plants, fungi, archaea, and even animals--forage, and they are all subject to natural selection. Therefore, their phenotypes work pretty well, but probably not optimally and definitely not optimally for all times and places. Nonetheless, OFT is an efficient theory about the behavior of an organism, in the absence of other complications. Therefore, it allows us to study the relative importance of those "other complications."

Foraging is a key link between the individual, and communities and ecosystems [@Beckerman2010]. All organisms interact with their environment via consumption, and the choices they make influence population dynamics, species interactions, nutrient cycling, and even the physical structures of terrestrial and aquatic habitats. The text and logic of this chapter rely heavily on @Stephens1986 and @Ellner2009.

In Scheiner and Willig's edited volume on *The Theory of Ecology*, Andy Sih [@Sih2011] proposes that the following propositions form the basis of foraging theory:

1. Foraging patterns maximize fitness or a correlate of fitness. 
2. Foraging patterns depend on the range of options available to the forager and on how each available option affects fitness or a correlate of fitness.
3. Foraging behaviour balances conflicting demands--tradeoffs are important in shaping foraging behaviour.

These properties are the outcome of natural selection operating on foraging behavior. Our understanding of foraging itself considers these three features [@Stephens1986]:

* currency (what is being optimized),
* constraints (features of behaviour that limit optimality), and
* the resulting decision rules.

*Currency* is that quantity that is optimized by the forager. This currency is assumed to be a quantity that limits fitness, such as energy or a particular consumable resource. It measure it as a rate, for instance, energy gained per unit time (E/T) or the uptake of a critically limiting resource per unit time (R/T). 

*Constraints* are limitations that we assume about organisms. These might include distances between resource patches, the time and costs associated with extracting a resource from a substrate or subduing prey. They also include constraints imposed by other species including competitors and predators. Constraints can get coplicated quickly; however, simple quanitative theory makes predictions against which we can evaluate more complicated assumptions.

*Decision rules* are what we ascribed to a forager's choices. A decision rule concern the probability of attacking prey if encountered, or when to leave one resource patch in order to search for another. 


An additional way to think about all this is *where, when, and what.* A great deal of effort has focused on understanding patch use: *where* foragers should explore for resources, and *when* they should give up and go in search of another patch [@Charnov1976b]. These are **patch use** models, and are based on economic models and the marginal value theorem. 

Another avenue of inquiry concerns *what* animals should eat. For instance, should they go after big prey that may be hard to catch and difficult to subdue, or just snack on what is easy? These are **prey models** or diet models, and attempt to explain why organisms consume what they do. Much like the patch models, theory attempts to predict which prey or resources organisms encounter, perceive, attack or "handle", and which they ignore.

*A note on "prey".* All organisms forage for resources. Plants extend branches toward the light, and proliferate leaves and roots into resource rich patches, and rhizomes grow longer faster through resource-poor soils. Bumblebees search for and learn where to find nectar-rich flowers, and wolves hunt in packs to take down large ungulates. Some bacterivorous nanoflagellates intercept particles selectively depending on the perceived nutritional value of particles [@Boenigk2002]. So, depending on the forager, its "prey" may be $\mathrm{NO}_{3}^-$ ions, nectar, moose, or bacteria. Therefore, we will refer to these resources variously as prey, prey items, resources, and resource items. Some of these ideas are best handled with patch-based models [@Charnov1976b] where a "resource patch" is a more intuitive and useful unit.

*A note on "handle".* All organisms pays costs to consume resources. In OFT, "handle" typically means expending energy an time to attack and subdue prey (predators), proliferate into resource rich areas (plants), exude extra cellular enzymes (fungi); ingest the item(s), and then resume searching.

## A prey model
Here the forager asks, "should I eat this?"^[whether "I" is a sugar maple tree or a human, most of us do not make this decision conciously...] Let's start where this field started, with a prey-centered model [@MacArthur1966; @Emlen1966]. 

\begin{itemize}
\item Currency - long-term energy gain per unit time, $G/T$.
\item Constraints
\begin{itemize}
    \item forager can consume only what it encounters.
    \item forager uses time and energy searching for items and handling items.
    \item if the forager chooses to pursue an item, searching stops and handling starts.
    \end{itemize}
\item Decision rules - the probability that a forager pauses from searching to handle an item.
\end{itemize}


### Our intuition
```{r ant, echo=FALSE, fig.cap="The amount of energy lost and gained by a foraging ant--it may decline slowly over time while searching, and decline quickly while handling a food item. Our ant gains energy when it consumes an item. Below: Our ant. She expends energy while searching for food. Upon encountering a food item, she may choose to 'handle' it (encounters 1 and 3) and gain energy, or not handle it (encounter 2) and save the added cost of handling it.", out.width="100%" }
knitr::include_graphics("figs/foraging_ant2.jpg")
```

What does our intuition tell us is the optimal strategy for foraging? It seems reasonable that if a forager encounters food, it should eat it. However, if handling it costs more than the forager gets back in energy, then it isn't worth it. We might think of this as the ratio as *profitability*,
$$\frac{e_i}{h_i}$$
where $e_i$ is the energy in an item of type $i$, and $h_i$ is the cost of handling said item. If $e_i/h_i<1$, then it doesn't make sense to select the item. Further, handling an item means that the forager is *not* looking for a better food item. This suggests that even if $e_i/h_i>1$, a forager may not want to handle it if it is likely to soon encounter food items of higher energy content. On top of this, the act of searching may expose a forager to a risk of running into competing foragers, or even being eaten by a bigger forager. Clearly, a forager faces tradeoffs as it searches and when it encounters resources.

### Mathematical support
Let us assume that natural selection tends to maximize the currency, or $G/T$. Our model will use these parameters and variables:

* $i =$ index for prey type
* $p_i =$ probability that a forager attacks prey if encountered (units are number handled per number encountered, or #/#; this is a dimensionless parameter)
* $S =$ total time spent searching (units = seconds, $s$).
* $T =$ total elapsed time (units = s)
* $\lambda_i =$ rate of encounter with prey of type $i$ (units = # encountered/s = $\#/s$; note this can also be #/area × area/s, if we like).
* $e_i =$ net energetic gain from catching and consuming a single type-$i$ prey item (units = Joules, J). This includes the gross energy of the item minus handling costs plus energy *not* lost by searching during that time.
* $h_i =$ handling time for an item of type $i$,  (units = s/#). 
* $c =$ energy cost per unit of time while searching (units = J)

From these definitions we can calculate other important quantities.

* Total number of items encountered of type $i$ is $S\lambda_i$ (units $= s\, \#\,s^{-1} = \#$).
* Total number of type $i$ items handled is the proportion, $p_i$, of those encountered that the foragers chooses to go after, or $S\lambda_i p_i$ (units = #).
* Total time spent *handling* all items of type $i$ is $S\lambda_i p_i h_i$ (units = s).
* Total elapsed time, $T$, is time spent searching plus time spent handling, which is  $S + \sum_i^n S\lambda_i h_i p_i$. We use the summation to add together the total handling times for each prey or resource type $i = \{1,\,2, \ldots ,\,n\}$.

*A note on $\lambda$* A forager encounters prey at random, and this is known as a "Poisson process", where the number of encounters in a specified time interval is a random variable drawn from the Poisson probability distribution. It turns out that the *time between events* of a Poisson process follows the Exponential distribution. The Poisson distribution is determined by a single parameter, its mean. The Exponential distribution is also described by a single parameter, its *rate*. It turns out that we use the same $\lambda$ for both. 

We have combined many variables and parameters. How can we check the logical consistency? We do that by checking whether the units are the same in all terms of a model. Here we check units for total handling time, $H = S\lambda_i p_i h_i$ by replacing the variables and parameters with their units. The units for $H$ are seconds, so we have
$$\mathrm{s = s \frac{\#}{s} \frac{\#}{\#} \frac{s}{\#}}$$
When we multiply these fractions, we find that the $\#$'s and two of the s's cancel out and we are left with $s=s$. This shows that our use of parameters and variables is at least logically consistent.


The total energy gain from eating all the items is the number of items of each type $i$ handled times the net amount of energy per item of type $i$, $e_i$, $\sum_{i=1}^n S\lambda_i p_i e_i$. Therefore, rate of energy intake while handling and eating is 
$$\mathrm{intake} = \frac{\sum_{i=1}^n S\lambda_i p_i e_i}{S + \sum_{i=1}^n S\lambda_i h_i p_i}$$
If we also subtract the cost of searching, we arrive at the quantity we want to maximize, $$\frac{\sum_{i=1}^n \lambda_i p_i e_i}{1 + \sum_{i=1}^n \lambda_i p_i h_i}-c$$


A major question in OFT is whether a forager should include a particular prey type. Say we rank the prey types, $i=\{1,2,...,m,...,n\}$, in terms of energy content, where type $i=1$ has the most energy per item, $i=m$ is intermediate, and type $i=n$ has the least. *Which items should a forager include in her diet?* Should it be only the most energy-dense, or should it include the second as well, or should it be all of them? Part of the answer rests on the ratio of energy gain versus handling costs, or profitability, $e_i/h_i$.

It turns out there is an useful rule, which we won't derive mathematically: *A less energy-dense item should be added if its net energy content is greater than the realized energy gain from all the other items,* or 

$$\frac{e_{m+1}}{h_{m+1}} > \frac{\sum_{i=1}^m \lambda_i e_i}{1 + \sum_{i=1}^{m} \lambda_i h_i} (\#eq:preypred1)$$
where the diet already includes items 1-$m$, and the *realized* energy content of the diet takes into account average encounter rates of each item type^[Prediction: foragers will rank prey types by their *profitability*, $e/h$.] Implicit in this is the prediction that items of type $i$ should either always be selected or never be selected if encountered^[Prediction: the zero-one rule: $p_i \subset {0,\,1}$]. (This can be shown algebraically). Thus, a particular type should always be included if the instantaneous net gain^[Prediction: Inclusion of type $m+1$ in the diet does not depend on its encounter rate.] of that type is greater than the *realized long term average* net gain of all the more profitable types. The quantity on the right side of \@ref(eq:preypred1) takes into account the long term encounter rates $\lambda_i$ as well as search costs.

Another prediction is that when encounter rates increase (as with increasing abundances), selectivity increases^[Prediction: selectivity increases with average encounter rates.]. Note that encounter rates are in the right hand side (RHS), so as they increase, so will that fraction on the right. That will make it harder for the above inequality to be true. If you don't believe it, try this simplified version (Fig. \@ref(fig:selectivity)).
```{r selectivity, echo=TRUE, fig.cap="Selectivity increases with average encounter rates.", out.width="60%", keep.source=TRUE}
eq = function(lambda){lambda/(1+lambda)} # create the function you want
myData <- data.frame( lambda=c(0, 10) ) # data you need
ggplot(data=myData, aes(x=lambda)) + # set the basic properties of the plot
  # in the stat_function, we indicate the parameters in our equation.
  stat_function(fun=eq, geom="line") +
  ylab(bquote(over(lambda, 1 + lambda))) + xlab(bquote(lambda))  # add labels
```

So how does this model fair in the real world? Well, the zero-one rule doesn't work at all; it turns out that for a variety of reasons, foragers do not completely ignore low-profit prey. However, there is great support for the other two predictions (above) [@Stephens1986]. Most importantly, in all cases, the theory has provided a clear framework to generate testable predictions from unambiguous assumptions, and that is what we want from efficient theory. The model itself helped guide research, and inclusion of greater complexity has led to deeper understanding of behavior and its consequences for species interactions.

## The patch model
*...in which ominscient rationale agents roam free.*

Here the forager asks, "how long should I stay here?" In the simple *prey* model, a forager searches for and then encounters prey one at a time, makes a decision to consume or not, and then resumes searching. In a simple *patch* model, a forager searches for and encounters patches one at a time, first consumes resources and then makes a decision to leave or not. Perhaps the single most important prediction of the simple patch model is that a forager should leave a patch when its current rate of energy gain drops down to the average or expected rate of energy gain for the habitat at large. 

In what follows, we rely on @Charnov1976b, who applies *the marginal-value theorem* to explain optimal behavior. Here, as in economics, "marginal value" refers to a rate - the slope of a function. In calculus, this is a derivative. Here, it is the derivative (i.e. slope) of the relation between energy gain and time.

Let's assume the simplest of all patch models: one patch type, all patches are the same, and they are distributed randomly in the habitat. Assume also that a forager uses time to travel between patches (travel time, $t_t$) and time searching within a patch (residence time, $t_r$). A forager encounters patches at random, with a rate of $\lambda$, and as such, would have a mean time to next encounter of $1/\lambda$. 

The patch is characterisized by its *gain function* $g(t_r)$ (Fig. \@ref{fig:gain)) which is the expected^["expected" value of a variable is the mean of that variable, because that is what we expect given the absence of other knowledge]
*cumulative net energy gained*, given time $t_r$ spent in the patch. The gain function is a cumulative total net amount. We can imagine different types of gain functions.
```{r gain, echo=FALSE, fig.cap="Net energy gain as a function of patch residence time may take different forms. Net energy gain increases through time but slows (decelerates) as a greater fraction of the resources in the patch are consumed. The top line (solid) assumes that there are diminishing returns as a patch is depleted, but the forager continues to find resources in excess of metabolic losses. The lower line (dashed) represents the net energy gain that could arise as a patch is depleted more fully and the costs continue unabated.", out.width='75%'}
eq1 <- function(tr) {.7 * tr/(.1+tr)}
eq2 <- function(tr) {-tr^2 + 1.5*tr}
df <- data.frame(tr=c(0,1) )
ggplot(df, aes(x = tr ) ) + 
  stat_function(fun=eq1, geom="line") + 
  stat_function(fun=eq2, geom="line", linetype="dashed") +
       ylab(expression("Energy intake, "*italic(g(t)))) + 
       xlab(expression("Residence time, "*italic(t[r])))
```

**Try this:**

1. Draw  a gain function where the prey remain well hidden at first, but the forager becomes increasingly able to find more and more prey. 
2. Draw a gain function where there is no cost to foraging, and where the forager eventually depletes all the prey. 
3. In one graph, draw two gain functions for a resource rich patch and for a resource poor patch.


So, our *currency* is long-term average energy intake, $R$, and we want to maximize this. The *decision* our forager needs to make is how long to stay in a patch. The forager's *constraints* share some similarity with the prey model [@Stephens1986]. 

* between-patch travel time and within-patch hunting time are distinct, and ...
* ... independent of each other,
* a forager encounters patches sequentially and randomly,
* in a given patch, net expected energy gain is a function of time spent in the patch...
* ...that is zero when $t=0$, and
* ...increases with time, but then decelerates
* the forager is omniscient - it knows everything about available patches and does not learn anything new as it forages (because it already know everything).

The forager must decide how long to stay in the patch to maximize $R$. Let 
$$R=\frac{g(t)}{t_t + t_r}$$
where $t_t + t_r$ is the total time from leaving one patch, traveling to the next patch, foraging in the second patch, and then leaving the second patch. 

Intuitively, we can imagine that the long term average rate of energy gain $R$ is unimodal (hump-shaped) in the following scenario. Upon encountering a patch the forager has no resources and thus $R$ is actually negative due to the costs of traveling to the new patch. As $t_r$ increases and the forager gains energy, $R$ will increase and become positive. An assumption of the theory (and reality) is that the gain function, $g(t_r)$, decelerates--the rate of energy intake declines as the patch is depleted. With increasing time in the patch and lower rate of energy intake, $R$ starts to decline. 

Because $R$ is unimodal, we can use calculus to find its maximum. This will occur when its slope is zero, and the slope of a function, $F$, is its derivative, $F^\prime$. If we asssume that travel time is constant, then we can take the partial derivative of $R$ with respect to just the residence time, $t_r$, $\delta R / \delta t_r$. First, recall the product rule of differentiation: 
$$F(x) = g(x)f(x)\quad ; \quad F^\prime(x) =f^\prime(x)g(x) +  f(x)g^\prime(x)$$
With that we can find the necessary derivative.
$$\frac{\delta R}{\delta t_r} = - \frac{1}{(t_t+t_r)^2} g(t_r) + \frac{1}{t_t+t_r}g^\prime(t_r)= \frac{(t_t+t_r)g^\prime(t_r) - g(t)}{(t_t+t_r)^2}$$
When we solve $\delta R / \delta t_r = 0$, we discover that 
$$g^\prime(t_r)=\frac{g(t_r)}{t_t + t_r}$$
Now we see that 
$$R  =\frac{g(t_r)}{t_t + t_r}= g^\prime(t_r)$$
which tells us...that in order to maximize the long-term average rate, we should stay in a patch until the instanteous rate, $g^\prime(t_r)$ drops to the long term average rate, $R$.

```{r marginal, echo=FALSE, fig.cap="Energy gain vs. time: The origin is when the forager enters the patch; to the left is time spent traveling from one patch to the next, and to the right is time spent in the patch. The graph represents two different habitats, one in which the patches are easy to get to (habitat 1), and another where it takes more time to get from patch to patch (habitat 2). In all cases, the patches are identical, having the same gain function. The curved line is the gain function, the net energy gain as a function of time spent in the patch. The slope of that curve is the derivative of the gain function. Its slope at any single time point is the instantaneous rate of gain. The two straight lines are the expected gains averaged over time for each habitat as a whole. Lambda is the rate at which a forager randomly encounters patches - because it is a Poisson process, the mean or expected time is 1/lambda. The forager should leave the patch when the instantaneous rate of gain in the patch equals the long term average rate of gain for the habitat as a whole.", out.width="100%" }
knitr::include_graphics("figs/marginal-value.jpg")
```

The simple patch model predicts that when average travel time is greater, foragers will stay longer in a patch. Similarly, the model predicts that when patch quality is lower, foragers stay longer in each patch. Use Fig. \@ref(fig:marginal) to construct explanations for these predictions.

**Just a starting point**  

The simple prey and patch models have been extended a great deal to help understand a broad range of foraging situations [@Sih2011]. Simultaneous, rather than sequential, encounters can lead to different predictions. In these cases, energy alone, $e_i$, rather than profitability, $e_i/h_i$, may determine prey selection that maximizes the long term mean average rate. Travel time and encounter rates interact with this to explain contrasting situations. 

Central place foragers play by slightly different rules [@Stephens1986]. Central place foragers are located in a single location, and remain there. For instance, a parent bird (or dinosaur) finds patches and returns repeatedly to the nest, bringing one or multiple prey items. With parent birds, their fitness depends on offspring viability, and so selection tends to optimize in a manner similar to an organism foraging for themselves. These cases have been built upon patch models, where the question is how to exploit patches that exist at different distances from the nest. 

Another example of a central place forager is a spider that acts as a ambush or sit-and-wait predator who remains stationary until a prey item gets close enough to attack. One approach to the spider problem is to consider the distance to the prey as a handling cost and search costs are negligible.

These simple foraging models provide the starting points for a field of inquiry spanning many decades. The interplay between these models, the natural history of species, and experiments have led to greater appreciation of why organisms behave as they do, and the consequences for their evolution and the food webs and ecosystems in which they reside.

\newpage
## A simulation of a prey model
Next, we embark on a simulation of the simple prey model. We will start with these assumptions:

* two prey types, $i = {1,2}$
* ranked effective energy contents, $e_1 > e_2$
* equal handling times, $h_1=h_2=1$
* equal relative abundances, $r_1=r_2=0.5$
* encounter rates determined by an overall prey encounter rate, $\lambda$, and the relative abundances where $\lambda_i = \lambda r_i$.
* equal probability of attack if prey is encountered, $p_1=p_2=1$.
* search cost is constant, $c_s=0.01$

In addition to these properties, our simulation needs several bookkeeping parameters and variables in order to track the forager energy content. It will need to run for a finite amount of time; we'll control that with the total search time, `Total`. Remember that encounter rates are means but that actual encounters are random or stochastic. As a result, our forager may go through lean periods in which their net energy intake is negative.

We need to keep track of total elapsed time, and for each cycle, the search time, search cost, handling time, and energy gain.

```{r oft.sim, echo=TRUE}
optimal.forager <- function(
  e = c(2, 1), # energy content of the prey types
  h = c(.5, .5), # handling times
  r = c(.5, .5), # relative abundance of prey types: sum(r) = 1
  lambda = 0.4, # overal encounter rate, for all prey combined
  p = c(1,1), # prob. of attack if encountered
  cs = 0.4, # cost of searching per unit time
  Total = 10 # limit to foraging time
  ) {
###############
### begin foraging
ec <- NULL # an object to tally gains and costs.
cycle <- 0 # the cycle count (= search, choose and maybe attack and eat)
ct <- 0 # start time of the cyclesan object to tally cycle times.
elapsed.time <- 0 # total time spent foraging
while( elapsed.time < Total ) {
  # count which search cycle we're on
  (cycle <- cycle + 1)
  
  # a random amount of search time, t.s, until it finds something.
  (lambda.r <- lambda * r)
  (ts <- rexp(2, rate=lambda.r))
  if(ts[1] < ts[2]) i <- 1 else i <- 2
  i
  # cost of searching for that time
  (cost.s <- ts[i] * cs)

  # choose to attack the encountered item with probability p
  (gain <- if(p[i] > runif(1)){e[i]} else {0})
  
  # observed handling time 
  if(gain > 0 ){
    h.obs <- h[i]
    h.obs
  } else {
    h.obs <- 0
  }
   h.obs
 (cycle.time <- ts[i] + h.obs )
 ct <- c(ct, cycle.time)
 (elapsed.time <- elapsed.time + cycle.time)
 (ec <- c(ec, gain - cost.s))
}
df <- data.frame(net.e = ec, cycle.start = cumsum(ct[1:cycle]))
params <- list(e=e, h=h, r=r, lambda=lambda, p=p, cs=cs, Total=Total)
  out <- list(N = cycle, G = sum(ec), Tt = sum(ct), 
              series = df,
              params = params)
  return(out)
}

```

Here we let the forager forage for 60 minutes and then examine the structure of the output object. 
```{r echo=TRUE, keep.source=TRUE}
myOut <- optimal.forager(Total=60)
str(myOut)
```

* N is the number of foraging cycles
* G is net energy gain
* Tt is total elapsed time
* series is a dataframe with two variables: net.e is energy gain minus search costs for each cycle, and cycle.start is the elapsed time at which each cycle starts
* params is a list that includes all the parameters we used in this run

Now let's graph something, because graphs are fun.
```{r oft1, echo=TRUE, fig.cap="The cumulative energy capital of a forager goes down while searching and handling resource items, but increases each time the prey is assimilated."}

ggplot(myOut$series, aes(x=cycle.start, y=cumsum(net.e))) + geom_line() 

```

*Use this simulation* to help solidify in your own mind predictions of the simple prey model. How should we do that?


What is the prediction we are interested in?

Prediction: Include type 2 if
$$\frac{e_2}{h_2} > \frac{\lambda_1 e_1 }{1 + \lambda_1 h_1} (\#eq:prediction)$$


```{r preypred, echo=FALSE, fig.cap="The right hand side of our prediction", fig.asp=.67, out.width="100%", fig.hold=TRUE}

par(mar=c(5,4,1,0), mgp=c(1.5,.4,0) ) # set figure margins in "lines"
layout(matrix(1:2, nr=1))
with(as.list(myOut$params),{
l1 <- lambda * r[1]
  curve(l1*x/(1 + l1*h[1]), 0, 20, xlab=bquote(e[1]),
      ylab=bquote( lambda[1]*e[1] / (1+lambda[1]*h[1])))
abline(h=e[2]/h[2], lty=2); text(0, e[2]/h[2], bquote(e[2]/h[2]), adj=c(0,-.1))
} )


with(as.list(myOut$params),{
l1 <- lambda * r[1]
  curve(x*e[1]/(1 + x*h[1]), 0, 20, xlab=bquote(lambda[1]),
      ylab=bquote( lambda*e[1] / (1+l1*h[1])))
abline(h=e[2]/h[2], lty=2); text(0, e[2]/h[2], bquote(e[2]/h[2]), adj=c(0,-.1))
} )

```
To get a sense of what our prediction \@ref(eq:prediction) means, we should graph the righthand quantity as a function of one relevant variable, such as energy content of type 1, or the encounter rate (Fig. \@ref(fig:preypred)).  The parameters that determined these curves are:
```{r echo=TRUE, results='verbatim'}
unlist( myOut$params )
```

### Lab exercise
1. Do these parameter values suggest that our forager should or should not include prey type 1 in her diet?
2. Create parameter combinations for which the forager (i) should and (ii) should not include prey type 2.
3. Use the simulation `optimal.forager()` to confirm your predictions.



```{r echo=FALSE, eval=FALSE}
# One possibility is that we can run the simulation with two prey types 
# setting e, h, and lambda to appropriate values. We then
# run it with both types ( p = (1,1) ) and then only type 1 (p=(1,0)) and
# then test whether G/T [both] is higher or lower than G/T [best only]

prey.test <- function(Total=100,...) {
  # run with both types
  both <- optimal.forager(p=c(1,1),...)
  # run with the better type only
  best <- optimal.forager(p=c(1,0),...)
  # Compare long term average energy gain as
  # both / best, where > 1 argues for including both
  (both$G/both$Tt) / (best$G/best$Tt)
}

reps <- replicate(5, prey.test(Total=10000, e=c(5,2), cs=.01) )
reps
# use summary to summarize your replicate simulations
```

<!--chapter:end:optimal_foraging.Rmd-->

# Populations in space

## Source-sink Dynamics
In Chapters 1-3, we considered *closed* populations. In contrast, one could imagine a population governed by *b*irths plus *i*mmigration, and *d*eaths plus *e*migration (a *BIDE* model). Ron Pulliam [@Pulliam:1988ez] proposed a simple model that includes all four components of *BIDE* which provides a foundation for thinking about connected subpopulations. We refer to the dynamics of these as *source-sink dynamics*. Examples of such linked populations might include many different types of species. For instance, a source-sink model could describe linked populations of a single species might occupy  habitat patches of different quality, where organisms might disperse from patch to patch.

```{r fig.cap='Two subpopulations linked with movement'}
ss <- matrix(c(NA, expression("e"[12]=="i"[21]), 
               expression("e"[21]=="i"[12]), NA), nc=2)

ss <- matrix(c(NA, "e[12]==i[21]", "e[21]==i[12]", NA), nc=2, byrow=TRUE)
colnames(ss) <- rownames(ss) <- c("Pop 1", "Pop 2")
ss
plotmat(ss,pos=c(2))
```

### Balanced dispersal

* i12=i21
* Requires that the proportion of the population migrating is lower in larger habitats, higher in smaller patches
     * Size-contingent dispersal startegy (McPeek and Holt 1992)
     * Higher edge:area ratio (Hambeck, Crist, et al. 2007)
* example Diffendorfer (1998) Balanced dispersal in small mammals.
    * Proportion migrating was size-dependent
    * 
     


### Habitat selection
ideal-despotic and ideal-free distributions   

## Metapopulations

$$\dot{p} = cp - cp^2 -ep = (c-e)p(1-p)$$

$$\dot{p} = cp - cp^2 - ep + ep^2$$

rate of change = fecundity x open patches 

$$\dot{p} = fp(1-p) - ec$$



Glanville Fritillary Butterfly (Hanksi 1999, Ojanen et al. 2013)

<!--chapter:end:Space.Rmd-->

---
title: "Untitled"
output: html_document
---

## Modeling with Data: Simulated Dynamics
Science strives to make predictions about about the behavior of systems.
 Ecologists and conservation biologists frequently strive to predict the fate of populations.  Here we  put into practice ideas about population biology to make informed predictions about the fate of the Song Sparrow population in Darrtown, OH. We also illustrate simple commputational methods for doing so.

The preceding sections (the bulk of the chapter) emphasized understanding the deterministic underpinnings of simple forms of density independent growth: geometric and exponential growth. This section explores the stochastic simulation of density independent growth. Our simulation makes most of the same assumptions we made at the beginning of the chapter. In addition, we assume that the observed annual growth rates ($N_{t+1}/N_t$) are representative of future growth rates, and that the growth rate in one year is entirely independent of any other year. 

To make meaning full projections of future population size, we should quantify  the uncertainty with our guess. Simulation is one way we can project populations and quantify the uncertainty. The way one often does that is to use the original data and sample it randomly to calculate model parameters. This way, the simulations are random, but based on our best available knowldge, i.e., the real data. The re-use of observed data occurs in many guises, and it is known often as bootstrapping or resampling.

### Data-based approaches
We could use the observed *changes* in population counts $R_t=N_{t+1}/N_t$ as our data. We would then draw an $R_t$ at random from among the many observed values, and project the population one year forward. We then repeat this into the future, say, for ten years. Each simulation of a ten year period will result in a different ten year trajectory because we draw $R_t$ at random from among the observed $R_t$. However, if we do many such simulations, we will have a *distribution* of outcomes that we can describe with simple statistics (e.g., median, mean, quantiles).

A different approach would be to estimate the individual probabilities of births and deaths in the entire Darrtown population, and use those probabilities and birth rates to simulate the entire population into the future. In such an *individual-based simulation*, we would simulate the fates of individuals, keeping track of all individual births and deaths. 

There are myriad other approaches, but these give you a taste of what might be possible. In this section we focus on the first of these alternatives, in which we use observed $R_t$ to simulate the dynamics of Song Sparrow counts. Do do so, in part, because we have those data, while we do not have any estimates of birth rates or death rates.

Here we investigate Song Sparrow (*Melospize melodia*) dynamics using data from the annual U.S. Breeding Bird Survey (http://www.mbr-pwrc.usgs.gov/ bbs/).  Below we will

1. create and examine visually the data (annual $R$'s),
2. simulate one projection,
3. scale up to multiple simulations,
4. simplify simulations and perform them 1000s of times, and
5. analyze the output.


### Creating and visualizing the data
Let's start by graphing the data^[I've come to abhor my use of the expression "look at"; I use it when I don't say what I mean. "Look at" can mean alomst anything these days, and it rarely means only "looking at"...]. *Graphing the data is always a good idea --- it is a principle of working with data*. We first load the data from the \texttt{primer} \R~package, and look at the names of the data frame. We then choose to `attach` the data frame, because it makes the code easier to read.^[Don't use `attach` for anything important]}
```{r}
library(primer)
data(sparrows)
names(sparrows)
attach(sparrows)
```

Now we plot these counts through time (Fig. \ref{fig:AllSS}).  
```{r cts, fig.cap="Observations of Song Sparrows in Darrtown, OH (http://www.mbr-pwrc.usgs.gov/bbs/).", out.width="75%"}
ggplot(data=sparrows, aes(x=Year, y=Count)) + geom_line() + geom_point(pch=1)
```

We see that Song Sparrow counts at this site (the DARRTOWN transect, OH, USA) fluctuated a fair bit between 1966 and 2003. They never were completely absent and never exceeded $\sim 120$ individuals.

Next we calculate annual $R_t=N_{t+1}/N_t$, that is, the observed growth rate for each year $t$.
```{r}
# the use of [-1[ in the index tells R to exclude the first element.
# length() is the length of a vector, so [-length(X)] means exclude the last
obs.R <- Count[-1]/Count[-length(Count)]
```
Thus our *data* are the observed $R_t$, not the counts *per se*. These $R$ form the basis of everything else we do. Because they are so important, let's plot these as well. Let's also indicate $R=1$ with a horizontal dotted line as a visual cue for zero population growth. Note that we exclude the last year because each $R_t$ is associated with $N_t$ rather than $N_{t+1}$. 
```{r Rrates, fig.cap="Annual growth rates (R=N[t+1]/N[t]) for Song Sparrows", out.width="75%"}
qplot(x=Year[-length(Count)], y=obs.R, geom="point") + geom_hline(yintercept=1, lty=3) + 
  labs(y=bquote(N[t+1]/N[t]), x="Year (t)")
```
One thing that emerges in our graphic data display (Fig. \@ref(fig:Rrates)) is we have an unusually high growth rate in the early 1990's, with the rest of the data clustered around 0.5--1.5. We may want to remember that.

### One simulation
Our simulation will,

1. determine the number of years we wish to simulate,
2. create an empty vector, `N`, to hold our simulated $N$, which is `years + 1` long,
3. draw a random sample of $R_t$, one for each year (`R`),
4. select a starting abundance $N_0$ and put it in `N[1]`.
5. multiply our first random $R$, `R[1]`, times `N[1]` to generate the next, `N[2]`.
6. repeat step 5 for each year to simulate each `N[t+1]` from `R[t]` and `N[t]`.


First, we decide how many years we want to simulate growth, and create an empty vector that will hold our data.
```{r}
years <- 10
N <-numeric(years+1) # rep(0,years+1) would do the same thing.
```
Our vector of $N$ has to be one longer than the number of $R$ we use. This is because each $R$ is sthe change *from one year to the next* and there will always be one more *next* than there is $R$.

Next we draw 10 $R$ at random with replacement. This is just like having all 35 observed $R$ written down on slips of paper and dropped into a paper bag. We then draw one slip of paper out of the bag, write the number down, and put the slip of paper back in the bag, and then repeat this 9 more times. This is *resampling with replacement*. In that case, we would be assuming that all of these $R_t$ are important and will occur at some point, but we just don't know when---they constitute the entire universe of possiblities. The R function *sample* will do this. 
[A random process occurs only in our imagination, or perhaps at the quantum level.^[Random, pseudorandom, and stochastic processes.] A stochastic process is one which we treat operationally as random while acknowledging that there are complex underlying deterministic drivers. A pseudorandom process is a completely deterministic and hidden process used by computers and their programmers to generate numbers that cannot be distinguished from random; we can repeat a pseudorandom process by stipulating a key hidden starting point.]

We can use `set.seed()` to make your pseudorandom process the same as mine, i.e., repeatable.
```{r}
set.seed(3)
# Draw a sample of our observed R with replacement, "years" times.
(rRs <- sample(x=obs.R, size=years, replace = TRUE))
```
Now that we have these 10 $R$, all we have to do is use them to generate the population sizes through time. For this, we need to use what programmers call a *for-loop*. In brief, a for-loop repeats a series of steps for a predetermined number of times. 


Let's start our simulated N with the sparrow count we had in the last year.
```{r}
N[1] <- Count[length(Count)]
```

Now we are ready to use the for-loop to project the population. For each year $t$, we multiply $N_t$ by the randomly selected $R_t$ to get $N_{t+1}$ and put it into the $t +1$ element of `N`. 
```{r forloop}
for( t in 1:years) { 
# starting with year = 1, and for each subsequent year, do... 
N[t+1] <- N[t] * rRs[t]
}
``` 
Let's graph the result.
```{r simten, fig.cap="A single simulated population projection."}
qplot(0:years, N, geom=c("point","line"))
```

It appears to work (Fig. \@ref(fig:simten)). Let's review what we have done. We

*had a bird count each year for 36 years. From this we calculated 35 $R$ (for all years except the very last).
* decided how many years we wanted to project the population (10\,y).
* drew at random and with replacement the observed $R$---one $R$ for each year we want to project forward.
* we created an empty vector and put in an initial value (the last year's real data).
* performed each year's calculation, and put it into the vector we made.


So what does Fig. \@ref(fig:OneSim) represent? It represents one possible outcome of a trajectory, if we assume that $R$ has an equal probability of being any of the observed $R_t$. This *particular* trajectory is very unlikely, because it would require one particular sequence of randomly selected $R$s. However,  it is *no less likely* than any other particular trajectory.

As only one realization of a set of randomly selected $R$, Fig. \@ref(fig:OneSim) tells us very little. What we need to do now is to replicate this process a very large number of times, and examine the *distribution* of outcomes, including moments of the distribution such as the mean, median, and confidence interval of eventual outcomes. 

### Multiple simulations
Now we create a way to perform the above simulation several times. There are a couple tricks we use to do this. We still want to start small so we can figure out the steps as we go. Here is what we would do next. 

1. We start by creating a function that will do the steps we did above.
2. We then do replicate independent simulations, using `replicate()`.


Here we write a function to combine several steps.
```{r}
myForLoop <- function(obs.R, years, initial.N) {
  # select all R at random
  rR <- sample(obs.R, size=years, replace=TRUE)
  # create a vector to hold N
  N <- numeric(years+1)
  # give it an initial population size
  N[1] <- initial.N
  # Do the for-loop
  for( t in 1:years ) {
    # project the population one time step
      N[t+1] <-  N[t] * rR[t]
  }
  # return the vector of N
  N
} 
# try it out with different hypothetical R
myForLoop(obs.R=0:3, years=5, initial.N=43)
```
Our function seems to work. Next we do ten such projection simulations, each for 50 time steps, using the sparrow data.
```{r tensim}
# specify the number of simulations and for how long
sims=10; years=50
set.seed(3)
outmat <- replicate(sims,   
                    expr=myForLoop(obs.R=obs.R, years=years, initial.N=43)
                    )
```
Now let's peek at the results (Fig. \@ref(fig:tensim)). It is fun to graph our output, but also helps us make sure we are not making a heinous mistake in our code. Note we use log scale to help us see the small populations.
```{r tensimfig, fig.cap="Using matplot() to plot a matrix vs. a single variable. Our simulated populations sometimes increase and sometimes decrease.", out.width="75%"}
matplot(0:years, outmat, type="l", log="y")
```

```{r tensimfig, fig.cap="Using ggplot() to plot one variable against vs. a single variable, organized by a grouping variable. Our simulated populations sometimes increase and sometimes decrease.", out.width="75%"}
# combine columns years, and our output
junk <- data.frame(years = 1:(years+1), outmat)
names(junk)
# make sure to load 'tidyr' if you did not already load it or tidyverse
# library(tidyr)
# "gather" many columns into one (except years)
out.long <- gather(junk, key="Run", value="N", -years)
ggplot(data=out.long, aes(x=years, y=N, group=Run)) + geom_line() +  
  scale_y_log10() 

# Or for colorful lines
# ggplot(data=out.long, aes(x=years, y=N,  linetype=Run, colour=Run)) + 
#    geom_line(show.legend=FALSE) + scale_y_log10() 
```


What does it mean that the simulation has an approximately even distribution of final population sizes \emph{on the log scale} (Fig. \ref{fig:TenSim})? If we plotted it on a linear scale, what would it look like?^[Plotting it on the log scale reveals that the relative change is independent of population size; this is true because the rate of change is geometric. If we plotted it on a linear scale, we would see that many trajectories result in small counts, and only a few get really big. That is, the median size is pretty small, but a few populations get huge.}

Rerunning this simulation, with new $R$ each time, will show different dynamics every time, and that is the point of simulations. Simulations are a way to make a few key assumptions, and then leave the rest to chance. In that sense it is a null model of population dynamics.

###  A distribution of possible futures
Now we are in a position to make an informed prediction, given our assumptions. We will predict the range of possible outcomes and the most likely outcomes, given our set of assumptions. 

We will simulate the population for 50 years 1000 times and describe the distribution of final populatin sizes. simulate 1000 populations,^[If we were doing this in a serious manner, we might do 10--100\,000 times.} and use \texttt{system.time} to tell me how long it takes on my computer.
```{r}
sims=1e4; years=50
set.seed(3)
## system.time keeps track of how long processes take.
system.time(
outmat <- replicate(sims,   
                    expr=myForLoop(obs.R=obs.R, years=years, initial.N=43)
                    )
            )
```
This tells me how long it took to complete 10\,000 simulations.  We also check the dimensions of the output, and they make sense.
```{r}
dim(outmat)
```
We see that we have an object that is the size we think it should be. We shall assume that everything worked way we think it should. 

### Analyzing results
We extract the last year of the simulations (last row), and summarize it with quartiles (0%, 25%, 50%, 75%, 100%, and also the mean).
```{r}
N.2053 <- outmat[51,]
summary(N.2053, digits=6)
```

```{r fig.cap="Distribution of the 10000 final base-10 log population sizes. Note the approximately Normal distribution.", out.width="75%"}
hist(log10(N.2053))
```

The `quantile()` function allows us to find a form of empirical confidence interval, including, approximately, the central 90% of the observations.^[Note that there are many ways to estimate quantiles (R has nine ways), but they are approximately similar to percentiles.]
```{r}
quantile(N.2053, prob=c(0.05, .95) )
```
These quantiles provide an estimate of the most likely range of possible populatin sizes, given our assumptions. 

### Inferring processes underlying growth rate
The above approach relies only on the observed data. That means that the growth rates, while representative, can never be different than what was observed. A different approaach would be to assume that the growth rates can be different than observed, *but drawn from the same underlying process* that caused the observed rates. 

The observed rates are simply a visible manifestation of unseen processes. We might summarize these by asserting that the observed growth rates were samples from a continuous distribution distribution, whose prperties we can infer from the sample. For instance, it may be that these processes cause annual rates to follow a Normal, or perhaps log-normal distribution. 

We can fit a Normal distribution to the logarithms of our observed $R$, and we see that it doesn't do too bad a job (Fig. 

```{r fitln, fig.cap="The logarithms of the observed R seem reasonably approximated by a Normal distribution whose mean and standard deviation are derived from the log-transformed data.", out.width="75%"}
mu <- mean( log(obs.R) )
sigma <- sd( log(obs.R) )

# a regular sequence for log-R
lR <- seq(-1, 1.1, by=0.01)
# the probability densities for the log-R
dR <- dnorm(lR, m=mu, sd=sigma)

hist(log(obs.R), breaks=10, prob=TRUE)
lines(lR, dR)
```

Now we will simulate populations just like before, but instead of random draws from the observed data, we do random draws from the inferred distribution.

Our new function.
```{r}
myForLoop2 <- function(mu, sigma, years, initial.N) {
  # select all R at random from 
  lrR <- rnorm(years, m=mu, sd=sigma)
  rR <- exp(lrR)
  # create a vector to hold N
  N <- numeric(years+1)
  # give it an initial population size
  N[1] <- initial.N
  # Do the for-loop
  for( t in 1:years ) {
    # project the population one time step
      N[t+1] <-  N[t] * rR[t]
  }
  # return the vector of N
  N
} 
```

Our new simulations.
```{r}
sims=1e4; years=50
set.seed(3)
outmat2 <- replicate(sims,   
                    expr=myForLoop2(mu=mu, sigma=sigma, years=years, initial.N=43)
                    )
N2.2053 <- outmat2[51,]
quantile(N2.2053, prob=c(0.05, .95) )
quantile(N.2053, prob=c(0.05, .95) )
```
The results are very similar to those based on only the observed $R$.

Our conclusions are based on a model of discrete density-independent population growth --- what assumptions are we making? are they valid? Are our unrealistic assumptions perhaps nonetheless a good approximation of reality? Most importantly, what would you like to add next to make the model a better approximation?


\section{Summary}
In this chapter, we have explored the meaning of density-independent population growth. It is a statistically demonstrable phenomenon, wherein the per captia growth rate exhibits no relation with population density. It is a useful starting point for conceptualizing population growth. We have derived discrete geometric and continuous exponential growth and seen how they are related. We have caculated doubling times. We have discussed the assumptions that different people might make regarding these growth models. Last, we have used simulation to explore prediction and inference in a density-independent context.

<!--chapter:end:Untitled.Rmd-->

